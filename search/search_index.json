{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pyjanitor-examples Example notebooks on how to use the pyjanitor package .","title":"pyjanitor-examples"},{"location":"#pyjanitor-examples","text":"Example notebooks on how to use the pyjanitor package .","title":"pyjanitor-examples"},{"location":"Row_to_Names/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); row_to_names : Elevates a row to be the column names of a DataFrame. Background This notebook serves to show a brief and simple example of how to swap column names using one of the rows in the dataframe. from io import StringIO import janitor import pandas as pd data = \"\"\"shoe, 220, 100 shoe, 450, 40 item, retail_price, cost shoe, 200, 38 bag, 305, 25 \"\"\" temp = pd.read_csv(StringIO(data), header=None) temp .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 shoe 220 100 1 shoe 450 40 2 item retail_price cost 3 shoe 200 38 4 bag 305 25 Looking at the dataframe above, we would love to use row 2 as our column names. One way to achieve this involves a couple of steps Use loc/iloc to assign row 2 to columns. Strip off any whitespace. Drop row 2 from the dataframe using the drop method. Set axis name to none. temp.columns = temp.iloc[2, :] temp.columns = temp.columns.str.strip() temp = temp.drop(2, axis=0) temp = temp.rename_axis(None, axis=\"columns\") temp .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } item retail_price cost 0 shoe 220 100 1 shoe 450 40 3 shoe 200 38 4 bag 305 25 However, the first two steps prevent us from method chaining. This is easily resolved using the row_to_names function df = pd.read_csv(StringIO(data), header=None).row_to_names( row_number=2, remove_row=True ) df /tmp/pyjanitor-examples_env/lib/python3.9/site-packages/janitor/functions/row_to_names.py:173: UserWarning: The function row_to_names will, in the official 1.0 release, change its behaviour to reset the dataframe's index by default. You can prepare for this change right now by explicitly setting `reset_index=True` when calling on `row_to_names`. warnings.warn( .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } item retail_price cost 0 shoe 220 100 1 shoe 450 40 3 shoe 200 38 4 bag 305 25","title":"Row to Names"},{"location":"Row_to_Names/#row_to_names-elevates-a-row-to-be-the-column-names-of-a-dataframe","text":"","title":"row_to_names : Elevates a row to be the column names of a DataFrame."},{"location":"Row_to_Names/#background","text":"This notebook serves to show a brief and simple example of how to swap column names using one of the rows in the dataframe. from io import StringIO import janitor import pandas as pd data = \"\"\"shoe, 220, 100 shoe, 450, 40 item, retail_price, cost shoe, 200, 38 bag, 305, 25 \"\"\" temp = pd.read_csv(StringIO(data), header=None) temp .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 shoe 220 100 1 shoe 450 40 2 item retail_price cost 3 shoe 200 38 4 bag 305 25 Looking at the dataframe above, we would love to use row 2 as our column names. One way to achieve this involves a couple of steps Use loc/iloc to assign row 2 to columns. Strip off any whitespace. Drop row 2 from the dataframe using the drop method. Set axis name to none. temp.columns = temp.iloc[2, :] temp.columns = temp.columns.str.strip() temp = temp.drop(2, axis=0) temp = temp.rename_axis(None, axis=\"columns\") temp .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } item retail_price cost 0 shoe 220 100 1 shoe 450 40 3 shoe 200 38 4 bag 305 25 However, the first two steps prevent us from method chaining. This is easily resolved using the row_to_names function df = pd.read_csv(StringIO(data), header=None).row_to_names( row_number=2, remove_row=True ) df /tmp/pyjanitor-examples_env/lib/python3.9/site-packages/janitor/functions/row_to_names.py:173: UserWarning: The function row_to_names will, in the official 1.0 release, change its behaviour to reset the dataframe's index by default. You can prepare for this change right now by explicitly setting `reset_index=True` when calling on `row_to_names`. warnings.warn( .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } item retail_price cost 0 shoe 220 100 1 shoe 450 40 3 shoe 200 38 4 bag 305 25","title":"Background"},{"location":"anime/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Processing Anime Data Background We will use pyjanitor to showcase how to conveniently chain methods together to perform data cleaning in one shot. We We first define and register a series of dataframe methods with pandas_flavor. Then we chain the dataframe methods together with pyjanitor methods to complete the data cleaning process. The below example shows a one-shot script followed by a step-by-step detail of each part of the method chain. We have adapted a TidyTuesday analysis that was originally performed in R. The original text from TidyTuesday will be shown in blockquotes. Note: TidyTuesday is based on the principles discussed and made popular by Hadley Wickham in his paper Tidy Data . The original text from TidyTuesday will be shown in blockquotes. Here is a description of the Anime data set that we will use. This week's data comes from Tam Nguyen and MyAnimeList.net via Kaggle . According to Wikipedia - \"MyAnimeList, often abbreviated as MAL, is an anime and manga social networking and social cataloging application website. The site provides its users with a list-like system to organize and score anime and manga. It facilitates finding users who share similar tastes and provides a large database on anime and manga. The site claims to have 4.4 million anime and 775,000 manga entries. In 2015, the site received 120 million visitors a month.\" Anime without rankings or popularity scores were excluded. Producers, genre, and studio were converted from lists to tidy observations, so there will be repetitions of shows with multiple producers, genres, etc. The raw data is also uploaded. Lots of interesting ways to explore the data this week! Import libraries and load data # Import pyjanitor and pandas # Supress user warnings when we try overwriting our custom pandas flavor functions import warnings import janitor import pandas as pd import pandas_flavor as pf warnings.filterwarnings(\"ignore\") One-Shot filename = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-23/raw_anime.csv\" df = pd.read_csv(filename) @pf.register_dataframe_method def str_remove(df, column_name: str, pat: str, *args, **kwargs): \"\"\"Wrapper around df.str.replace\"\"\" df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) return df @pf.register_dataframe_method def str_trim(df, column_name: str, *args, **kwargs): \"\"\"Wrapper around df.str.strip\"\"\" df[column_name] = df[column_name].str.strip(*args, **kwargs) return df @pf.register_dataframe_method def explode(df: pd.DataFrame, column_name: str, sep: str): \"\"\" For rows with a list of values, this function will create new rows for each value in the list \"\"\" df[\"id\"] = df.index wdf = ( pd.DataFrame(df[column_name].str.split(sep).fillna(\"\").tolist()) .stack() .reset_index() ) # exploded_column = column_name wdf.columns = [\"id\", \"depth\", column_name] # plural form to singular form # wdf[column_name] = wdf[column_name].apply(lambda x: x.strip()) # trim wdf.drop(\"depth\", axis=1, inplace=True) return pd.merge(df, wdf, on=\"id\", suffixes=(\"_drop\", \"\")).drop( columns=[\"id\", column_name + \"_drop\"] ) @pf.register_dataframe_method def str_word( df, column_name: str, start: int = None, stop: int = None, pat: str = \" \", *args, **kwargs, ): \"\"\" Wrapper around `df.str.split` with additional `start` and `end` arguments to select a slice of the list of words. \"\"\" df[column_name] = df[column_name].str.split(pat).str[start:stop] return df @pf.register_dataframe_method def str_join(df, column_name: str, sep: str, *args, **kwargs): \"\"\" Wrapper around `df.str.join` Joins items in a list. \"\"\" df[column_name] = df[column_name].str.join(sep) return df @pf.register_dataframe_method def str_slice( df, column_name: str, start: int = None, stop: int = None, *args, **kwargs ): \"\"\" Wrapper around `df.str.slice \"\"\" df[column_name] = df[column_name].str[start:stop] return df clean_df = ( df.str_remove(column_name=\"producers\", pat=\"\\[|\\]\") .explode(column_name=\"producers\", sep=\",\") .str_remove(column_name=\"producers\", pat=\"'\") .str_trim(\"producers\") .str_remove(column_name=\"genre\", pat=\"\\[|\\]\") .explode(column_name=\"genre\", sep=\",\") .str_remove(column_name=\"genre\", pat=\"'\") .str_trim(column_name=\"genre\") .str_remove(column_name=\"studio\", pat=\"\\[|\\]\") .explode(column_name=\"studio\", sep=\",\") .str_remove(column_name=\"studio\", pat=\"'\") .str_trim(column_name=\"studio\") .str_remove(column_name=\"aired\", pat=\"\\{|\\}|'from':\\s*|'to':\\s*\") .str_word(column_name=\"aired\", start=0, stop=2, pat=\",\") .str_join(column_name=\"aired\", sep=\",\") .deconcatenate_column( column_name=\"aired\", new_column_names=[\"start_date\", \"end_date\"], sep=\",\" ) .remove_columns(column_names=[\"aired\"]) .str_remove(column_name=\"start_date\", pat=\"'\") .str_slice(column_name=\"start_date\", start=0, stop=10) .str_remove(column_name=\"end_date\", pat=\"'\") .str_slice(column_name=\"end_date\", start=0, stop=11) .to_datetime(\"start_date\", format=\"%Y-%m-%d\", errors=\"coerce\") .to_datetime(\"end_date\", format=\"%Y-%m-%d\", errors=\"coerce\") .fill_empty(columns=[\"rank\", \"popularity\"], value=0) .filter_on(\"rank != 0 & popularity != 0\") ) clean_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } animeID name title_english title_japanese title_synonyms type source episodes status airing ... synopsis background premiered broadcast related producers genre studio start_date end_date 0 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Action Sunrise 1998-04-03 1999-04-24 1 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Adventure Sunrise 1998-04-03 1999-04-24 2 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Comedy Sunrise 1998-04-03 1999-04-24 3 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Drama Sunrise 1998-04-03 1999-04-24 4 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Sci-Fi Sunrise 1998-04-03 1999-04-24 5 rows \u00d7 28 columns Multi-Step Data Dictionary Heads up the dataset is about 97 mb - if you want to free up some space, drop the synopsis and background, they are long strings, or broadcast, premiered, related as they are redundant or less useful. variable class description animeID double Anime ID (as in https://myanimelist.net/anime/animeID) name character anime title - extracted from the site. title_english character title in English (sometimes is different, sometimes is missing) title_japanese character title in Japanese (if Anime is Chinese or Korean, the title, if available, in the respective language) title_synonyms character other variants of the title type character anime type (e.g. TV, Movie, OVA) source character source of anime (i.e original, manga, game, music, visual novel etc.) producers character producers genre character genre studio character studio episodes double number of episodes status character Aired or not aired airing logical True/False is still airing start_date double Start date (ymd) end_date double End date (ymd) duration character Per episode duration or entire duration, text string rating character Age rating score double Score (higher = better) scored_by double Number of users that scored rank double Rank - weight according to MyAnimeList formula popularity double based on how many members/users have the respective anime in their list members double number members that added this anime in their list favorites double number members that favorites these in their list synopsis character long string with anime synopsis background character long string with production background and other things premiered character anime premiered on season/year broadcast character when is (regularly) broadcasted related character dictionary: related animes, series, games etc. Step 0: Load data filename = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-23/raw_anime.csv\" df = pd.read_csv(filename) df.head(3).T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 animeID 1 5 6 name Cowboy Bebop Cowboy Bebop: Tengoku no Tobira Trigun title_english Cowboy Bebop Cowboy Bebop: The Movie Trigun title_japanese \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 \u5929\u56fd\u306e\u6249 \u30c8\u30e9\u30a4\u30ac\u30f3 title_synonyms [] [\"Cowboy Bebop: Knockin' on Heaven's Door\"] [] type TV Movie TV source Original Original Manga producers ['Bandai Visual'] ['Sunrise', 'Bandai Visual'] ['Victor Entertainment'] genre ['Action', 'Adventure', 'Comedy', 'Drama', 'Sc... ['Action', 'Drama', 'Mystery', 'Sci-Fi', 'Space'] ['Action', 'Sci-Fi', 'Adventure', 'Comedy', 'D... studio ['Sunrise'] ['Bones'] ['Madhouse'] episodes 26.0 1.0 26.0 status Finished Airing Finished Airing Finished Airing airing False False False aired {'from': '1998-04-03T00:00:00+00:00', 'to': '1... {'from': '2001-09-01T00:00:00+00:00', 'to': No... {'from': '1998-04-01T00:00:00+00:00', 'to': '1... duration 24 min per ep 1 hr 55 min 24 min per ep rating R - 17+ (violence & profanity) R - 17+ (violence & profanity) PG-13 - Teens 13 or older score 8.81 8.41 8.3 scored_by 405664.0 120243.0 212537.0 rank 26.0 164.0 255.0 popularity 39.0 449.0 146.0 members 795733.0 197791.0 408548.0 favorites 43460.0 776.0 10432.0 synopsis In the year 2071, humanity has colonized sever... Another day, another bounty\u2014such is the life o... Vash the Stampede is the man with a $$60,000,0... background When Cowboy Bebop first aired in spring of 199... NaN The Japanese release by Victor Entertainment h... premiered Spring 1998 NaN Spring 1998 broadcast Saturdays at 01:00 (JST) NaN Thursdays at 01:15 (JST) related {'Adaptation': [{'mal_id': 173, 'type': 'manga... {'Parent story': [{'mal_id': 1, 'type': 'anime... {'Adaptation': [{'mal_id': 703, 'type': 'manga... Step 1: Clean producers column The first step tries to clean up the producers column by removing some brackets ('[]') and trim off some empty spaces clean_df <- raw_df %>% # Producers mutate(producers = str_remove(producers, \"\\\\[\"), producers = str_remove(producers, \"\\\\]\")) What is mutate? This link compares R's mutate to be similar to pandas' df.assign . However, df.assign returns a new DataFrame whereas mutate adds a new variable while preserving the previous ones. Therefore, for this example, I will compare mutate to be similar to df['col'] = X As we can see, this is looks like a list of items but in string form # Let's see what we trying to remove df.loc[df[\"producers\"].str.contains(\"\\[\", na=False), \"producers\"].head() 0 ['Bandai Visual'] 1 ['Sunrise', 'Bandai Visual'] 2 ['Victor Entertainment'] 3 ['Bandai Visual'] 4 ['TV Tokyo', 'Dentsu'] Name: producers, dtype: object Let's use pandas flavor to create a custom method for just removing some strings so we don't have to use str.replace so many times. @pf.register_dataframe_method def str_remove(df, column_name: str, pat: str, *args, **kwargs): \"\"\" Wrapper around df.str.replace The function will loop through regex patterns and remove them from the desired column. :param df: A pandas DataFrame. :param column_name: A `str` indicating which column the string removal action is to be made. :param pat: A regex pattern to match and remove. \"\"\" if not isinstance(pat, str): raise TypeError( f\"Pattern should be a valid regex pattern. Received pattern: {pat} with dtype: {type(pat)}\" ) df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) return df clean_df = df.str_remove(column_name=\"producers\", pat=\"\\[|\\]\") With brackets removed. clean_df[\"producers\"].head() 0 'Bandai Visual' 1 'Sunrise', 'Bandai Visual' 2 'Victor Entertainment' 3 'Bandai Visual' 4 'TV Tokyo', 'Dentsu' Name: producers, dtype: object Brackets are removed. Now the next part separate_rows(producers, sep = \",\") %>% It seems like separate rows will go through each value of the column, and if the value is a list, will create a new row for each value in the list with the remaining column values being the same. This is commonly known as an explode method but it is not yet implemented in pandas. We will need a function for this (code adopted from here ). @pf.register_dataframe_method def explode(df: pd.DataFrame, column_name: str, sep: str): \"\"\" For rows with a list of values, this function will create new rows for each value in the list :param df: A pandas DataFrame. :param column_name: A `str` indicating which column the string removal action is to be made. :param sep: The delimiter. Example delimiters include `|`, `, `, `,` etc. \"\"\" df[\"id\"] = df.index wdf = ( pd.DataFrame(df[column_name].str.split(sep).fillna(\"\").tolist()) .stack() .reset_index() ) # exploded_column = column_name wdf.columns = [\"id\", \"depth\", column_name] # plural form to singular form # wdf[column_name] = wdf[column_name].apply(lambda x: x.strip()) # trim wdf.drop(\"depth\", axis=1, inplace=True) return pd.merge(df, wdf, on=\"id\", suffixes=(\"_drop\", \"\")).drop( columns=[\"id\", column_name + \"_drop\"] ) clean_df = clean_df.explode(column_name=\"producers\", sep=\",\") Now every producer is its own row. clean_df[\"producers\"].head() 0 'Bandai Visual' 1 'Sunrise' 2 'Bandai Visual' 3 'Victor Entertainment' 4 'Bandai Visual' Name: producers, dtype: object","title":"Anime"},{"location":"anime/#processing-anime-data","text":"","title":"Processing Anime Data"},{"location":"anime/#background","text":"We will use pyjanitor to showcase how to conveniently chain methods together to perform data cleaning in one shot. We We first define and register a series of dataframe methods with pandas_flavor. Then we chain the dataframe methods together with pyjanitor methods to complete the data cleaning process. The below example shows a one-shot script followed by a step-by-step detail of each part of the method chain. We have adapted a TidyTuesday analysis that was originally performed in R. The original text from TidyTuesday will be shown in blockquotes. Note: TidyTuesday is based on the principles discussed and made popular by Hadley Wickham in his paper Tidy Data . The original text from TidyTuesday will be shown in blockquotes. Here is a description of the Anime data set that we will use. This week's data comes from Tam Nguyen and MyAnimeList.net via Kaggle . According to Wikipedia - \"MyAnimeList, often abbreviated as MAL, is an anime and manga social networking and social cataloging application website. The site provides its users with a list-like system to organize and score anime and manga. It facilitates finding users who share similar tastes and provides a large database on anime and manga. The site claims to have 4.4 million anime and 775,000 manga entries. In 2015, the site received 120 million visitors a month.\" Anime without rankings or popularity scores were excluded. Producers, genre, and studio were converted from lists to tidy observations, so there will be repetitions of shows with multiple producers, genres, etc. The raw data is also uploaded. Lots of interesting ways to explore the data this week! Import libraries and load data # Import pyjanitor and pandas # Supress user warnings when we try overwriting our custom pandas flavor functions import warnings import janitor import pandas as pd import pandas_flavor as pf warnings.filterwarnings(\"ignore\")","title":"Background"},{"location":"anime/#one-shot","text":"filename = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-23/raw_anime.csv\" df = pd.read_csv(filename) @pf.register_dataframe_method def str_remove(df, column_name: str, pat: str, *args, **kwargs): \"\"\"Wrapper around df.str.replace\"\"\" df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) return df @pf.register_dataframe_method def str_trim(df, column_name: str, *args, **kwargs): \"\"\"Wrapper around df.str.strip\"\"\" df[column_name] = df[column_name].str.strip(*args, **kwargs) return df @pf.register_dataframe_method def explode(df: pd.DataFrame, column_name: str, sep: str): \"\"\" For rows with a list of values, this function will create new rows for each value in the list \"\"\" df[\"id\"] = df.index wdf = ( pd.DataFrame(df[column_name].str.split(sep).fillna(\"\").tolist()) .stack() .reset_index() ) # exploded_column = column_name wdf.columns = [\"id\", \"depth\", column_name] # plural form to singular form # wdf[column_name] = wdf[column_name].apply(lambda x: x.strip()) # trim wdf.drop(\"depth\", axis=1, inplace=True) return pd.merge(df, wdf, on=\"id\", suffixes=(\"_drop\", \"\")).drop( columns=[\"id\", column_name + \"_drop\"] ) @pf.register_dataframe_method def str_word( df, column_name: str, start: int = None, stop: int = None, pat: str = \" \", *args, **kwargs, ): \"\"\" Wrapper around `df.str.split` with additional `start` and `end` arguments to select a slice of the list of words. \"\"\" df[column_name] = df[column_name].str.split(pat).str[start:stop] return df @pf.register_dataframe_method def str_join(df, column_name: str, sep: str, *args, **kwargs): \"\"\" Wrapper around `df.str.join` Joins items in a list. \"\"\" df[column_name] = df[column_name].str.join(sep) return df @pf.register_dataframe_method def str_slice( df, column_name: str, start: int = None, stop: int = None, *args, **kwargs ): \"\"\" Wrapper around `df.str.slice \"\"\" df[column_name] = df[column_name].str[start:stop] return df clean_df = ( df.str_remove(column_name=\"producers\", pat=\"\\[|\\]\") .explode(column_name=\"producers\", sep=\",\") .str_remove(column_name=\"producers\", pat=\"'\") .str_trim(\"producers\") .str_remove(column_name=\"genre\", pat=\"\\[|\\]\") .explode(column_name=\"genre\", sep=\",\") .str_remove(column_name=\"genre\", pat=\"'\") .str_trim(column_name=\"genre\") .str_remove(column_name=\"studio\", pat=\"\\[|\\]\") .explode(column_name=\"studio\", sep=\",\") .str_remove(column_name=\"studio\", pat=\"'\") .str_trim(column_name=\"studio\") .str_remove(column_name=\"aired\", pat=\"\\{|\\}|'from':\\s*|'to':\\s*\") .str_word(column_name=\"aired\", start=0, stop=2, pat=\",\") .str_join(column_name=\"aired\", sep=\",\") .deconcatenate_column( column_name=\"aired\", new_column_names=[\"start_date\", \"end_date\"], sep=\",\" ) .remove_columns(column_names=[\"aired\"]) .str_remove(column_name=\"start_date\", pat=\"'\") .str_slice(column_name=\"start_date\", start=0, stop=10) .str_remove(column_name=\"end_date\", pat=\"'\") .str_slice(column_name=\"end_date\", start=0, stop=11) .to_datetime(\"start_date\", format=\"%Y-%m-%d\", errors=\"coerce\") .to_datetime(\"end_date\", format=\"%Y-%m-%d\", errors=\"coerce\") .fill_empty(columns=[\"rank\", \"popularity\"], value=0) .filter_on(\"rank != 0 & popularity != 0\") ) clean_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } animeID name title_english title_japanese title_synonyms type source episodes status airing ... synopsis background premiered broadcast related producers genre studio start_date end_date 0 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Action Sunrise 1998-04-03 1999-04-24 1 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Adventure Sunrise 1998-04-03 1999-04-24 2 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Comedy Sunrise 1998-04-03 1999-04-24 3 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Drama Sunrise 1998-04-03 1999-04-24 4 1 Cowboy Bebop Cowboy Bebop \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 [] TV Original 26.0 Finished Airing False ... In the year 2071, humanity has colonized sever... When Cowboy Bebop first aired in spring of 199... Spring 1998 Saturdays at 01:00 (JST) {'Adaptation': [{'mal_id': 173, 'type': 'manga... Bandai Visual Sci-Fi Sunrise 1998-04-03 1999-04-24 5 rows \u00d7 28 columns","title":"One-Shot"},{"location":"anime/#multi-step","text":"Data Dictionary Heads up the dataset is about 97 mb - if you want to free up some space, drop the synopsis and background, they are long strings, or broadcast, premiered, related as they are redundant or less useful. variable class description animeID double Anime ID (as in https://myanimelist.net/anime/animeID) name character anime title - extracted from the site. title_english character title in English (sometimes is different, sometimes is missing) title_japanese character title in Japanese (if Anime is Chinese or Korean, the title, if available, in the respective language) title_synonyms character other variants of the title type character anime type (e.g. TV, Movie, OVA) source character source of anime (i.e original, manga, game, music, visual novel etc.) producers character producers genre character genre studio character studio episodes double number of episodes status character Aired or not aired airing logical True/False is still airing start_date double Start date (ymd) end_date double End date (ymd) duration character Per episode duration or entire duration, text string rating character Age rating score double Score (higher = better) scored_by double Number of users that scored rank double Rank - weight according to MyAnimeList formula popularity double based on how many members/users have the respective anime in their list members double number members that added this anime in their list favorites double number members that favorites these in their list synopsis character long string with anime synopsis background character long string with production background and other things premiered character anime premiered on season/year broadcast character when is (regularly) broadcasted related character dictionary: related animes, series, games etc.","title":"Multi-Step"},{"location":"anime/#step-0-load-data","text":"filename = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-23/raw_anime.csv\" df = pd.read_csv(filename) df.head(3).T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 animeID 1 5 6 name Cowboy Bebop Cowboy Bebop: Tengoku no Tobira Trigun title_english Cowboy Bebop Cowboy Bebop: The Movie Trigun title_japanese \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 \u30ab\u30a6\u30dc\u30fc\u30a4\u30d3\u30d0\u30c3\u30d7 \u5929\u56fd\u306e\u6249 \u30c8\u30e9\u30a4\u30ac\u30f3 title_synonyms [] [\"Cowboy Bebop: Knockin' on Heaven's Door\"] [] type TV Movie TV source Original Original Manga producers ['Bandai Visual'] ['Sunrise', 'Bandai Visual'] ['Victor Entertainment'] genre ['Action', 'Adventure', 'Comedy', 'Drama', 'Sc... ['Action', 'Drama', 'Mystery', 'Sci-Fi', 'Space'] ['Action', 'Sci-Fi', 'Adventure', 'Comedy', 'D... studio ['Sunrise'] ['Bones'] ['Madhouse'] episodes 26.0 1.0 26.0 status Finished Airing Finished Airing Finished Airing airing False False False aired {'from': '1998-04-03T00:00:00+00:00', 'to': '1... {'from': '2001-09-01T00:00:00+00:00', 'to': No... {'from': '1998-04-01T00:00:00+00:00', 'to': '1... duration 24 min per ep 1 hr 55 min 24 min per ep rating R - 17+ (violence & profanity) R - 17+ (violence & profanity) PG-13 - Teens 13 or older score 8.81 8.41 8.3 scored_by 405664.0 120243.0 212537.0 rank 26.0 164.0 255.0 popularity 39.0 449.0 146.0 members 795733.0 197791.0 408548.0 favorites 43460.0 776.0 10432.0 synopsis In the year 2071, humanity has colonized sever... Another day, another bounty\u2014such is the life o... Vash the Stampede is the man with a $$60,000,0... background When Cowboy Bebop first aired in spring of 199... NaN The Japanese release by Victor Entertainment h... premiered Spring 1998 NaN Spring 1998 broadcast Saturdays at 01:00 (JST) NaN Thursdays at 01:15 (JST) related {'Adaptation': [{'mal_id': 173, 'type': 'manga... {'Parent story': [{'mal_id': 1, 'type': 'anime... {'Adaptation': [{'mal_id': 703, 'type': 'manga...","title":"Step 0: Load data"},{"location":"anime/#step-1-clean-producers-column","text":"The first step tries to clean up the producers column by removing some brackets ('[]') and trim off some empty spaces clean_df <- raw_df %>% # Producers mutate(producers = str_remove(producers, \"\\\\[\"), producers = str_remove(producers, \"\\\\]\")) What is mutate? This link compares R's mutate to be similar to pandas' df.assign . However, df.assign returns a new DataFrame whereas mutate adds a new variable while preserving the previous ones. Therefore, for this example, I will compare mutate to be similar to df['col'] = X As we can see, this is looks like a list of items but in string form # Let's see what we trying to remove df.loc[df[\"producers\"].str.contains(\"\\[\", na=False), \"producers\"].head() 0 ['Bandai Visual'] 1 ['Sunrise', 'Bandai Visual'] 2 ['Victor Entertainment'] 3 ['Bandai Visual'] 4 ['TV Tokyo', 'Dentsu'] Name: producers, dtype: object Let's use pandas flavor to create a custom method for just removing some strings so we don't have to use str.replace so many times. @pf.register_dataframe_method def str_remove(df, column_name: str, pat: str, *args, **kwargs): \"\"\" Wrapper around df.str.replace The function will loop through regex patterns and remove them from the desired column. :param df: A pandas DataFrame. :param column_name: A `str` indicating which column the string removal action is to be made. :param pat: A regex pattern to match and remove. \"\"\" if not isinstance(pat, str): raise TypeError( f\"Pattern should be a valid regex pattern. Received pattern: {pat} with dtype: {type(pat)}\" ) df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) return df clean_df = df.str_remove(column_name=\"producers\", pat=\"\\[|\\]\") With brackets removed. clean_df[\"producers\"].head() 0 'Bandai Visual' 1 'Sunrise', 'Bandai Visual' 2 'Victor Entertainment' 3 'Bandai Visual' 4 'TV Tokyo', 'Dentsu' Name: producers, dtype: object Brackets are removed. Now the next part separate_rows(producers, sep = \",\") %>% It seems like separate rows will go through each value of the column, and if the value is a list, will create a new row for each value in the list with the remaining column values being the same. This is commonly known as an explode method but it is not yet implemented in pandas. We will need a function for this (code adopted from here ). @pf.register_dataframe_method def explode(df: pd.DataFrame, column_name: str, sep: str): \"\"\" For rows with a list of values, this function will create new rows for each value in the list :param df: A pandas DataFrame. :param column_name: A `str` indicating which column the string removal action is to be made. :param sep: The delimiter. Example delimiters include `|`, `, `, `,` etc. \"\"\" df[\"id\"] = df.index wdf = ( pd.DataFrame(df[column_name].str.split(sep).fillna(\"\").tolist()) .stack() .reset_index() ) # exploded_column = column_name wdf.columns = [\"id\", \"depth\", column_name] # plural form to singular form # wdf[column_name] = wdf[column_name].apply(lambda x: x.strip()) # trim wdf.drop(\"depth\", axis=1, inplace=True) return pd.merge(df, wdf, on=\"id\", suffixes=(\"_drop\", \"\")).drop( columns=[\"id\", column_name + \"_drop\"] ) clean_df = clean_df.explode(column_name=\"producers\", sep=\",\") Now every producer is its own row. clean_df[\"producers\"].head() 0 'Bandai Visual' 1 'Sunrise' 2 'Bandai Visual' 3 'Victor Entertainment' 4 'Bandai Visual' Name: producers, dtype: object","title":"Step 1: Clean producers column"},{"location":"bad_values/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Replacing Bad Values This is US wind turbine data. The numeric fields use -9999 as a null value for missing data. Using -9999 as a null value in numeric fields will cause big problems for any summary statistics like totals, means, etc, we should change that to something else, like np.NaN which Pandas sum and mean functions will automatically filter out. You can see that the means for before and after replacing -9999 with np.NaN are very different. You can use Janitor's find_replace to easily replace them. import janitor import numpy as np import pandas as pd Load Wind Turbine Data wind = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-06/us_wind.csv\" ) wind.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } case_id faa_ors faa_asn usgs_pr_id t_state t_county t_fips p_name p_year p_tnum ... t_hh t_rd t_rsa t_ttlh t_conf_atr t_conf_loc t_img_date t_img_srce xlong ylat 0 3073429 missing missing 4960 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 1/1/2012 NAIP -118.360725 35.083778 1 3071522 missing missing 4997 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 1/1/2012 NAIP -118.361168 35.081512 2 3073425 missing missing 4957 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 1/1/2012 NAIP -118.360420 35.084709 3 3071569 missing missing 5023 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 7/31/2016 Digital Globe -118.364029 35.079418 4 3005252 missing missing 5768 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 11/23/2017 Digital Globe -118.354286 35.085594 5 rows \u00d7 24 columns Check Mean wind.t_hh.mean() -1069.986537767466 The t_hh column appears to be affected by -9999 values. What are all the columns that are affected? [col for col in wind.columns if -9999 in wind[col].values] ['usgs_pr_id', 'p_year', 'p_cap', 't_cap', 't_hh', 't_rd', 't_rsa', 't_ttlh'] Note: When replacing the -9999 values you can make a copy of the dataframe to prevent making modifications to the original dataframe. At first glance, it looks like the mean is negative, but this is only because of the bad values (-9999.0) throughout the column. To get the right mean, we should replace them!## Replace Bad Values with NaNs mapping = {-9999.0: np.nan} wind2 = wind.find_replace( usgs_pr_id=mapping, p_tnum=mapping, p_cap=mapping, t_cap=mapping, t_hh=mapping, t_rd=mapping, t_rsa=mapping, t_ttlh=mapping, ) wind2.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } case_id faa_ors faa_asn usgs_pr_id t_state t_county t_fips p_name p_year p_tnum ... t_hh t_rd t_rsa t_ttlh t_conf_atr t_conf_loc t_img_date t_img_srce xlong ylat 0 3073429 missing missing 4960.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 1/1/2012 NAIP -118.360725 35.083778 1 3071522 missing missing 4997.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 1/1/2012 NAIP -118.361168 35.081512 2 3073425 missing missing 4957.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 1/1/2012 NAIP -118.360420 35.084709 3 3071569 missing missing 5023.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 7/31/2016 Digital Globe -118.364029 35.079418 4 3005252 missing missing 5768.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 11/23/2017 Digital Globe -118.354286 35.085594 5 rows \u00d7 24 columns Check the Mean (again) wind2.t_hh.mean() 77.31203064391 And, now that the bad values were replaced by NaNs (which the mean() method ignores), the calculated mean is correct! Alternate method If we look at the description of the data (see README ) we can find descriptions for our data values, for example: p_year : Year project became operational t_hh : Turbine hub height (meters) xlong : Longitude Using our knowledge of the data, this would give us bounds we could use for these values. For example, the oldest electric wind turbine was built in 1887 and this document was written in 2018, so $1887 \\leq \\text{p_year} \\leq 2018$. Turbine hub height should be positive, and a value above 1 km would be silly, so $0 < \\text{t_hh} < 1000$. These are wind turbines in the United States, so $-161.76 < \\text{xlong} < -68.01$. (Note that the README actually gives us minima and maxima for the data, so we could get much tighter bounds from that.) To filter out potential bad values, we will use update_where to remove values outside these ranges. # Note that update_where mutates the original dataframe ( wind.update_where( (wind[\"p_year\"] < 1887) | (wind[\"p_year\"] > 2018), \"p_year\", np.nan ) .update_where((wind[\"t_hh\"] <= 0) | (wind[\"t_hh\"] >= 1000), \"t_hh\", np.nan) .update_where((wind[\"xlong\"] < -161.76) | (wind[\"xlong\"] > -68.01), \"xlong\", np.nan) ); Confirming this produces the same result wind.t_hh.mean() 77.31203064391","title":"Bad values"},{"location":"bad_values/#replacing-bad-values","text":"This is US wind turbine data. The numeric fields use -9999 as a null value for missing data. Using -9999 as a null value in numeric fields will cause big problems for any summary statistics like totals, means, etc, we should change that to something else, like np.NaN which Pandas sum and mean functions will automatically filter out. You can see that the means for before and after replacing -9999 with np.NaN are very different. You can use Janitor's find_replace to easily replace them. import janitor import numpy as np import pandas as pd","title":"Replacing Bad Values"},{"location":"bad_values/#load-wind-turbine-data","text":"wind = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-06/us_wind.csv\" ) wind.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } case_id faa_ors faa_asn usgs_pr_id t_state t_county t_fips p_name p_year p_tnum ... t_hh t_rd t_rsa t_ttlh t_conf_atr t_conf_loc t_img_date t_img_srce xlong ylat 0 3073429 missing missing 4960 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 1/1/2012 NAIP -118.360725 35.083778 1 3071522 missing missing 4997 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 1/1/2012 NAIP -118.361168 35.081512 2 3073425 missing missing 4957 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 1/1/2012 NAIP -118.360420 35.084709 3 3071569 missing missing 5023 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 7/31/2016 Digital Globe -118.364029 35.079418 4 3005252 missing missing 5768 CA Kern County 6029 251 Wind 1987 194 ... -9999.0 -9999.0 -9999.0 -9999.0 2 3 11/23/2017 Digital Globe -118.354286 35.085594 5 rows \u00d7 24 columns","title":"Load Wind Turbine Data"},{"location":"bad_values/#check-mean","text":"wind.t_hh.mean() -1069.986537767466 The t_hh column appears to be affected by -9999 values. What are all the columns that are affected? [col for col in wind.columns if -9999 in wind[col].values] ['usgs_pr_id', 'p_year', 'p_cap', 't_cap', 't_hh', 't_rd', 't_rsa', 't_ttlh'] Note: When replacing the -9999 values you can make a copy of the dataframe to prevent making modifications to the original dataframe. At first glance, it looks like the mean is negative, but this is only because of the bad values (-9999.0) throughout the column. To get the right mean, we should replace them!## Replace Bad Values with NaNs mapping = {-9999.0: np.nan} wind2 = wind.find_replace( usgs_pr_id=mapping, p_tnum=mapping, p_cap=mapping, t_cap=mapping, t_hh=mapping, t_rd=mapping, t_rsa=mapping, t_ttlh=mapping, ) wind2.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } case_id faa_ors faa_asn usgs_pr_id t_state t_county t_fips p_name p_year p_tnum ... t_hh t_rd t_rsa t_ttlh t_conf_atr t_conf_loc t_img_date t_img_srce xlong ylat 0 3073429 missing missing 4960.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 1/1/2012 NAIP -118.360725 35.083778 1 3071522 missing missing 4997.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 1/1/2012 NAIP -118.361168 35.081512 2 3073425 missing missing 4957.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 1/1/2012 NAIP -118.360420 35.084709 3 3071569 missing missing 5023.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 7/31/2016 Digital Globe -118.364029 35.079418 4 3005252 missing missing 5768.0 CA Kern County 6029 251 Wind 1987 194 ... NaN NaN NaN NaN 2 3 11/23/2017 Digital Globe -118.354286 35.085594 5 rows \u00d7 24 columns","title":"Check Mean"},{"location":"bad_values/#check-the-mean-again","text":"wind2.t_hh.mean() 77.31203064391 And, now that the bad values were replaced by NaNs (which the mean() method ignores), the calculated mean is correct!","title":"Check the Mean (again)"},{"location":"bad_values/#alternate-method","text":"If we look at the description of the data (see README ) we can find descriptions for our data values, for example: p_year : Year project became operational t_hh : Turbine hub height (meters) xlong : Longitude Using our knowledge of the data, this would give us bounds we could use for these values. For example, the oldest electric wind turbine was built in 1887 and this document was written in 2018, so $1887 \\leq \\text{p_year} \\leq 2018$. Turbine hub height should be positive, and a value above 1 km would be silly, so $0 < \\text{t_hh} < 1000$. These are wind turbines in the United States, so $-161.76 < \\text{xlong} < -68.01$. (Note that the README actually gives us minima and maxima for the data, so we could get much tighter bounds from that.) To filter out potential bad values, we will use update_where to remove values outside these ranges. # Note that update_where mutates the original dataframe ( wind.update_where( (wind[\"p_year\"] < 1887) | (wind[\"p_year\"] > 2018), \"p_year\", np.nan ) .update_where((wind[\"t_hh\"] <= 0) | (wind[\"t_hh\"] >= 1000), \"t_hh\", np.nan) .update_where((wind[\"xlong\"] < -161.76) | (wind[\"xlong\"] > -68.01), \"xlong\", np.nan) );","title":"Alternate method"},{"location":"bad_values/#confirming-this-produces-the-same-result","text":"wind.t_hh.mean() 77.31203064391","title":"Confirming this produces the same result"},{"location":"bird_call/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Processing Bird Call Data Background The following example was obtained by translating the R code from TidyTuesday 2019-04-30 to Python using Pandas and PyJanitor. It provides a simple example of using pyjanitor for: - column renaming - column name cleaning - dataframe merging The data originates from a study of the effects of articifial light on bird behaviour. It is a subset of the original study for the Chicago area. Citations This data set originates from the publication: Winger BM, Weeks BC, Farnsworth A, Jones AW, Hennen M, Willard DE (2019) Nocturnal flight-calling behaviour predicts vulnerability to artificial light in migratory birds. Proceedings of the Royal Society B 286(1900): 20190364. https://doi.org/10.1098/rspb.2019.0364 To reference only the data, please cite the Dryad data package: Winger BM, Weeks BC, Farnsworth A, Jones AW, Hennen M, Willard DE (2019) Data from: Nocturnal flight-calling behaviour predicts vulnerability to artificial light in migratory birds. Dryad Digital Repository. https://doi.org/10.5061/dryad.8rr0498 import janitor import pandas as pd Get Raw Data Using pandas to import csv data. raw_birds = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/raw/Chicago_collision_data.csv\" ) raw_call = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/raw/bird_call.csv\", sep=\" \", ) raw_light = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/raw/Light_levels_dryad.csv\" ) Original DataFrames Taking a quick look at the three imported (raw) pandas dataframes. raw_birds.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Genus Species Date Locality 0 Ammodramus nelsoni 1982-10-03 MP 1 Ammodramus nelsoni 1984-05-21 CHI 2 Ammodramus nelsoni 1984-05-25 MP 3 Ammodramus nelsoni 1985-10-08 MP 4 Ammodramus nelsoni 1986-09-10 MP raw_call.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Species Family Collisions Flight Call Habitat Stratum 0 Zonotrichia albicollis Passerellidae 10133 Yes Forest Lower 1 Junco hyemalis Passerellidae 6303 Yes Edge Lower 2 Melospiza melodia Passerellidae 5124 Yes Edge Lower 3 Melospiza georgiana Passerellidae 4910 Yes Open Lower 4 Seiurus aurocapilla Parulidae 4580 Yes Forest Lower raw_light.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Light_Score 0 2000-03-06 3 1 2000-03-08 15 2 2000-03-10 3 3 2000-03-31 3 4 2000-04-02 17 Cleaning Data Using Pyjanitor Pyjanitor provides additional method calls to standard pandas dataframe objects. The clean_names() method is one example which removes whitespace and lowercases all column names. clean_light = raw_light.clean_names() clean_light.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date light_score 0 2000-03-06 3 1 2000-03-08 15 2 2000-03-10 3 3 2000-03-31 3 4 2000-04-02 17 Pyjanitor champions the cleaning process using the call chaining approach . We use this here to provide multiple column renaming. As our dataframes have inconsistent column names we rename the columns in the raw_call dataframe. clean_call = raw_call.rename_column( \"Species\", \"Genus\" ).rename_column( # rename 'Species' column to 'Genus' \"Family\", \"Species\" ) # rename 'Family' columnto 'Species' clean_call.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Genus Species Collisions Flight Call Habitat Stratum 0 Zonotrichia albicollis Passerellidae 10133 Yes Forest Lower 1 Junco hyemalis Passerellidae 6303 Yes Edge Lower 2 Melospiza melodia Passerellidae 5124 Yes Edge Lower 3 Melospiza georgiana Passerellidae 4910 Yes Open Lower 4 Seiurus aurocapilla Parulidae 4580 Yes Forest Lower We can chain as many standard pandas commands as we like, along with any pyjanitor specific methods. clean_birds = ( raw_birds.merge( clean_call, how=\"left\" ) # merge the raw_birds dataframe with clean_raw dataframe .select_columns( [ \"Genus\", \"Species\", \"Date\", \"Locality\", \"Collisions\", \"Call\", \"Habitat\", \"Stratum\", ] ) # include list of cols .clean_names() .rename_column( \"collisions\", \"family\" ) # rename 'collisions' column to 'family' in merged dataframe .rename_column(\"call\", \"flight_call\") .dropna() # drop all rows which contain a NaN ) clean_birds.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } genus species date locality family flight_call habitat stratum 89 Passerculus sandwichensis 1978-10-27 MP Passerellidae Yes Open Lower\\t 90 Passerculus sandwichensis 1979-10-23 MP Passerellidae Yes Open Lower\\t 91 Passerculus sandwichensis 1980-04-19 MP Passerellidae Yes Open Lower\\t 92 Passerculus sandwichensis 1981-09-23 MP Passerellidae Yes Open Lower\\t 93 Passerculus sandwichensis 1982-05-20 MP Passerellidae Yes Open Lower\\t","title":"Bird call"},{"location":"bird_call/#processing-bird-call-data","text":"","title":"Processing Bird Call Data"},{"location":"bird_call/#background","text":"The following example was obtained by translating the R code from TidyTuesday 2019-04-30 to Python using Pandas and PyJanitor. It provides a simple example of using pyjanitor for: - column renaming - column name cleaning - dataframe merging The data originates from a study of the effects of articifial light on bird behaviour. It is a subset of the original study for the Chicago area.","title":"Background"},{"location":"bird_call/#citations","text":"This data set originates from the publication: Winger BM, Weeks BC, Farnsworth A, Jones AW, Hennen M, Willard DE (2019) Nocturnal flight-calling behaviour predicts vulnerability to artificial light in migratory birds. Proceedings of the Royal Society B 286(1900): 20190364. https://doi.org/10.1098/rspb.2019.0364 To reference only the data, please cite the Dryad data package: Winger BM, Weeks BC, Farnsworth A, Jones AW, Hennen M, Willard DE (2019) Data from: Nocturnal flight-calling behaviour predicts vulnerability to artificial light in migratory birds. Dryad Digital Repository. https://doi.org/10.5061/dryad.8rr0498 import janitor import pandas as pd","title":"Citations"},{"location":"bird_call/#get-raw-data","text":"Using pandas to import csv data. raw_birds = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/raw/Chicago_collision_data.csv\" ) raw_call = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/raw/bird_call.csv\", sep=\" \", ) raw_light = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-04-30/raw/Light_levels_dryad.csv\" )","title":"Get Raw Data"},{"location":"bird_call/#original-dataframes","text":"Taking a quick look at the three imported (raw) pandas dataframes. raw_birds.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Genus Species Date Locality 0 Ammodramus nelsoni 1982-10-03 MP 1 Ammodramus nelsoni 1984-05-21 CHI 2 Ammodramus nelsoni 1984-05-25 MP 3 Ammodramus nelsoni 1985-10-08 MP 4 Ammodramus nelsoni 1986-09-10 MP raw_call.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Species Family Collisions Flight Call Habitat Stratum 0 Zonotrichia albicollis Passerellidae 10133 Yes Forest Lower 1 Junco hyemalis Passerellidae 6303 Yes Edge Lower 2 Melospiza melodia Passerellidae 5124 Yes Edge Lower 3 Melospiza georgiana Passerellidae 4910 Yes Open Lower 4 Seiurus aurocapilla Parulidae 4580 Yes Forest Lower raw_light.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Date Light_Score 0 2000-03-06 3 1 2000-03-08 15 2 2000-03-10 3 3 2000-03-31 3 4 2000-04-02 17","title":"Original DataFrames"},{"location":"bird_call/#cleaning-data-using-pyjanitor","text":"Pyjanitor provides additional method calls to standard pandas dataframe objects. The clean_names() method is one example which removes whitespace and lowercases all column names. clean_light = raw_light.clean_names() clean_light.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date light_score 0 2000-03-06 3 1 2000-03-08 15 2 2000-03-10 3 3 2000-03-31 3 4 2000-04-02 17 Pyjanitor champions the cleaning process using the call chaining approach . We use this here to provide multiple column renaming. As our dataframes have inconsistent column names we rename the columns in the raw_call dataframe. clean_call = raw_call.rename_column( \"Species\", \"Genus\" ).rename_column( # rename 'Species' column to 'Genus' \"Family\", \"Species\" ) # rename 'Family' columnto 'Species' clean_call.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Genus Species Collisions Flight Call Habitat Stratum 0 Zonotrichia albicollis Passerellidae 10133 Yes Forest Lower 1 Junco hyemalis Passerellidae 6303 Yes Edge Lower 2 Melospiza melodia Passerellidae 5124 Yes Edge Lower 3 Melospiza georgiana Passerellidae 4910 Yes Open Lower 4 Seiurus aurocapilla Parulidae 4580 Yes Forest Lower We can chain as many standard pandas commands as we like, along with any pyjanitor specific methods. clean_birds = ( raw_birds.merge( clean_call, how=\"left\" ) # merge the raw_birds dataframe with clean_raw dataframe .select_columns( [ \"Genus\", \"Species\", \"Date\", \"Locality\", \"Collisions\", \"Call\", \"Habitat\", \"Stratum\", ] ) # include list of cols .clean_names() .rename_column( \"collisions\", \"family\" ) # rename 'collisions' column to 'family' in merged dataframe .rename_column(\"call\", \"flight_call\") .dropna() # drop all rows which contain a NaN ) clean_birds.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } genus species date locality family flight_call habitat stratum 89 Passerculus sandwichensis 1978-10-27 MP Passerellidae Yes Open Lower\\t 90 Passerculus sandwichensis 1979-10-23 MP Passerellidae Yes Open Lower\\t 91 Passerculus sandwichensis 1980-04-19 MP Passerellidae Yes Open Lower\\t 92 Passerculus sandwichensis 1981-09-23 MP Passerellidae Yes Open Lower\\t 93 Passerculus sandwichensis 1982-05-20 MP Passerellidae Yes Open Lower\\t","title":"Cleaning Data Using Pyjanitor"},{"location":"board_games/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import janitor import pandas as pd import seaborn as sns Processing Board Game Data Background This dataset comes from the Board Game Geek database . The site's database has more than 90,000 games, with crowd-sourced ratings. This particular subset is limited to only games with at least 50 ratings which were published between 1950 and 2016. This still leaves us with 10,532 games! For more information please check out the tidytuesday repo which is where this example was taken from. Data Cleaning %matplotlib inline One-Shot This cell demonstrates the cleaning process using the call chaining approach championed in pyjanitor cleaned_df = ( pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12//board_games.csv\" # noqa: E501 ) # ingest raw data .clean_names() # removes whitespace, punctuation/symbols, capitalization .remove_empty() # removes entirely empty rows / columns .drop( columns=[\"image\", \"thumbnail\", \"compilation\", \"game_id\"] ) # drops unnecessary columns ) Multi-Step These cells repeat the process in a step-by-step manner in order to explain it in more detail Read in the csv df = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv\" # noqa: E501 ) df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } game_id description image max_players max_playtime min_age min_players min_playtime name playing_time ... artist category compilation designer expansion family mechanic publisher average_rating users_rated 0 1 Die Macher is a game about seven sequential po... //cf.geekdo-images.com/images/pic159509.jpg 5 240 14 3 240 Die Macher 240 ... Marcus Gschwendtner Economic,Negotiation,Political NaN Karl-Heinz Schmiel NaN Country: Germany,Valley Games Classic Line Area Control / Area Influence,Auction/Bidding,... Hans im Gl\u00fcck Verlags-GmbH,Moskito Spiele,Vall... 7.66508 4498 1 2 Dragonmaster is a trick-taking card game based... //cf.geekdo-images.com/images/pic184174.jpg 4 30 12 3 30 Dragonmaster 30 ... Bob Pepper Card Game,Fantasy NaN G. W. \"Jerry\" D'Arcey NaN Animals: Dragons Trick-taking E.S. Lowe,Milton Bradley 6.60815 478 2 3 Part of the Knizia tile-laying trilogy, Samura... //cf.geekdo-images.com/images/pic3211873.jpg 4 60 10 2 30 Samurai 60 ... Franz Vohwinkel Abstract Strategy,Medieval NaN Reiner Knizia NaN Asian Theme,Country: Japan,Knizia tile-laying ... Area Control / Area Influence,Hand Management,... 999 Games,ABACUSSPIELE,Astrel Games,Ceilikan J... 7.44119 12019 3 rows \u00d7 22 columns Remove the whitespace, punctuation/symbols, and capitalization form columns df = df.clean_names() df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } game_id description image max_players max_playtime min_age min_players min_playtime name playing_time ... artist category compilation designer expansion family mechanic publisher average_rating users_rated 0 1 Die Macher is a game about seven sequential po... //cf.geekdo-images.com/images/pic159509.jpg 5 240 14 3 240 Die Macher 240 ... Marcus Gschwendtner Economic,Negotiation,Political NaN Karl-Heinz Schmiel NaN Country: Germany,Valley Games Classic Line Area Control / Area Influence,Auction/Bidding,... Hans im Gl\u00fcck Verlags-GmbH,Moskito Spiele,Vall... 7.66508 4498 1 2 Dragonmaster is a trick-taking card game based... //cf.geekdo-images.com/images/pic184174.jpg 4 30 12 3 30 Dragonmaster 30 ... Bob Pepper Card Game,Fantasy NaN G. W. \"Jerry\" D'Arcey NaN Animals: Dragons Trick-taking E.S. Lowe,Milton Bradley 6.60815 478 2 3 Part of the Knizia tile-laying trilogy, Samura... //cf.geekdo-images.com/images/pic3211873.jpg 4 60 10 2 30 Samurai 60 ... Franz Vohwinkel Abstract Strategy,Medieval NaN Reiner Knizia NaN Asian Theme,Country: Japan,Knizia tile-laying ... Area Control / Area Influence,Hand Management,... 999 Games,ABACUSSPIELE,Astrel Games,Ceilikan J... 7.44119 12019 3 rows \u00d7 22 columns Remove all the empty rows and columns if present df = df.remove_empty() df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } game_id description image max_players max_playtime min_age min_players min_playtime name playing_time ... artist category compilation designer expansion family mechanic publisher average_rating users_rated 0 1 Die Macher is a game about seven sequential po... //cf.geekdo-images.com/images/pic159509.jpg 5 240 14 3 240 Die Macher 240 ... Marcus Gschwendtner Economic,Negotiation,Political NaN Karl-Heinz Schmiel NaN Country: Germany,Valley Games Classic Line Area Control / Area Influence,Auction/Bidding,... Hans im Gl\u00fcck Verlags-GmbH,Moskito Spiele,Vall... 7.66508 4498 1 2 Dragonmaster is a trick-taking card game based... //cf.geekdo-images.com/images/pic184174.jpg 4 30 12 3 30 Dragonmaster 30 ... Bob Pepper Card Game,Fantasy NaN G. W. \"Jerry\" D'Arcey NaN Animals: Dragons Trick-taking E.S. Lowe,Milton Bradley 6.60815 478 2 3 Part of the Knizia tile-laying trilogy, Samura... //cf.geekdo-images.com/images/pic3211873.jpg 4 60 10 2 30 Samurai 60 ... Franz Vohwinkel Abstract Strategy,Medieval NaN Reiner Knizia NaN Asian Theme,Country: Japan,Knizia tile-laying ... Area Control / Area Influence,Hand Management,... 999 Games,ABACUSSPIELE,Astrel Games,Ceilikan J... 7.44119 12019 3 rows \u00d7 22 columns Check to see whether \"min_playtime\" and \"max_playtime\" columns are equal len(df[df[\"min_playtime\"] != df[\"max_playtime\"]]) 1565 Check to see what percentage of the values in the \"compilation\" column are not null len(df[df[\"compilation\"].notnull()]) / len(df) 0.03892897835169009 Drop unnecessary columns The 'compilation' column was demonstrated to have little value, the \"image\" and \"thumbnail\" columns link to images and are not a factor in this analysis. The \"game_id\" column can be replaced by using the index. df = df.drop(columns=[\"image\", \"thumbnail\", \"compilation\", \"game_id\"]) df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } description max_players max_playtime min_age min_players min_playtime name playing_time year_published artist category designer expansion family mechanic publisher average_rating users_rated 0 Die Macher is a game about seven sequential po... 5 240 14 3 240 Die Macher 240 1986 Marcus Gschwendtner Economic,Negotiation,Political Karl-Heinz Schmiel NaN Country: Germany,Valley Games Classic Line Area Control / Area Influence,Auction/Bidding,... Hans im Gl\u00fcck Verlags-GmbH,Moskito Spiele,Vall... 7.66508 4498 1 Dragonmaster is a trick-taking card game based... 4 30 12 3 30 Dragonmaster 30 1981 Bob Pepper Card Game,Fantasy G. W. \"Jerry\" D'Arcey NaN Animals: Dragons Trick-taking E.S. Lowe,Milton Bradley 6.60815 478 2 Part of the Knizia tile-laying trilogy, Samura... 4 60 10 2 30 Samurai 60 1998 Franz Vohwinkel Abstract Strategy,Medieval Reiner Knizia NaN Asian Theme,Country: Japan,Knizia tile-laying ... Area Control / Area Influence,Hand Management,... 999 Games,ABACUSSPIELE,Astrel Games,Ceilikan J... 7.44119 12019 Sample Analysis What Categories appear most often? df[\"category\"].value_counts().head(10) Wargame,World War II 449 Card Game 438 Abstract Strategy 284 Napoleonic,Wargame 124 Economic 116 Card Game,Fantasy 110 Dice 107 American Civil War,Wargame 97 Modern Warfare,Wargame 89 Party Game 77 Name: category, dtype: int64 What is the relationship between games' player numbers, reccomended minimum age, and the game's estimated length? sns.pairplot( df, x_vars=[\"min_age\", \"min_players\", \"min_playtime\"], y_vars=\"users_rated\", height=7, aspect=0.7, ); Preliminary analysis Without digging into the data too much more it becomes apparent that there are some entries that were improperly entered e.g. having a minimum playtime of 60000 minutes. Otherwise we see some nice bell curves.","title":"Board games"},{"location":"board_games/#processing-board-game-data","text":"","title":"Processing Board Game Data"},{"location":"board_games/#background","text":"This dataset comes from the Board Game Geek database . The site's database has more than 90,000 games, with crowd-sourced ratings. This particular subset is limited to only games with at least 50 ratings which were published between 1950 and 2016. This still leaves us with 10,532 games! For more information please check out the tidytuesday repo which is where this example was taken from.","title":"Background"},{"location":"board_games/#data-cleaning","text":"%matplotlib inline","title":"Data Cleaning"},{"location":"board_games/#one-shot","text":"This cell demonstrates the cleaning process using the call chaining approach championed in pyjanitor cleaned_df = ( pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12//board_games.csv\" # noqa: E501 ) # ingest raw data .clean_names() # removes whitespace, punctuation/symbols, capitalization .remove_empty() # removes entirely empty rows / columns .drop( columns=[\"image\", \"thumbnail\", \"compilation\", \"game_id\"] ) # drops unnecessary columns )","title":"One-Shot"},{"location":"board_games/#multi-step","text":"These cells repeat the process in a step-by-step manner in order to explain it in more detail","title":"Multi-Step"},{"location":"board_games/#read-in-the-csv","text":"df = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv\" # noqa: E501 ) df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } game_id description image max_players max_playtime min_age min_players min_playtime name playing_time ... artist category compilation designer expansion family mechanic publisher average_rating users_rated 0 1 Die Macher is a game about seven sequential po... //cf.geekdo-images.com/images/pic159509.jpg 5 240 14 3 240 Die Macher 240 ... Marcus Gschwendtner Economic,Negotiation,Political NaN Karl-Heinz Schmiel NaN Country: Germany,Valley Games Classic Line Area Control / Area Influence,Auction/Bidding,... Hans im Gl\u00fcck Verlags-GmbH,Moskito Spiele,Vall... 7.66508 4498 1 2 Dragonmaster is a trick-taking card game based... //cf.geekdo-images.com/images/pic184174.jpg 4 30 12 3 30 Dragonmaster 30 ... Bob Pepper Card Game,Fantasy NaN G. W. \"Jerry\" D'Arcey NaN Animals: Dragons Trick-taking E.S. Lowe,Milton Bradley 6.60815 478 2 3 Part of the Knizia tile-laying trilogy, Samura... //cf.geekdo-images.com/images/pic3211873.jpg 4 60 10 2 30 Samurai 60 ... Franz Vohwinkel Abstract Strategy,Medieval NaN Reiner Knizia NaN Asian Theme,Country: Japan,Knizia tile-laying ... Area Control / Area Influence,Hand Management,... 999 Games,ABACUSSPIELE,Astrel Games,Ceilikan J... 7.44119 12019 3 rows \u00d7 22 columns","title":"Read in the csv"},{"location":"board_games/#remove-the-whitespace-punctuationsymbols-and-capitalization-form-columns","text":"df = df.clean_names() df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } game_id description image max_players max_playtime min_age min_players min_playtime name playing_time ... artist category compilation designer expansion family mechanic publisher average_rating users_rated 0 1 Die Macher is a game about seven sequential po... //cf.geekdo-images.com/images/pic159509.jpg 5 240 14 3 240 Die Macher 240 ... Marcus Gschwendtner Economic,Negotiation,Political NaN Karl-Heinz Schmiel NaN Country: Germany,Valley Games Classic Line Area Control / Area Influence,Auction/Bidding,... Hans im Gl\u00fcck Verlags-GmbH,Moskito Spiele,Vall... 7.66508 4498 1 2 Dragonmaster is a trick-taking card game based... //cf.geekdo-images.com/images/pic184174.jpg 4 30 12 3 30 Dragonmaster 30 ... Bob Pepper Card Game,Fantasy NaN G. W. \"Jerry\" D'Arcey NaN Animals: Dragons Trick-taking E.S. Lowe,Milton Bradley 6.60815 478 2 3 Part of the Knizia tile-laying trilogy, Samura... //cf.geekdo-images.com/images/pic3211873.jpg 4 60 10 2 30 Samurai 60 ... Franz Vohwinkel Abstract Strategy,Medieval NaN Reiner Knizia NaN Asian Theme,Country: Japan,Knizia tile-laying ... Area Control / Area Influence,Hand Management,... 999 Games,ABACUSSPIELE,Astrel Games,Ceilikan J... 7.44119 12019 3 rows \u00d7 22 columns","title":"Remove the whitespace, punctuation/symbols, and capitalization  form columns"},{"location":"board_games/#remove-all-the-empty-rows-and-columns-if-present","text":"df = df.remove_empty() df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } game_id description image max_players max_playtime min_age min_players min_playtime name playing_time ... artist category compilation designer expansion family mechanic publisher average_rating users_rated 0 1 Die Macher is a game about seven sequential po... //cf.geekdo-images.com/images/pic159509.jpg 5 240 14 3 240 Die Macher 240 ... Marcus Gschwendtner Economic,Negotiation,Political NaN Karl-Heinz Schmiel NaN Country: Germany,Valley Games Classic Line Area Control / Area Influence,Auction/Bidding,... Hans im Gl\u00fcck Verlags-GmbH,Moskito Spiele,Vall... 7.66508 4498 1 2 Dragonmaster is a trick-taking card game based... //cf.geekdo-images.com/images/pic184174.jpg 4 30 12 3 30 Dragonmaster 30 ... Bob Pepper Card Game,Fantasy NaN G. W. \"Jerry\" D'Arcey NaN Animals: Dragons Trick-taking E.S. Lowe,Milton Bradley 6.60815 478 2 3 Part of the Knizia tile-laying trilogy, Samura... //cf.geekdo-images.com/images/pic3211873.jpg 4 60 10 2 30 Samurai 60 ... Franz Vohwinkel Abstract Strategy,Medieval NaN Reiner Knizia NaN Asian Theme,Country: Japan,Knizia tile-laying ... Area Control / Area Influence,Hand Management,... 999 Games,ABACUSSPIELE,Astrel Games,Ceilikan J... 7.44119 12019 3 rows \u00d7 22 columns","title":"Remove all the empty rows and columns if present"},{"location":"board_games/#check-to-see-whether-min_playtime-and-max_playtime-columns-are-equal","text":"len(df[df[\"min_playtime\"] != df[\"max_playtime\"]]) 1565","title":"Check to see whether \"min_playtime\" and \"max_playtime\" columns are equal"},{"location":"board_games/#check-to-see-what-percentage-of-the-values-in-the-compilation-column-are-not-null","text":"len(df[df[\"compilation\"].notnull()]) / len(df) 0.03892897835169009","title":"Check to see what percentage of the values in the \"compilation\" column are not null"},{"location":"board_games/#drop-unnecessary-columns","text":"The 'compilation' column was demonstrated to have little value, the \"image\" and \"thumbnail\" columns link to images and are not a factor in this analysis. The \"game_id\" column can be replaced by using the index. df = df.drop(columns=[\"image\", \"thumbnail\", \"compilation\", \"game_id\"]) df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } description max_players max_playtime min_age min_players min_playtime name playing_time year_published artist category designer expansion family mechanic publisher average_rating users_rated 0 Die Macher is a game about seven sequential po... 5 240 14 3 240 Die Macher 240 1986 Marcus Gschwendtner Economic,Negotiation,Political Karl-Heinz Schmiel NaN Country: Germany,Valley Games Classic Line Area Control / Area Influence,Auction/Bidding,... Hans im Gl\u00fcck Verlags-GmbH,Moskito Spiele,Vall... 7.66508 4498 1 Dragonmaster is a trick-taking card game based... 4 30 12 3 30 Dragonmaster 30 1981 Bob Pepper Card Game,Fantasy G. W. \"Jerry\" D'Arcey NaN Animals: Dragons Trick-taking E.S. Lowe,Milton Bradley 6.60815 478 2 Part of the Knizia tile-laying trilogy, Samura... 4 60 10 2 30 Samurai 60 1998 Franz Vohwinkel Abstract Strategy,Medieval Reiner Knizia NaN Asian Theme,Country: Japan,Knizia tile-laying ... Area Control / Area Influence,Hand Management,... 999 Games,ABACUSSPIELE,Astrel Games,Ceilikan J... 7.44119 12019","title":"Drop unnecessary columns"},{"location":"board_games/#sample-analysis","text":"","title":"Sample Analysis"},{"location":"board_games/#what-categories-appear-most-often","text":"df[\"category\"].value_counts().head(10) Wargame,World War II 449 Card Game 438 Abstract Strategy 284 Napoleonic,Wargame 124 Economic 116 Card Game,Fantasy 110 Dice 107 American Civil War,Wargame 97 Modern Warfare,Wargame 89 Party Game 77 Name: category, dtype: int64","title":"What Categories appear most often?"},{"location":"board_games/#what-is-the-relationship-between-games-player-numbers-reccomended-minimum-age-and-the-games-estimated-length","text":"sns.pairplot( df, x_vars=[\"min_age\", \"min_players\", \"min_playtime\"], y_vars=\"users_rated\", height=7, aspect=0.7, );","title":"What is the relationship between games' player numbers, reccomended minimum age, and the game's estimated length?"},{"location":"board_games/#preliminary-analysis","text":"Without digging into the data too much more it becomes apparent that there are some entries that were improperly entered e.g. having a minimum playtime of 60000 minutes. Otherwise we see some nice bell curves.","title":"Preliminary analysis"},{"location":"case_when/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Multiple Conditions with case_when import janitor import pandas as pd from janitor.functions import case_when janitor.__version__ '0.22.0' # https://stackoverflow.com/q/19913659/7175713 df = pd.DataFrame({\"col1\": list(\"ABBC\"), \"col2\": list(\"ZZXY\")}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 0 A Z 1 B Z 2 B X 3 C Y Single Condition: df.case_when( df.col1 == \"Z\", # condition \"green\", # value if True \"red\", # value if False column_name=\"color\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 color 0 A Z red 1 B Z red 2 B X red 3 C Y red Multiple Conditions: df.case_when( df.col2.eq(\"Z\") & df.col1.eq(\"A\"), \"yellow\", # first condition and value df.col2.eq(\"Z\") & df.col1.eq(\"B\"), \"blue\", # second condition and value df.col1.eq(\"B\"), \"purple\", # third condition and value \"black\", # default if no condition is True column_name=\"color\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 color 0 A Z yellow 1 B Z blue 2 B X purple 3 C Y black Anonymous functions (lambda) are supported as well: # https://stackoverflow.com/q/43391591/7175713 raw_data = {\"age1\": [23, 45, 21], \"age2\": [10, 20, 50]} df = pd.DataFrame(raw_data, columns=[\"age1\", \"age2\"]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age1 age2 0 23 10 1 45 20 2 21 50 df.case_when( lambda df: (df.age1 - df.age2) > 0, # condition lambda df: df.age1 - df.age2, # value if True lambda df: df.age2 - df.age1, # default if False column_name=\"diff\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age1 age2 diff 0 23 10 13 1 45 20 25 2 21 50 29 data types are preserved; under the hood it uses pd.Series.mask : df = df.astype(\"Int64\") df.dtypes age1 Int64 age2 Int64 dtype: object result = df.case_when( lambda df: (df.age1 - df.age2) > 0, lambda df: df.age1 - df.age2, lambda df: df.age2 - df.age1, column_name=\"diff\", ) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age1 age2 diff 0 23 10 13 1 45 20 25 2 21 50 29 result.dtypes age1 Int64 age2 Int64 diff Int64 dtype: object The conditions can be a string, as long as they can be evaluated with pd.eval on the DataFrame, and return a boolean array: # https://stackoverflow.com/q/54653356/7175713 data = { \"name\": [\"Jason\", \"Molly\", \"Tina\", \"Jake\", \"Amy\"], \"age\": [42, 52, 36, 24, 73], \"preTestScore\": [4, 24, 31, 2, 3], \"postTestScore\": [25, 94, 57, 62, 70], } df = pd.DataFrame(data, columns=[\"name\", \"age\", \"preTestScore\", \"postTestScore\"]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age preTestScore postTestScore 0 Jason 42 4 25 1 Molly 52 24 94 2 Tina 36 31 57 3 Jake 24 2 62 4 Amy 73 3 70 df.case_when( \"age < 10\", \"baby\", \"10 <= age < 20\", \"kid\", \"20 <= age < 30\", \"young\", \"30 <= age < 50\", \"mature\", \"grandpa\", column_name=\"elderly\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age preTestScore postTestScore elderly 0 Jason 42 4 25 mature 1 Molly 52 24 94 grandpa 2 Tina 36 31 57 mature 3 Jake 24 2 62 young 4 Amy 73 3 70 grandpa When multiple conditions are satisfied, the first one is used: df = range(3, 30, 3) df = pd.DataFrame(df, columns=[\"odd\"]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } odd 0 3 1 6 2 9 3 12 4 15 5 18 6 21 7 24 8 27 df.case_when( df.odd % 9 == 0, \"divisible by 9\", \"divisible by 3\", column_name=\"div_by_3_or_9\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } odd div_by_3_or_9 0 3 divisible by 3 1 6 divisible by 3 2 9 divisible by 9 3 12 divisible by 3 4 15 divisible by 3 5 18 divisible by 9 6 21 divisible by 3 7 24 divisible by 3 8 27 divisible by 9 lines 2, 5 and 8 are divisible by 3; however, because the first condition tests if it is divisible by 9, that outcome is used instead. If column_name exists in the DataFrame, then that column's values will be replaced with the outcome of case_when : df.case_when(df.odd % 9 == 0, \"divisible by 9\", \"divisible by 3\", column_name=\"odd\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } odd 0 divisible by 3 1 divisible by 3 2 divisible by 9 3 divisible by 3 4 divisible by 3 5 divisible by 9 6 divisible by 3 7 divisible by 3 8 divisible by 9","title":"Case when"},{"location":"case_when/#multiple-conditions-with-case_when","text":"import janitor import pandas as pd from janitor.functions import case_when janitor.__version__ '0.22.0' # https://stackoverflow.com/q/19913659/7175713 df = pd.DataFrame({\"col1\": list(\"ABBC\"), \"col2\": list(\"ZZXY\")}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 0 A Z 1 B Z 2 B X 3 C Y Single Condition: df.case_when( df.col1 == \"Z\", # condition \"green\", # value if True \"red\", # value if False column_name=\"color\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 color 0 A Z red 1 B Z red 2 B X red 3 C Y red Multiple Conditions: df.case_when( df.col2.eq(\"Z\") & df.col1.eq(\"A\"), \"yellow\", # first condition and value df.col2.eq(\"Z\") & df.col1.eq(\"B\"), \"blue\", # second condition and value df.col1.eq(\"B\"), \"purple\", # third condition and value \"black\", # default if no condition is True column_name=\"color\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 color 0 A Z yellow 1 B Z blue 2 B X purple 3 C Y black Anonymous functions (lambda) are supported as well: # https://stackoverflow.com/q/43391591/7175713 raw_data = {\"age1\": [23, 45, 21], \"age2\": [10, 20, 50]} df = pd.DataFrame(raw_data, columns=[\"age1\", \"age2\"]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age1 age2 0 23 10 1 45 20 2 21 50 df.case_when( lambda df: (df.age1 - df.age2) > 0, # condition lambda df: df.age1 - df.age2, # value if True lambda df: df.age2 - df.age1, # default if False column_name=\"diff\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age1 age2 diff 0 23 10 13 1 45 20 25 2 21 50 29 data types are preserved; under the hood it uses pd.Series.mask : df = df.astype(\"Int64\") df.dtypes age1 Int64 age2 Int64 dtype: object result = df.case_when( lambda df: (df.age1 - df.age2) > 0, lambda df: df.age1 - df.age2, lambda df: df.age2 - df.age1, column_name=\"diff\", ) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age1 age2 diff 0 23 10 13 1 45 20 25 2 21 50 29 result.dtypes age1 Int64 age2 Int64 diff Int64 dtype: object The conditions can be a string, as long as they can be evaluated with pd.eval on the DataFrame, and return a boolean array: # https://stackoverflow.com/q/54653356/7175713 data = { \"name\": [\"Jason\", \"Molly\", \"Tina\", \"Jake\", \"Amy\"], \"age\": [42, 52, 36, 24, 73], \"preTestScore\": [4, 24, 31, 2, 3], \"postTestScore\": [25, 94, 57, 62, 70], } df = pd.DataFrame(data, columns=[\"name\", \"age\", \"preTestScore\", \"postTestScore\"]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age preTestScore postTestScore 0 Jason 42 4 25 1 Molly 52 24 94 2 Tina 36 31 57 3 Jake 24 2 62 4 Amy 73 3 70 df.case_when( \"age < 10\", \"baby\", \"10 <= age < 20\", \"kid\", \"20 <= age < 30\", \"young\", \"30 <= age < 50\", \"mature\", \"grandpa\", column_name=\"elderly\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name age preTestScore postTestScore elderly 0 Jason 42 4 25 mature 1 Molly 52 24 94 grandpa 2 Tina 36 31 57 mature 3 Jake 24 2 62 young 4 Amy 73 3 70 grandpa When multiple conditions are satisfied, the first one is used: df = range(3, 30, 3) df = pd.DataFrame(df, columns=[\"odd\"]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } odd 0 3 1 6 2 9 3 12 4 15 5 18 6 21 7 24 8 27 df.case_when( df.odd % 9 == 0, \"divisible by 9\", \"divisible by 3\", column_name=\"div_by_3_or_9\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } odd div_by_3_or_9 0 3 divisible by 3 1 6 divisible by 3 2 9 divisible by 9 3 12 divisible by 3 4 15 divisible by 3 5 18 divisible by 9 6 21 divisible by 3 7 24 divisible by 3 8 27 divisible by 9 lines 2, 5 and 8 are divisible by 3; however, because the first condition tests if it is divisible by 9, that outcome is used instead. If column_name exists in the DataFrame, then that column's values will be replaced with the outcome of case_when : df.case_when(df.odd % 9 == 0, \"divisible by 9\", \"divisible by 3\", column_name=\"odd\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } odd 0 divisible by 3 1 divisible by 3 2 divisible by 9 3 divisible by 3 4 divisible by 3 5 divisible by 9 6 divisible by 3 7 divisible by 3 8 divisible by 9","title":"Multiple Conditions with case_when"},{"location":"complete/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Expose explicitly missing values with complete import janitor import numpy as np import pandas as pd # from http://imachordata.com/2016/02/05/you-complete-me/ df = pd.DataFrame( { \"Year\": [1999, 2000, 2004, 1999, 2004], \"Taxon\": [ \"Saccharina\", \"Saccharina\", \"Saccharina\", \"Agarum\", \"Agarum\", ], \"Abundance\": [4, 5, 2, 1, 8], } ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Saccharina 4 1 2000 Saccharina 5 2 2004 Saccharina 2 3 1999 Agarum 1 4 2004 Agarum 8 Note that Year 2000 and Agarum pairing is missing in the DataFrame above. Let\u2019s make it explicit: df.complete(\"Year\", \"Taxon\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Saccharina 4.0 1 2000 Saccharina 5.0 2 2004 Saccharina 2.0 3 1999 Agarum 1.0 4 2004 Agarum 8.0 5 2000 Agarum NaN # A better viewing based on order df.complete(\"Year\", \"Taxon\", sort=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2004 Agarum 8.0 5 2004 Saccharina 2.0 What if we wanted the explicit missing values for all the years from 1999 to 2004? Easy - simply pass a dictionary pairing the column name with the new values: new_year_values = {\"Year\": range(df.Year.min(), df.Year.max() + 1)} df.complete(new_year_values, \"Taxon\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Saccharina 4.0 1 2000 Saccharina 5.0 2 2004 Saccharina 2.0 3 1999 Agarum 1.0 4 2004 Agarum 8.0 5 2000 Agarum NaN 6 2001 Saccharina NaN 7 2001 Agarum NaN 8 2002 Saccharina NaN 9 2002 Agarum NaN 10 2003 Saccharina NaN 11 2003 Agarum NaN You can pass a callable as values in the dictionary: def new_year_values(year): return range(year.min(), year.max() + 1) df.complete({\"Year\": new_year_values}, \"Taxon\", sort=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2001 Agarum NaN 5 2001 Saccharina NaN 6 2002 Agarum NaN 7 2002 Saccharina NaN 8 2003 Agarum NaN 9 2003 Saccharina NaN 10 2004 Agarum 8.0 11 2004 Saccharina 2.0 You can get explcit rows, based only on existing data: # https://stackoverflow.com/q/62266057/7175713 df = { \"Name\": (\"Bob\", \"Bob\", \"Emma\"), \"Age\": (23, 23, 78), \"Gender\": (\"Male\", \"Male\", \"Female\"), \"Item\": (\"house\", \"car\", \"house\"), \"Value\": (5, 1, 3), } df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Age Gender Item Value 0 Bob 23 Male house 5 1 Bob 23 Male car 1 2 Emma 78 Female house 3 In the DataFrame above, there is no car Item value for the Name , Age , Gender combination -> (Emma, 78, Female) . Pass (Name, Age, Gender) and Item to explicitly expose the missing row: df.complete((\"Name\", \"Age\", \"Gender\"), \"Item\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Age Gender Item Value 0 Bob 23 Male house 5.0 1 Bob 23 Male car 1.0 2 Emma 78 Female house 3.0 3 Emma 78 Female car NaN The example above showed how to expose missing rows on a group basis. There is also the option of exposing missing rows with the by parameter: df = pd.DataFrame( { \"state\": [\"CA\", \"CA\", \"HI\", \"HI\", \"HI\", \"NY\", \"NY\"], \"year\": [2010, 2013, 2010, 2012, 2016, 2009, 2013], \"value\": [1, 3, 1, 2, 3, 2, 5], } ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year value 0 CA 2010 1 1 CA 2013 3 2 HI 2010 1 3 HI 2012 2 4 HI 2016 3 5 NY 2009 2 6 NY 2013 5 Let's expose all the missing years, based on the minimum and maximum year, for each state: result = df.complete({\"year\": new_year_values}, by=\"state\", sort=True) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year value 0 CA 2010 1.0 1 CA 2011 NaN 2 CA 2012 NaN 3 CA 2013 3.0 4 HI 2010 1.0 5 HI 2011 NaN 6 HI 2012 2.0 7 HI 2013 NaN 8 HI 2014 NaN 9 HI 2015 NaN 10 HI 2016 3.0 11 NY 2009 2.0 12 NY 2010 NaN 13 NY 2011 NaN 14 NY 2012 NaN 15 NY 2013 5.0 You can fill the nulls with Pandas' fillna : result.fillna(0, downcast=\"infer\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year value 0 CA 2010 1 1 CA 2011 0 2 CA 2012 0 3 CA 2013 3 4 HI 2010 1 5 HI 2011 0 6 HI 2012 2 7 HI 2013 0 8 HI 2014 0 9 HI 2015 0 10 HI 2016 3 11 NY 2009 2 12 NY 2010 0 13 NY 2011 0 14 NY 2012 0 15 NY 2013 5","title":"Complete"},{"location":"complete/#expose-explicitly-missing-values-with-complete","text":"import janitor import numpy as np import pandas as pd # from http://imachordata.com/2016/02/05/you-complete-me/ df = pd.DataFrame( { \"Year\": [1999, 2000, 2004, 1999, 2004], \"Taxon\": [ \"Saccharina\", \"Saccharina\", \"Saccharina\", \"Agarum\", \"Agarum\", ], \"Abundance\": [4, 5, 2, 1, 8], } ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Saccharina 4 1 2000 Saccharina 5 2 2004 Saccharina 2 3 1999 Agarum 1 4 2004 Agarum 8 Note that Year 2000 and Agarum pairing is missing in the DataFrame above. Let\u2019s make it explicit: df.complete(\"Year\", \"Taxon\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Saccharina 4.0 1 2000 Saccharina 5.0 2 2004 Saccharina 2.0 3 1999 Agarum 1.0 4 2004 Agarum 8.0 5 2000 Agarum NaN # A better viewing based on order df.complete(\"Year\", \"Taxon\", sort=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2004 Agarum 8.0 5 2004 Saccharina 2.0 What if we wanted the explicit missing values for all the years from 1999 to 2004? Easy - simply pass a dictionary pairing the column name with the new values: new_year_values = {\"Year\": range(df.Year.min(), df.Year.max() + 1)} df.complete(new_year_values, \"Taxon\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Saccharina 4.0 1 2000 Saccharina 5.0 2 2004 Saccharina 2.0 3 1999 Agarum 1.0 4 2004 Agarum 8.0 5 2000 Agarum NaN 6 2001 Saccharina NaN 7 2001 Agarum NaN 8 2002 Saccharina NaN 9 2002 Agarum NaN 10 2003 Saccharina NaN 11 2003 Agarum NaN You can pass a callable as values in the dictionary: def new_year_values(year): return range(year.min(), year.max() + 1) df.complete({\"Year\": new_year_values}, \"Taxon\", sort=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Taxon Abundance 0 1999 Agarum 1.0 1 1999 Saccharina 4.0 2 2000 Agarum NaN 3 2000 Saccharina 5.0 4 2001 Agarum NaN 5 2001 Saccharina NaN 6 2002 Agarum NaN 7 2002 Saccharina NaN 8 2003 Agarum NaN 9 2003 Saccharina NaN 10 2004 Agarum 8.0 11 2004 Saccharina 2.0 You can get explcit rows, based only on existing data: # https://stackoverflow.com/q/62266057/7175713 df = { \"Name\": (\"Bob\", \"Bob\", \"Emma\"), \"Age\": (23, 23, 78), \"Gender\": (\"Male\", \"Male\", \"Female\"), \"Item\": (\"house\", \"car\", \"house\"), \"Value\": (5, 1, 3), } df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Age Gender Item Value 0 Bob 23 Male house 5 1 Bob 23 Male car 1 2 Emma 78 Female house 3 In the DataFrame above, there is no car Item value for the Name , Age , Gender combination -> (Emma, 78, Female) . Pass (Name, Age, Gender) and Item to explicitly expose the missing row: df.complete((\"Name\", \"Age\", \"Gender\"), \"Item\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Name Age Gender Item Value 0 Bob 23 Male house 5.0 1 Bob 23 Male car 1.0 2 Emma 78 Female house 3.0 3 Emma 78 Female car NaN The example above showed how to expose missing rows on a group basis. There is also the option of exposing missing rows with the by parameter: df = pd.DataFrame( { \"state\": [\"CA\", \"CA\", \"HI\", \"HI\", \"HI\", \"NY\", \"NY\"], \"year\": [2010, 2013, 2010, 2012, 2016, 2009, 2013], \"value\": [1, 3, 1, 2, 3, 2, 5], } ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year value 0 CA 2010 1 1 CA 2013 3 2 HI 2010 1 3 HI 2012 2 4 HI 2016 3 5 NY 2009 2 6 NY 2013 5 Let's expose all the missing years, based on the minimum and maximum year, for each state: result = df.complete({\"year\": new_year_values}, by=\"state\", sort=True) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year value 0 CA 2010 1.0 1 CA 2011 NaN 2 CA 2012 NaN 3 CA 2013 3.0 4 HI 2010 1.0 5 HI 2011 NaN 6 HI 2012 2.0 7 HI 2013 NaN 8 HI 2014 NaN 9 HI 2015 NaN 10 HI 2016 3.0 11 NY 2009 2.0 12 NY 2010 NaN 13 NY 2011 NaN 14 NY 2012 NaN 15 NY 2013 5.0 You can fill the nulls with Pandas' fillna : result.fillna(0, downcast=\"infer\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } state year value 0 CA 2010 1 1 CA 2011 0 2 CA 2012 0 3 CA 2013 3 4 HI 2010 1 5 HI 2011 0 6 HI 2012 2 7 HI 2013 0 8 HI 2014 0 9 HI 2015 0 10 HI 2016 3 11 NY 2009 2 12 NY 2010 0 13 NY 2011 0 14 NY 2012 0 15 NY 2013 5","title":"Expose explicitly missing values with complete"},{"location":"conditional_join/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Joining on Non-Equi Operators import janitor import pandas as pd # https://stackoverflow.com/q/61948103/7175713 df1 = pd.DataFrame({\"id\": [1, 1, 1, 2, 2, 3], \"value_1\": [2, 5, 7, 1, 3, 4]}) df2 = pd.DataFrame( { \"id\": [1, 1, 1, 1, 2, 2, 2, 3], \"value_2A\": [0, 3, 7, 12, 0, 2, 3, 1], \"value_2B\": [1, 5, 9, 15, 1, 4, 6, 3], } ) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id value_1 0 1 2 1 1 5 2 1 7 3 2 1 4 2 3 5 3 4 df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id value_2A value_2B 0 1 0 1 1 1 3 5 2 1 7 9 3 1 12 15 4 2 0 1 5 2 2 4 6 2 3 6 7 3 1 3 Join on equi and non-equi operators is possible: df1.conditional_join( df2, (\"id\", \"id\", \"==\"), (\"value_1\", \"value_2A\", \">=\"), (\"value_1\", \"value_2B\", \"<=\"), sort_by_appearance=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 5 1 3 5 1 1 7 1 7 9 2 2 1 2 0 1 3 2 3 2 2 4 4 2 3 2 3 6 The default join is inner. left and right joins are supported as well: df1.conditional_join( df2, (\"id\", \"id\", \"==\"), (\"value_1\", \"value_2A\", \">=\"), (\"value_1\", \"value_2B\", \"<=\"), how=\"left\", sort_by_appearance=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 2 NaN NaN NaN 1 1 5 1.0 3.0 5.0 2 1 7 1.0 7.0 9.0 3 2 1 2.0 0.0 1.0 4 2 3 2.0 2.0 4.0 5 2 3 2.0 3.0 6.0 6 3 4 NaN NaN NaN df1.conditional_join( df2, (\"id\", \"id\", \"==\"), (\"value_1\", \"value_2A\", \">=\"), (\"value_1\", \"value_2B\", \"<=\"), how=\"right\", sort_by_appearance=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 NaN NaN 1 0 1 1 1.0 5.0 1 3 5 2 1.0 7.0 1 7 9 3 NaN NaN 1 12 15 4 2.0 1.0 2 0 1 5 2.0 3.0 2 2 4 6 2.0 3.0 2 3 6 7 NaN NaN 3 1 3 Join on just the non-equi joins is also possible: df1.conditional_join( df2, (\"value_1\", \"value_2A\", \">\"), (\"value_1\", \"value_2B\", \"<\"), how=\"inner\", sort_by_appearance=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 2 3 1 3 1 1 5 2 3 6 2 2 3 2 2 4 3 3 4 1 3 5 4 3 4 2 3 6 Join on just equality is also possible, but should be avoided, as Pandas' merge/join is more efficient (it uses Pandas' internal merge functions anyway); df1.conditional_join(df2, (\"id\", \"id\", \"==\")) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 2 1 0 1 1 1 2 1 3 5 2 1 2 1 7 9 3 1 2 1 12 15 4 1 5 1 0 1 5 1 5 1 3 5 6 1 5 1 7 9 7 1 5 1 12 15 8 1 7 1 0 1 9 1 7 1 3 5 10 1 7 1 7 9 11 1 7 1 12 15 12 2 1 2 0 1 13 2 1 2 2 4 14 2 1 2 3 6 15 2 3 2 0 1 16 2 3 2 2 4 17 2 3 2 3 6 18 3 4 3 1 3 Join on not equal -> != df1.conditional_join(df2, (\"id\", \"id\", \"!=\")) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 2 2 0 1 1 1 2 2 2 4 2 1 2 2 3 6 3 1 2 3 1 3 4 1 5 2 0 1 5 1 5 2 2 4 6 1 5 2 3 6 7 1 5 3 1 3 8 1 7 2 0 1 9 1 7 2 2 4 10 1 7 2 3 6 11 1 7 3 1 3 12 2 1 3 1 3 13 2 3 3 1 3 14 2 1 1 0 1 15 2 1 1 3 5 16 2 1 1 7 9 17 2 1 1 12 15 18 2 3 1 0 1 19 2 3 1 3 5 20 2 3 1 7 9 21 2 3 1 12 15 22 3 4 1 0 1 23 3 4 1 3 5 24 3 4 1 7 9 25 3 4 1 12 15 26 3 4 2 0 1 27 3 4 2 2 4 28 3 4 2 3 6 If the columns from both dataframes have nothing in common, a single indexed column is returned: ( df1.select_columns(\"value_1\").conditional_join( df2.select_columns(\"val*\"), (\"value_1\", \"value_2A\", \">\"), (\"value_1\", \"value_2B\", \"<\"), ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value_1 value_2A value_2B 0 2 1 3 1 5 3 6 2 3 2 4 3 4 3 5 4 4 3 6","title":"Conditional join"},{"location":"conditional_join/#joining-on-non-equi-operators","text":"import janitor import pandas as pd # https://stackoverflow.com/q/61948103/7175713 df1 = pd.DataFrame({\"id\": [1, 1, 1, 2, 2, 3], \"value_1\": [2, 5, 7, 1, 3, 4]}) df2 = pd.DataFrame( { \"id\": [1, 1, 1, 1, 2, 2, 2, 3], \"value_2A\": [0, 3, 7, 12, 0, 2, 3, 1], \"value_2B\": [1, 5, 9, 15, 1, 4, 6, 3], } ) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id value_1 0 1 2 1 1 5 2 1 7 3 2 1 4 2 3 5 3 4 df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id value_2A value_2B 0 1 0 1 1 1 3 5 2 1 7 9 3 1 12 15 4 2 0 1 5 2 2 4 6 2 3 6 7 3 1 3 Join on equi and non-equi operators is possible: df1.conditional_join( df2, (\"id\", \"id\", \"==\"), (\"value_1\", \"value_2A\", \">=\"), (\"value_1\", \"value_2B\", \"<=\"), sort_by_appearance=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 5 1 3 5 1 1 7 1 7 9 2 2 1 2 0 1 3 2 3 2 2 4 4 2 3 2 3 6 The default join is inner. left and right joins are supported as well: df1.conditional_join( df2, (\"id\", \"id\", \"==\"), (\"value_1\", \"value_2A\", \">=\"), (\"value_1\", \"value_2B\", \"<=\"), how=\"left\", sort_by_appearance=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 2 NaN NaN NaN 1 1 5 1.0 3.0 5.0 2 1 7 1.0 7.0 9.0 3 2 1 2.0 0.0 1.0 4 2 3 2.0 2.0 4.0 5 2 3 2.0 3.0 6.0 6 3 4 NaN NaN NaN df1.conditional_join( df2, (\"id\", \"id\", \"==\"), (\"value_1\", \"value_2A\", \">=\"), (\"value_1\", \"value_2B\", \"<=\"), how=\"right\", sort_by_appearance=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 NaN NaN 1 0 1 1 1.0 5.0 1 3 5 2 1.0 7.0 1 7 9 3 NaN NaN 1 12 15 4 2.0 1.0 2 0 1 5 2.0 3.0 2 2 4 6 2.0 3.0 2 3 6 7 NaN NaN 3 1 3 Join on just the non-equi joins is also possible: df1.conditional_join( df2, (\"value_1\", \"value_2A\", \">\"), (\"value_1\", \"value_2B\", \"<\"), how=\"inner\", sort_by_appearance=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 2 3 1 3 1 1 5 2 3 6 2 2 3 2 2 4 3 3 4 1 3 5 4 3 4 2 3 6 Join on just equality is also possible, but should be avoided, as Pandas' merge/join is more efficient (it uses Pandas' internal merge functions anyway); df1.conditional_join(df2, (\"id\", \"id\", \"==\")) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 2 1 0 1 1 1 2 1 3 5 2 1 2 1 7 9 3 1 2 1 12 15 4 1 5 1 0 1 5 1 5 1 3 5 6 1 5 1 7 9 7 1 5 1 12 15 8 1 7 1 0 1 9 1 7 1 3 5 10 1 7 1 7 9 11 1 7 1 12 15 12 2 1 2 0 1 13 2 1 2 2 4 14 2 1 2 3 6 15 2 3 2 0 1 16 2 3 2 2 4 17 2 3 2 3 6 18 3 4 3 1 3 Join on not equal -> != df1.conditional_join(df2, (\"id\", \"id\", \"!=\")) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } left right id value_1 id value_2A value_2B 0 1 2 2 0 1 1 1 2 2 2 4 2 1 2 2 3 6 3 1 2 3 1 3 4 1 5 2 0 1 5 1 5 2 2 4 6 1 5 2 3 6 7 1 5 3 1 3 8 1 7 2 0 1 9 1 7 2 2 4 10 1 7 2 3 6 11 1 7 3 1 3 12 2 1 3 1 3 13 2 3 3 1 3 14 2 1 1 0 1 15 2 1 1 3 5 16 2 1 1 7 9 17 2 1 1 12 15 18 2 3 1 0 1 19 2 3 1 3 5 20 2 3 1 7 9 21 2 3 1 12 15 22 3 4 1 0 1 23 3 4 1 3 5 24 3 4 1 7 9 25 3 4 1 12 15 26 3 4 2 0 1 27 3 4 2 2 4 28 3 4 2 3 6 If the columns from both dataframes have nothing in common, a single indexed column is returned: ( df1.select_columns(\"value_1\").conditional_join( df2.select_columns(\"val*\"), (\"value_1\", \"value_2A\", \">\"), (\"value_1\", \"value_2B\", \"<\"), ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value_1 value_2A value_2B 0 2 1 3 1 5 3 6 2 3 2 4 3 4 3 5 4 4 3 6","title":"Joining on Non-Equi Operators"},{"location":"dirty_data/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Processing Dirty Data Background This is fake data generated to demonstrate the capabilities of pyjanitor . It contains a bunch of common problems that we regularly encounter when working with data. Let's go fix it! Load Packages Importing pyjanitor is all that's needed to give Pandas Dataframes extra methods to work with your data. import janitor import matplotlib.pyplot as plt import missingno as mn import pandas as pd from pyprojroot import here %matplotlib inline Load Data df = pd.read_excel(here() / \"data/dirty_data.xlsx\", engine=\"openpyxl\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Last Name Employee Status Subject Hire Date % Allocated Full time? do not edit! ---> Certification Certification.1 Certification.2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN 2 Alicia Keys Teacher Music 37118.0 1.00 Yes NaN Instr. music Vocal music NaN 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes NaN PENDING Computers NaN 4 Desus Nice Administration Dean 41431.0 1.00 Yes NaN PENDING NaN NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 7 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 James Joyce Teacher English 32994.0 0.50 No NaN NaN English 6-12 NaN 9 Hedy Lamarr Teacher Science 27919.0 0.50 No NaN PENDING NaN NaN 10 Carlos Boozer Coach Basketball 42221.0 NaN No NaN Physical ed NaN NaN 11 Young Boozer Coach NaN 34700.0 NaN No NaN NaN Political sci. NaN 12 Micheal Larsen Teacher English 40071.0 0.80 No NaN Vocal music English NaN Visualizing the NaN Values Quickly visualizing the NaN values with the package missingno can help us see how dirty the data may be and if the NaN values have any relationships between columns. And as we will see later, it can also be used to visualize the improvement afforded by pyjanitor . Here is the current visual state of the data (the right side of the chart gives a vertical line graph of min/max row information): mn.matrix(df); Cleaning Column Names There are a bunch of problems with this data. Firstly, the column names are not lowercase, and they have spaces. This will make it cumbersome to use in a programmatic function. To solve this, we can use the clean_names() method. df_clean = df.clean_names() df_clean.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date %_allocated full_time_ do_not_edit!_> certification certification_1 certification_2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN Notice now how the column names have been made better. If you squint at the unclean dataset, you'll notice one row and one column of data that are missing. We can also fix this! Building on top of the code block from above, let's now remove those empty columns using the remove_empty() method: df_clean = df.clean_names().remove_empty() df_clean.head(9).tail(4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date %_allocated full_time_ certification certification_1 5 Chien-Shiung Wu Teacher Physics 11037.0 0.5 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.5 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.5 No NaN English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.5 No PENDING NaN Now this is starting to shape up well! Renaming Individual Columns Next, let's rename some of the columns. %_allocated and full_time? contain non-alphanumeric characters, so they make it a bit harder to use. We can rename them using the :py:meth: rename_column() method: df_clean = ( df.clean_names() .remove_empty() .rename_column(\"%_allocated\", \"percent_allocated\") .rename_column(\"full_time_\", \"full_time\") ) df_clean.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN Note how now we have really nice column names! You might be wondering why I'm not modifying the two certifiation columns -- that is the next thing we'll tackle. Coalescing Columns If we look more closely at the two certification columns, we'll see that they look like this: df_clean[[\"certification\", \"certification_1\"]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } certification certification_1 0 Physical ed Theater 1 Physical ed Theater 2 Instr. music Vocal music 3 PENDING Computers 4 PENDING NaN 5 Science 6-12 Physics 6 Science 6-12 Physics 7 NaN English 6-12 8 PENDING NaN 9 Physical ed NaN 10 NaN Political sci. 11 Vocal music English Rows 8 and 11 have NaN in the left certification column, but have a value in the right certification column. Let's assume for a moment that the left certification column is intended to record the first certification that a teacher had obtained. In this case, the values in the right certification column on rows 8 and 11 should be moved to the first column. Let's do that with Janitor, using the coalesce() method, which does the following: df_clean = ( df.clean_names() .remove_empty() .rename_column(\"%_allocated\", \"percent_allocated\") .rename_column(\"full_time_\", \"full_time\") .coalesce(\"certification\", \"certification_1\", new_column_name=\"certification\") ) df_clean .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 42221.0 NaN No Physical ed NaN 10 Young Boozer Coach NaN 34700.0 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 40071.0 0.80 No Vocal music English Awesome stuff! Now we don't have two columns of scattered data, we have one column of densely populated data.` Dealing with Excel Dates Finally, notice how the hire_date column isn't date formatted. It's got this weird Excel serialization. To clean up this data, we can use the :py:meth: convert_excel_date method. df_clean = ( df.clean_names() .remove_empty() .rename_column(\"%_allocated\", \"percent_allocated\") .rename_column(\"full_time_\", \"full_time\") .coalesce(\"certification\", \"certification_1\", target_column_name=\"certification\") .convert_excel_date(\"hire_date\") ) df_clean .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 2008-08-30 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 2008-08-30 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 2001-08-15 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 1975-05-01 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 2013-06-06 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 1930-03-20 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 1990-05-01 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 1976-06-08 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 2015-08-05 NaN No Physical ed NaN 10 Young Boozer Coach NaN 1995-01-01 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 2009-09-15 0.80 No Vocal music English Comparing the DataFrame Before and After pyjanitor fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 15)) mn.matrix(df, ax=ax1, sparkline=False) mn.matrix(df_clean, ax=ax2, sparkline=False) plt.tight_layout(); We have a cleaned dataframe!","title":"Dirty data"},{"location":"dirty_data/#processing-dirty-data","text":"","title":"Processing Dirty Data"},{"location":"dirty_data/#background","text":"This is fake data generated to demonstrate the capabilities of pyjanitor . It contains a bunch of common problems that we regularly encounter when working with data. Let's go fix it!","title":"Background"},{"location":"dirty_data/#load-packages","text":"Importing pyjanitor is all that's needed to give Pandas Dataframes extra methods to work with your data. import janitor import matplotlib.pyplot as plt import missingno as mn import pandas as pd from pyprojroot import here %matplotlib inline","title":"Load Packages"},{"location":"dirty_data/#load-data","text":"df = pd.read_excel(here() / \"data/dirty_data.xlsx\", engine=\"openpyxl\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Last Name Employee Status Subject Hire Date % Allocated Full time? do not edit! ---> Certification Certification.1 Certification.2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN 2 Alicia Keys Teacher Music 37118.0 1.00 Yes NaN Instr. music Vocal music NaN 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes NaN PENDING Computers NaN 4 Desus Nice Administration Dean 41431.0 1.00 Yes NaN PENDING NaN NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 7 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 James Joyce Teacher English 32994.0 0.50 No NaN NaN English 6-12 NaN 9 Hedy Lamarr Teacher Science 27919.0 0.50 No NaN PENDING NaN NaN 10 Carlos Boozer Coach Basketball 42221.0 NaN No NaN Physical ed NaN NaN 11 Young Boozer Coach NaN 34700.0 NaN No NaN NaN Political sci. NaN 12 Micheal Larsen Teacher English 40071.0 0.80 No NaN Vocal music English NaN","title":"Load Data"},{"location":"dirty_data/#visualizing-the-nan-values","text":"Quickly visualizing the NaN values with the package missingno can help us see how dirty the data may be and if the NaN values have any relationships between columns. And as we will see later, it can also be used to visualize the improvement afforded by pyjanitor . Here is the current visual state of the data (the right side of the chart gives a vertical line graph of min/max row information): mn.matrix(df);","title":"Visualizing the NaN Values"},{"location":"dirty_data/#cleaning-column-names","text":"There are a bunch of problems with this data. Firstly, the column names are not lowercase, and they have spaces. This will make it cumbersome to use in a programmatic function. To solve this, we can use the clean_names() method. df_clean = df.clean_names() df_clean.head(2) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date %_allocated full_time_ do_not_edit!_> certification certification_1 certification_2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN Notice now how the column names have been made better. If you squint at the unclean dataset, you'll notice one row and one column of data that are missing. We can also fix this! Building on top of the code block from above, let's now remove those empty columns using the remove_empty() method: df_clean = df.clean_names().remove_empty() df_clean.head(9).tail(4) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date %_allocated full_time_ certification certification_1 5 Chien-Shiung Wu Teacher Physics 11037.0 0.5 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.5 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.5 No NaN English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.5 No PENDING NaN Now this is starting to shape up well!","title":"Cleaning Column Names"},{"location":"dirty_data/#renaming-individual-columns","text":"Next, let's rename some of the columns. %_allocated and full_time? contain non-alphanumeric characters, so they make it a bit harder to use. We can rename them using the :py:meth: rename_column() method: df_clean = ( df.clean_names() .remove_empty() .rename_column(\"%_allocated\", \"percent_allocated\") .rename_column(\"full_time_\", \"full_time\") ) df_clean.head(5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN Note how now we have really nice column names! You might be wondering why I'm not modifying the two certifiation columns -- that is the next thing we'll tackle.","title":"Renaming Individual Columns"},{"location":"dirty_data/#coalescing-columns","text":"If we look more closely at the two certification columns, we'll see that they look like this: df_clean[[\"certification\", \"certification_1\"]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } certification certification_1 0 Physical ed Theater 1 Physical ed Theater 2 Instr. music Vocal music 3 PENDING Computers 4 PENDING NaN 5 Science 6-12 Physics 6 Science 6-12 Physics 7 NaN English 6-12 8 PENDING NaN 9 Physical ed NaN 10 NaN Political sci. 11 Vocal music English Rows 8 and 11 have NaN in the left certification column, but have a value in the right certification column. Let's assume for a moment that the left certification column is intended to record the first certification that a teacher had obtained. In this case, the values in the right certification column on rows 8 and 11 should be moved to the first column. Let's do that with Janitor, using the coalesce() method, which does the following: df_clean = ( df.clean_names() .remove_empty() .rename_column(\"%_allocated\", \"percent_allocated\") .rename_column(\"full_time_\", \"full_time\") .coalesce(\"certification\", \"certification_1\", new_column_name=\"certification\") ) df_clean .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 42221.0 NaN No Physical ed NaN 10 Young Boozer Coach NaN 34700.0 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 40071.0 0.80 No Vocal music English Awesome stuff! Now we don't have two columns of scattered data, we have one column of densely populated data.`","title":"Coalescing Columns"},{"location":"dirty_data/#dealing-with-excel-dates","text":"Finally, notice how the hire_date column isn't date formatted. It's got this weird Excel serialization. To clean up this data, we can use the :py:meth: convert_excel_date method. df_clean = ( df.clean_names() .remove_empty() .rename_column(\"%_allocated\", \"percent_allocated\") .rename_column(\"full_time_\", \"full_time\") .coalesce(\"certification\", \"certification_1\", target_column_name=\"certification\") .convert_excel_date(\"hire_date\") ) df_clean .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 2008-08-30 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 2008-08-30 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 2001-08-15 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 1975-05-01 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 2013-06-06 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 1930-03-20 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 1990-05-01 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 1976-06-08 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 2015-08-05 NaN No Physical ed NaN 10 Young Boozer Coach NaN 1995-01-01 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 2009-09-15 0.80 No Vocal music English","title":"Dealing with Excel Dates"},{"location":"dirty_data/#comparing-the-dataframe-before-and-after-pyjanitor","text":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 15)) mn.matrix(df, ax=ax1, sparkline=False) mn.matrix(df_clean, ax=ax2, sparkline=False) plt.tight_layout(); We have a cleaned dataframe!","title":"Comparing the DataFrame Before and After pyjanitor"},{"location":"expand_grid/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Expand_grid : Create a dataframe from all combinations of inputs. Background This notebook serves to show examples of how expand_grid works. Expand_grid aims to offer similar functionality to R's expand_grid function. Expand_grid creates a dataframe from a combination of all inputs. One requirement is that a dictionary be provided. If a dataframe is provided, a key must be provided as well. Some of the examples used here are from tidyr's expand_grid page and from Pandas' cookbook. import numpy as np import pandas as pd from janitor import expand_grid data = {\"x\": [1, 2, 3], \"y\": [1, 2]} result = expand_grid(others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } x y 0 0 0 1 1 1 1 2 2 2 1 3 2 2 4 3 1 5 3 2 # combination of letters data = {\"l1\": list(\"abcde\"), \"l2\": list(\"ABCDE\")} letters = expand_grid(others=data) letters.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } l1 l2 0 0 0 a A 1 a B 2 a C 3 a D 4 a E 5 b A 6 b B 7 b C 8 b D 9 b E data = {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]} result = expand_grid(others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } height weight sex 0 0 0 0 60 100 Male 1 60 100 Female 2 60 140 Male 3 60 140 Female 4 60 180 Male 5 60 180 Female 6 70 100 Male 7 70 100 Female 8 70 140 Male 9 70 140 Female 10 70 180 Male 11 70 180 Female # A dictionary of arrays # Arrays can only have dimensions of 1 or 2 data = {\"x1\": np.array([[1, 3], [2, 4]]), \"x2\": np.array([[5, 7], [6, 8]])} result = expand_grid(others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } x1 x2 0 1 0 1 0 1 3 5 7 1 1 3 6 8 2 2 4 5 7 3 2 4 6 8 # This shows how to method chain expand_grid # to an existing dataframe df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]}) data = {\"z\": [1, 2, 3]} # a key has to be passed in for the dataframe # this is added to the column name of the dataframe result = df.expand_grid(df_key=\"df\", others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } df z x y 0 0 1 2 1 1 1 2 2 2 1 2 3 3 2 1 1 4 2 1 2 5 2 1 3 # expand_grid can work on multiple dataframes # Ensure that there are keys # for each dataframe in the dictionary df1 = pd.DataFrame({\"x\": range(1, 3), \"y\": [2, 1]}) df2 = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [3, 2, 1]}) df3 = pd.DataFrame({\"x\": [2, 3], \"y\": [\"a\", \"b\"]}) data = {\"df1\": df1, \"df2\": df2, \"df3\": df3} result = expand_grid(others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } df1 df2 df3 x y x y x y 0 1 2 1 3 2 a 1 1 2 1 3 3 b 2 1 2 2 2 2 a 3 1 2 2 2 3 b 4 1 2 3 1 2 a 5 1 2 3 1 3 b 6 2 1 1 3 2 a 7 2 1 1 3 3 b 8 2 1 2 2 2 a 9 2 1 2 2 3 b 10 2 1 3 1 2 a 11 2 1 3 1 3 b Columns can be flattened with pyjanitor's collapse_levels : result.collapse_levels() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } df1_x df1_y df2_x df2_y df3_x df3_y 0 1 2 1 3 2 a 1 1 2 1 3 3 b 2 1 2 2 2 2 a 3 1 2 2 2 3 b 4 1 2 3 1 2 a 5 1 2 3 1 3 b 6 2 1 1 3 2 a 7 2 1 1 3 3 b 8 2 1 2 2 2 a 9 2 1 2 2 3 b 10 2 1 3 1 2 a 11 2 1 3 1 3 b Or a level dropped with Pandas' droplevel method: letters.droplevel(level=-1, axis=\"columns\").head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } l1 l2 0 a A 1 a B 2 a C 3 a D 4 a E 5 b A 6 b B 7 b C 8 b D 9 b E","title":"Expand grid"},{"location":"expand_grid/#expand_grid-create-a-dataframe-from-all-combinations-of-inputs","text":"","title":"Expand_grid : Create a dataframe from all combinations of inputs."},{"location":"expand_grid/#background","text":"This notebook serves to show examples of how expand_grid works. Expand_grid aims to offer similar functionality to R's expand_grid function. Expand_grid creates a dataframe from a combination of all inputs. One requirement is that a dictionary be provided. If a dataframe is provided, a key must be provided as well. Some of the examples used here are from tidyr's expand_grid page and from Pandas' cookbook. import numpy as np import pandas as pd from janitor import expand_grid data = {\"x\": [1, 2, 3], \"y\": [1, 2]} result = expand_grid(others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } x y 0 0 0 1 1 1 1 2 2 2 1 3 2 2 4 3 1 5 3 2 # combination of letters data = {\"l1\": list(\"abcde\"), \"l2\": list(\"ABCDE\")} letters = expand_grid(others=data) letters.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } l1 l2 0 0 0 a A 1 a B 2 a C 3 a D 4 a E 5 b A 6 b B 7 b C 8 b D 9 b E data = {\"height\": [60, 70], \"weight\": [100, 140, 180], \"sex\": [\"Male\", \"Female\"]} result = expand_grid(others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } height weight sex 0 0 0 0 60 100 Male 1 60 100 Female 2 60 140 Male 3 60 140 Female 4 60 180 Male 5 60 180 Female 6 70 100 Male 7 70 100 Female 8 70 140 Male 9 70 140 Female 10 70 180 Male 11 70 180 Female # A dictionary of arrays # Arrays can only have dimensions of 1 or 2 data = {\"x1\": np.array([[1, 3], [2, 4]]), \"x2\": np.array([[5, 7], [6, 8]])} result = expand_grid(others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } x1 x2 0 1 0 1 0 1 3 5 7 1 1 3 6 8 2 2 4 5 7 3 2 4 6 8 # This shows how to method chain expand_grid # to an existing dataframe df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]}) data = {\"z\": [1, 2, 3]} # a key has to be passed in for the dataframe # this is added to the column name of the dataframe result = df.expand_grid(df_key=\"df\", others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } df z x y 0 0 1 2 1 1 1 2 2 2 1 2 3 3 2 1 1 4 2 1 2 5 2 1 3 # expand_grid can work on multiple dataframes # Ensure that there are keys # for each dataframe in the dictionary df1 = pd.DataFrame({\"x\": range(1, 3), \"y\": [2, 1]}) df2 = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [3, 2, 1]}) df3 = pd.DataFrame({\"x\": [2, 3], \"y\": [\"a\", \"b\"]}) data = {\"df1\": df1, \"df2\": df2, \"df3\": df3} result = expand_grid(others=data) result .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } df1 df2 df3 x y x y x y 0 1 2 1 3 2 a 1 1 2 1 3 3 b 2 1 2 2 2 2 a 3 1 2 2 2 3 b 4 1 2 3 1 2 a 5 1 2 3 1 3 b 6 2 1 1 3 2 a 7 2 1 1 3 3 b 8 2 1 2 2 2 a 9 2 1 2 2 3 b 10 2 1 3 1 2 a 11 2 1 3 1 3 b Columns can be flattened with pyjanitor's collapse_levels : result.collapse_levels() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } df1_x df1_y df2_x df2_y df3_x df3_y 0 1 2 1 3 2 a 1 1 2 1 3 3 b 2 1 2 2 2 2 a 3 1 2 2 2 3 b 4 1 2 3 1 2 a 5 1 2 3 1 3 b 6 2 1 1 3 2 a 7 2 1 1 3 3 b 8 2 1 2 2 2 a 9 2 1 2 2 3 b 10 2 1 3 1 2 a 11 2 1 3 1 3 b Or a level dropped with Pandas' droplevel method: letters.droplevel(level=-1, axis=\"columns\").head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } l1 l2 0 a A 1 a B 2 a C 3 a D 4 a E 5 b A 6 b B 7 b C 8 b D 9 b E","title":"Background"},{"location":"french_trains/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Processing French train data Background The SNCF (National Society of French Railways) is France's national state-owned railway company. Founded in 1938, it operates the country's national rail traffic along with Monaco, including the TGV, France's high-speed rail network. This dataset covers 2015-2018 with many different train stations. The dataset primarily covers aggregate trip times, delay times, cause for delay, etc., for each station there are 27 columns in total. A TGV route map can be seen here . The data The source data set is available from the SNCF . Check out this visualization of it. This has been used in a tidy tuesday previously. The full data set is available but we will work with a subset . Preliminaries import os from collections import Counter import janitor import pandas as pd import seaborn as sns # allow plots to appear directly in the notebook %matplotlib inline Call chaining example First, we run all the methods using pyjanitor's preferred call chaining approach. This code updates the column names, removes any empty rows/columns, and drops some unneeded columns in a very readable manner. chained_df = ( pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/small_trains.csv\" ) # ingest raw data .clean_names() # removes whitespace, punctuation/symbols, capitalization .remove_empty() # removes entirely empty rows / columns .rename_column(\"num_late_at_departure\", \"num_departing_late\") # renames 1 column .drop( columns=[\"service\", \"delay_cause\", \"delayed_number\"] ) # drops 3 unnecessary columns # add 2 new columns with a calculation .join_apply( lambda df: df.num_departing_late / df.total_num_trips, \"prop_late_departures\" ) .join_apply( lambda df: df.num_arriving_late / df.total_num_trips, \"prop_late_arrivals\" ) ) chained_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month departure_station arrival_station journey_time_avg total_num_trips avg_delay_all_departing avg_delay_all_arriving num_departing_late num_arriving_late prop_late_departures prop_late_arrivals 0 2017 9 PARIS EST METZ 85.133779 299 0.752007 0.419844 15 17.0 0.050167 0.056856 1 2017 9 REIMS PARIS EST 47.064516 218 1.263518 1.137558 10 23.0 0.045872 0.105505 2 2017 9 PARIS EST STRASBOURG 116.234940 333 1.139257 1.586396 20 19.0 0.060060 0.057057 Step by step through the methods Now, we will import the French data again and then use the methods from the call chain one at a time. Our subset of the train data has over 32000 rows and 13 columns. df = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/small_trains.csv\" ) df.shape (32772, 13) Cleaning column names The clean_names method converts the column names to lowercase and replaces all spaces with underscores. For this data set, it actually does not modify any of the names. original_columns = df.columns df = df.clean_names() new_columns = df.columns original_columns == new_columns array([ True, True, True, True, True, True, True, True, True, True, True, True, True]) new_columns Index(['year', 'month', 'service', 'departure_station', 'arrival_station', 'journey_time_avg', 'total_num_trips', 'avg_delay_all_departing', 'avg_delay_all_arriving', 'num_late_at_departure', 'num_arriving_late', 'delay_cause', 'delayed_number'], dtype='object') Renaming columns We rename the \"num_late_at_departure\" column for consistency purposes with the rename_column method. df = df.rename_column(\"num_late_at_departure\", \"num_departing_late\") df.columns Index(['year', 'month', 'service', 'departure_station', 'arrival_station', 'journey_time_avg', 'total_num_trips', 'avg_delay_all_departing', 'avg_delay_all_arriving', 'num_departing_late', 'num_arriving_late', 'delay_cause', 'delayed_number'], dtype='object') Dropping empty columns and rows The remove_empty method looks for empty columns and rows and drops them if found. df.shape (32772, 13) df = df.remove_empty() df.shape (32772, 13) Drop unneeded columns We identify 3 columns that we decided are unnecessary for the analysis and can quickly drop them with the aptly named drop_columns method. df = df.drop(columns=[\"service\", \"delay_cause\", \"delayed_number\"]) df.columns Index(['year', 'month', 'departure_station', 'arrival_station', 'journey_time_avg', 'total_num_trips', 'avg_delay_all_departing', 'avg_delay_all_arriving', 'num_departing_late', 'num_arriving_late'], dtype='object') # gives us the top ten departure stations from that column Counter(df[\"departure_station\"]).most_common(10) [('PARIS LYON', 6834), ('PARIS MONTPARNASSE', 4512), ('PARIS EST', 1692), ('LYON PART DIEU', 1476), ('PARIS NORD', 1128), ('MARSEILLE ST CHARLES', 1044), ('LILLE', 846), ('RENNES', 630), ('NANTES', 630), ('MONTPELLIER', 564)] We use seaborn to quickly visualize how quickly departure and arrivals times were late versus the total number of trips for each of the over 30000 routes in the database. sns.pairplot( df, x_vars=[\"num_departing_late\", \"num_arriving_late\"], y_vars=\"total_num_trips\", height=7, aspect=0.7, ) <seaborn.axisgrid.PairGrid at 0x7f9de4465910> Add additional statistics as new columns We can add columns containing additional statistics concerning the proportion of time each route is late either departing or arriving by using the add_columns method for each route. Note the difference between how we added the two columns below and the same code in the chained_df file creation at the top of the notebook. In order to operate on the df that was in the process of being created in the call chain, we had to use join_apply with a lambda function instead of the add_columns method. Alternatively, we could have split the chain into two separate chains with the df being created in the first chain and the add_columns method being used in the second chain. df_prop = df.add_columns( prop_late_departures=df.num_departing_late / df.total_num_trips, prop_late_arrivals=df.num_arriving_late / df.total_num_trips, ) df_prop.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month departure_station arrival_station journey_time_avg total_num_trips avg_delay_all_departing avg_delay_all_arriving num_departing_late num_arriving_late prop_late_departures prop_late_arrivals 0 2017 9 PARIS EST METZ 85.133779 299 0.752007 0.419844 15 17.0 0.050167 0.056856 1 2017 9 REIMS PARIS EST 47.064516 218 1.263518 1.137558 10 23.0 0.045872 0.105505 2 2017 9 PARIS EST STRASBOURG 116.234940 333 1.139257 1.586396 20 19.0 0.060060 0.057057","title":"French trains"},{"location":"french_trains/#processing-french-train-data","text":"","title":"Processing French train data"},{"location":"french_trains/#background","text":"The SNCF (National Society of French Railways) is France's national state-owned railway company. Founded in 1938, it operates the country's national rail traffic along with Monaco, including the TGV, France's high-speed rail network. This dataset covers 2015-2018 with many different train stations. The dataset primarily covers aggregate trip times, delay times, cause for delay, etc., for each station there are 27 columns in total. A TGV route map can be seen here .","title":"Background"},{"location":"french_trains/#the-data","text":"The source data set is available from the SNCF . Check out this visualization of it. This has been used in a tidy tuesday previously. The full data set is available but we will work with a subset .","title":"The data"},{"location":"french_trains/#preliminaries","text":"import os from collections import Counter import janitor import pandas as pd import seaborn as sns # allow plots to appear directly in the notebook %matplotlib inline","title":"Preliminaries"},{"location":"french_trains/#call-chaining-example","text":"First, we run all the methods using pyjanitor's preferred call chaining approach. This code updates the column names, removes any empty rows/columns, and drops some unneeded columns in a very readable manner. chained_df = ( pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/small_trains.csv\" ) # ingest raw data .clean_names() # removes whitespace, punctuation/symbols, capitalization .remove_empty() # removes entirely empty rows / columns .rename_column(\"num_late_at_departure\", \"num_departing_late\") # renames 1 column .drop( columns=[\"service\", \"delay_cause\", \"delayed_number\"] ) # drops 3 unnecessary columns # add 2 new columns with a calculation .join_apply( lambda df: df.num_departing_late / df.total_num_trips, \"prop_late_departures\" ) .join_apply( lambda df: df.num_arriving_late / df.total_num_trips, \"prop_late_arrivals\" ) ) chained_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month departure_station arrival_station journey_time_avg total_num_trips avg_delay_all_departing avg_delay_all_arriving num_departing_late num_arriving_late prop_late_departures prop_late_arrivals 0 2017 9 PARIS EST METZ 85.133779 299 0.752007 0.419844 15 17.0 0.050167 0.056856 1 2017 9 REIMS PARIS EST 47.064516 218 1.263518 1.137558 10 23.0 0.045872 0.105505 2 2017 9 PARIS EST STRASBOURG 116.234940 333 1.139257 1.586396 20 19.0 0.060060 0.057057","title":"Call chaining example"},{"location":"french_trains/#step-by-step-through-the-methods","text":"Now, we will import the French data again and then use the methods from the call chain one at a time. Our subset of the train data has over 32000 rows and 13 columns. df = pd.read_csv( \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/small_trains.csv\" ) df.shape (32772, 13)","title":"Step by step through the methods"},{"location":"french_trains/#cleaning-column-names","text":"The clean_names method converts the column names to lowercase and replaces all spaces with underscores. For this data set, it actually does not modify any of the names. original_columns = df.columns df = df.clean_names() new_columns = df.columns original_columns == new_columns array([ True, True, True, True, True, True, True, True, True, True, True, True, True]) new_columns Index(['year', 'month', 'service', 'departure_station', 'arrival_station', 'journey_time_avg', 'total_num_trips', 'avg_delay_all_departing', 'avg_delay_all_arriving', 'num_late_at_departure', 'num_arriving_late', 'delay_cause', 'delayed_number'], dtype='object')","title":"Cleaning column names"},{"location":"french_trains/#renaming-columns","text":"We rename the \"num_late_at_departure\" column for consistency purposes with the rename_column method. df = df.rename_column(\"num_late_at_departure\", \"num_departing_late\") df.columns Index(['year', 'month', 'service', 'departure_station', 'arrival_station', 'journey_time_avg', 'total_num_trips', 'avg_delay_all_departing', 'avg_delay_all_arriving', 'num_departing_late', 'num_arriving_late', 'delay_cause', 'delayed_number'], dtype='object')","title":"Renaming columns"},{"location":"french_trains/#dropping-empty-columns-and-rows","text":"The remove_empty method looks for empty columns and rows and drops them if found. df.shape (32772, 13) df = df.remove_empty() df.shape (32772, 13)","title":"Dropping empty columns and rows"},{"location":"french_trains/#drop-unneeded-columns","text":"We identify 3 columns that we decided are unnecessary for the analysis and can quickly drop them with the aptly named drop_columns method. df = df.drop(columns=[\"service\", \"delay_cause\", \"delayed_number\"]) df.columns Index(['year', 'month', 'departure_station', 'arrival_station', 'journey_time_avg', 'total_num_trips', 'avg_delay_all_departing', 'avg_delay_all_arriving', 'num_departing_late', 'num_arriving_late'], dtype='object') # gives us the top ten departure stations from that column Counter(df[\"departure_station\"]).most_common(10) [('PARIS LYON', 6834), ('PARIS MONTPARNASSE', 4512), ('PARIS EST', 1692), ('LYON PART DIEU', 1476), ('PARIS NORD', 1128), ('MARSEILLE ST CHARLES', 1044), ('LILLE', 846), ('RENNES', 630), ('NANTES', 630), ('MONTPELLIER', 564)] We use seaborn to quickly visualize how quickly departure and arrivals times were late versus the total number of trips for each of the over 30000 routes in the database. sns.pairplot( df, x_vars=[\"num_departing_late\", \"num_arriving_late\"], y_vars=\"total_num_trips\", height=7, aspect=0.7, ) <seaborn.axisgrid.PairGrid at 0x7f9de4465910>","title":"Drop unneeded columns"},{"location":"french_trains/#add-additional-statistics-as-new-columns","text":"We can add columns containing additional statistics concerning the proportion of time each route is late either departing or arriving by using the add_columns method for each route. Note the difference between how we added the two columns below and the same code in the chained_df file creation at the top of the notebook. In order to operate on the df that was in the process of being created in the call chain, we had to use join_apply with a lambda function instead of the add_columns method. Alternatively, we could have split the chain into two separate chains with the df being created in the first chain and the add_columns method being used in the second chain. df_prop = df.add_columns( prop_late_departures=df.num_departing_late / df.total_num_trips, prop_late_arrivals=df.num_arriving_late / df.total_num_trips, ) df_prop.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } year month departure_station arrival_station journey_time_avg total_num_trips avg_delay_all_departing avg_delay_all_arriving num_departing_late num_arriving_late prop_late_departures prop_late_arrivals 0 2017 9 PARIS EST METZ 85.133779 299 0.752007 0.419844 15 17.0 0.050167 0.056856 1 2017 9 REIMS PARIS EST 47.064516 218 1.263518 1.137558 10 23.0 0.045872 0.105505 2 2017 9 PARIS EST STRASBOURG 116.234940 333 1.139257 1.586396 20 19.0 0.060060 0.057057","title":"Add additional statistics as new columns"},{"location":"groupby_agg/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Groupby_agg : Shortcut for assigning a groupby-transform to a new column. Background This notebook serves to show how to use the groupby_agg method from pyjanitor's general functions submodule. The groupby_agg method allows us to add the result of an aggregation from a grouping, as a new column, back to the dataframe. Currently in pandas, to append a column back to a dataframe, you do it in three steps: 1. Groupby a column or columns 2. Apply the transform method with an aggregate function on the grouping, and finally 3. Assign the result of the transform to a new column in the dataframe. The groupby_agg allows you to replicate the same process in one step and with sensible arguments. The example below illustrates this function # load modules import numpy as np import pandas as pd from janitor import groupby_agg data = { \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], \"MRP\": [220, 450, 320, 200, 305], \"number_sold\": [100, 40, 56, 38, 25], } df = pd.DataFrame(data) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } item MRP number_sold 0 shoe 220 100 1 shoe 450 40 2 bag 320 56 3 shoe 200 38 4 bag 305 25 Use grouby_agg to find average price for each item and append column to dataframe df = df.groupby_agg( by=\"item\", agg=\"mean\", agg_column_name=\"MRP\", new_column_name=\"Avg_MRP\" ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } item MRP number_sold Avg_MRP 0 shoe 220 100 290.0 1 shoe 450 40 290.0 2 bag 320 56 312.5 3 shoe 200 38 290.0 4 bag 305 25 312.5 Null cells are retained as well : df = pd.DataFrame( { \"name\": (\"black\", \"black\", \"black\", \"red\", \"red\"), \"type\": (\"chair\", \"chair\", \"sofa\", \"sofa\", \"plate\"), \"num\": (4, 5, 12, 4, 3), \"nulls\": (1, 1, np.nan, np.nan, 3), } ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type num nulls 0 black chair 4 1.0 1 black chair 5 1.0 2 black sofa 12 NaN 3 red sofa 4 NaN 4 red plate 3 3.0 filtered_df = df.groupby_agg( by=[\"nulls\"], agg=\"size\", agg_column_name=\"type\", new_column_name=\"counter\" ) filtered_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type num nulls counter 0 black chair 4 1.0 2.0 1 black chair 5 1.0 2.0 2 black sofa 12 NaN NaN 3 red sofa 4 NaN NaN 4 red plate 3 3.0 1.0 The groupby_agg method can be extended for different purposes. One of these is groupwise filtering, where only groups that meet a condition are retained. Let's explore this with an example: filtered_df = df.groupby_agg( by=[\"name\", \"type\"], agg=\"size\", agg_column_name=\"type\", new_column_name=\"counter\" ).query(\"counter > 1\") filtered_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type num nulls counter 0 black chair 4 1.0 2 1 black chair 5 1.0 2","title":"Groupby agg"},{"location":"groupby_agg/#groupby_agg-shortcut-for-assigning-a-groupby-transform-to-a-new-column","text":"","title":"Groupby_agg : Shortcut for assigning a groupby-transform to a new column."},{"location":"groupby_agg/#background","text":"This notebook serves to show how to use the groupby_agg method from pyjanitor's general functions submodule. The groupby_agg method allows us to add the result of an aggregation from a grouping, as a new column, back to the dataframe. Currently in pandas, to append a column back to a dataframe, you do it in three steps: 1. Groupby a column or columns 2. Apply the transform method with an aggregate function on the grouping, and finally 3. Assign the result of the transform to a new column in the dataframe. The groupby_agg allows you to replicate the same process in one step and with sensible arguments. The example below illustrates this function # load modules import numpy as np import pandas as pd from janitor import groupby_agg data = { \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"], \"MRP\": [220, 450, 320, 200, 305], \"number_sold\": [100, 40, 56, 38, 25], } df = pd.DataFrame(data) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } item MRP number_sold 0 shoe 220 100 1 shoe 450 40 2 bag 320 56 3 shoe 200 38 4 bag 305 25","title":"Background"},{"location":"groupby_agg/#use-grouby_agg-to-find-average-price-for-each-item-and-append-column-to-dataframe","text":"df = df.groupby_agg( by=\"item\", agg=\"mean\", agg_column_name=\"MRP\", new_column_name=\"Avg_MRP\" ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } item MRP number_sold Avg_MRP 0 shoe 220 100 290.0 1 shoe 450 40 290.0 2 bag 320 56 312.5 3 shoe 200 38 290.0 4 bag 305 25 312.5 Null cells are retained as well : df = pd.DataFrame( { \"name\": (\"black\", \"black\", \"black\", \"red\", \"red\"), \"type\": (\"chair\", \"chair\", \"sofa\", \"sofa\", \"plate\"), \"num\": (4, 5, 12, 4, 3), \"nulls\": (1, 1, np.nan, np.nan, 3), } ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type num nulls 0 black chair 4 1.0 1 black chair 5 1.0 2 black sofa 12 NaN 3 red sofa 4 NaN 4 red plate 3 3.0 filtered_df = df.groupby_agg( by=[\"nulls\"], agg=\"size\", agg_column_name=\"type\", new_column_name=\"counter\" ) filtered_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type num nulls counter 0 black chair 4 1.0 2.0 1 black chair 5 1.0 2.0 2 black sofa 12 NaN NaN 3 red sofa 4 NaN NaN 4 red plate 3 3.0 1.0 The groupby_agg method can be extended for different purposes. One of these is groupwise filtering, where only groups that meet a condition are retained. Let's explore this with an example: filtered_df = df.groupby_agg( by=[\"name\", \"type\"], agg=\"size\", agg_column_name=\"type\", new_column_name=\"counter\" ).query(\"counter > 1\") filtered_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name type num nulls counter 0 black chair 4 1.0 2 1 black chair 5 1.0 2","title":"Use grouby_agg to find average price for each item and append column to dataframe"},{"location":"normalize/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Normalization and Standardization Normalization makes data more meaningful by converting absolute values into comparisons with related values. Chris Vallier has produced this demonstration of normalization using PyJanitor. pyjanitor functions demonstrated here: min_max_scale transform_columns import janitor import pandas as pd import seaborn as sns sns.set(style=\"whitegrid\") Load data We'll use a dataset with fuel efficiency in miles per gallon (\"mpg\"), engine displacement in cubic centimeters (\"disp\"), and horsepower (\"hp\") for a variety of car models. It's a crazy, but customary, mix of units. csv_file = \"https://gist.githubusercontent.com/seankross/a412dfbd88b3db70b74b/raw/5f23f993cd87c283ce766e7ac6b329ee7cc2e1d1/mtcars.csv\" cars_df = pd.read_csv(csv_file) Quantities without units are dangerous, so let's use pyjanitor's rename_column ... cars_df = cars_df.rename_column(\"disp\", \"disp_cc\") Examine raw data cars_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model mpg cyl disp_cc hp drat wt qsec vs am gear carb 0 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 1 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 2 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 3 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 4 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 Visualize Each value makes more sense viewed in comparison to the other models. We'll use simple Seaborn bar plots. mpg by model cars_df = cars_df.sort_values(\"mpg\", ascending=False) sns.barplot( y=\"model\", x=\"mpg\", data=cars_df, color=\"b\", orient=\"h\", ) <AxesSubplot:xlabel='mpg', ylabel='model'> displacement by model cars_df = cars_df.sort_values(\"disp_cc\", ascending=False) sns.barplot(y=\"model\", x=\"disp_cc\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='disp_cc', ylabel='model'> horsepower by model cars_df = cars_df.sort_values(\"hp\", ascending=False) sns.barplot(y=\"model\", x=\"hp\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='hp', ylabel='model'> min-max normalization First we'll use pyjanitor's min_max_scale to rescale the mpg , disp_cc , and hp columns in-place so that each value varies from 0 to 1. ( cars_df.min_max_scale(col_name=\"mpg\", new_max=1, new_min=0) .min_max_scale(col_name=\"disp_cc\", new_max=1, new_min=0) .min_max_scale(col_name=\"hp\", new_max=1, new_min=0) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model mpg cyl disp_cc hp drat wt qsec vs am gear carb 30 Maserati Bora 0.195745 8 0.573460 1.000000 3.54 3.570 14.60 0 1 5 8 28 Ford Pantera L 0.229787 8 0.698179 0.749117 4.22 3.170 14.50 0 1 5 4 6 Duster 360 0.165957 8 0.720629 0.681979 3.21 3.570 15.84 0 0 3 4 23 Camaro Z28 0.123404 8 0.695685 0.681979 3.73 3.840 15.41 0 0 3 4 16 Chrysler Imperial 0.182979 8 0.920180 0.628975 3.23 5.345 17.42 0 0 3 4 15 Lincoln Continental 0.000000 8 0.970067 0.575972 3.00 5.424 17.82 0 0 3 4 14 Cadillac Fleetwood 0.000000 8 1.000000 0.540636 2.93 5.250 17.98 0 0 3 4 13 Merc 450SLC 0.204255 8 0.510601 0.452297 3.07 3.780 18.00 0 0 3 3 11 Merc 450SE 0.255319 8 0.510601 0.452297 3.07 4.070 17.40 0 0 3 3 12 Merc 450SL 0.293617 8 0.510601 0.452297 3.07 3.730 17.60 0 0 3 3 24 Pontiac Firebird 0.374468 8 0.820404 0.434629 3.08 3.845 17.05 0 0 3 2 4 Hornet Sportabout 0.353191 8 0.720629 0.434629 3.15 3.440 17.02 0 0 3 2 29 Ferrari Dino 0.395745 6 0.184335 0.434629 3.62 2.770 15.50 0 1 5 6 21 Dodge Challenger 0.217021 8 0.615864 0.346290 2.76 3.520 16.87 0 0 3 2 22 AMC Javelin 0.204255 8 0.580943 0.346290 3.15 3.435 17.30 0 0 3 2 10 Merc 280C 0.314894 6 0.240708 0.250883 3.92 3.440 18.90 1 0 4 4 9 Merc 280 0.374468 6 0.240708 0.250883 3.92 3.440 18.30 1 0 4 4 27 Lotus Europa 0.851064 4 0.059865 0.215548 3.77 1.513 16.90 1 1 5 2 0 Mazda RX4 0.451064 6 0.221751 0.204947 3.90 2.620 16.46 0 1 4 4 1 Mazda RX4 Wag 0.451064 6 0.221751 0.204947 3.90 2.875 17.02 0 1 4 4 3 Hornet 4 Drive 0.468085 6 0.466201 0.204947 3.08 3.215 19.44 1 0 3 1 31 Volvo 142E 0.468085 4 0.124470 0.201413 4.11 2.780 18.60 1 1 4 2 5 Valiant 0.327660 6 0.383886 0.187279 2.76 3.460 20.22 1 0 3 1 20 Toyota Corona 0.472340 4 0.122225 0.159011 3.70 2.465 20.01 1 0 3 1 8 Merc 230 0.527660 4 0.173859 0.151943 3.92 3.150 22.90 1 0 4 2 2 Datsun 710 0.527660 4 0.092043 0.144876 3.85 2.320 18.61 1 1 4 1 26 Porsche 914-2 0.663830 4 0.122724 0.137809 4.43 2.140 16.70 0 1 5 2 25 Fiat X1-9 0.719149 4 0.019706 0.049470 4.08 1.935 18.90 1 1 4 1 17 Fiat 128 0.936170 4 0.018957 0.049470 4.08 2.200 19.47 1 1 4 1 19 Toyota Corolla 1.000000 4 0.000000 0.045936 4.22 1.835 19.90 1 1 4 1 7 Merc 240D 0.595745 4 0.188576 0.035336 3.69 3.190 20.00 1 0 4 2 18 Honda Civic 0.851064 4 0.011474 0.000000 4.93 1.615 18.52 1 1 4 2 The shapes of the bar graphs remain the same, but the horizontal axes show the new scale. mpg (min-max normalized) cars_df = cars_df.sort_values(\"mpg\", ascending=False) sns.barplot(y=\"model\", x=\"mpg\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='mpg', ylabel='model'> displacement (min-max normalized) cars_df = cars_df.sort_values(\"disp_cc\", ascending=False) sns.barplot(y=\"model\", x=\"disp_cc\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='disp_cc', ylabel='model'> horsepower (min-max normalized) cars_df = cars_df.sort_values(\"hp\", ascending=False) sns.barplot(y=\"model\", x=\"hp\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='hp', ylabel='model'> Standardization (z-score) Next we'll convert to standard scores . This expresses each value in terms of its standard deviations from the mean, expressing where each model stands in relation to the others. We'll use pyjanitor's transform_columns to apply the standard score calculation, (x - x.mean()) / x.std() , to each value in each of the columns we're evaluating. cars_df.transform_columns( [\"mpg\", \"disp_cc\", \"hp\"], lambda x: (x - x.mean()) / x.std(), elementwise=False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model mpg cyl disp_cc hp drat wt qsec vs am gear carb 30 Maserati Bora -0.844644 8 0.567039 2.746567 3.54 3.570 14.60 0 1 5 8 28 Ford Pantera L -0.711907 8 0.970465 1.711021 4.22 3.170 14.50 0 1 5 4 6 Duster 360 -0.960789 8 1.043081 1.433903 3.21 3.570 15.84 0 0 3 4 23 Camaro Z28 -1.126710 8 0.962396 1.433903 3.73 3.840 15.41 0 0 3 4 16 Chrysler Imperial -0.894420 8 1.688562 1.215126 3.23 5.345 17.42 0 0 3 4 15 Lincoln Continental -1.607883 8 1.849932 0.996348 3.00 5.424 17.82 0 0 3 4 14 Cadillac Fleetwood -1.607883 8 1.946754 0.850497 2.93 5.250 17.98 0 0 3 4 13 Merc 450SLC -0.811460 8 0.363713 0.485868 3.07 3.780 18.00 0 0 3 3 11 Merc 450SE -0.612354 8 0.363713 0.485868 3.07 4.070 17.40 0 0 3 3 12 Merc 450SL -0.463025 8 0.363713 0.485868 3.07 3.730 17.60 0 0 3 3 24 Pontiac Firebird -0.147774 8 1.365821 0.412942 3.08 3.845 17.05 0 0 3 2 4 Hornet Sportabout -0.230735 8 1.043081 0.412942 3.15 3.440 17.02 0 0 3 2 29 Ferrari Dino -0.064813 6 -0.691647 0.412942 3.62 2.770 15.50 0 1 5 6 21 Dodge Challenger -0.761683 8 0.704204 0.048313 2.76 3.520 16.87 0 0 3 2 22 AMC Javelin -0.811460 8 0.591245 0.048313 3.15 3.435 17.30 0 0 3 2 10 Merc 280C -0.380064 6 -0.509299 -0.345486 3.92 3.440 18.90 1 0 4 4 9 Merc 280 -0.147774 6 -0.509299 -0.345486 3.92 3.440 18.30 1 0 4 4 27 Lotus Europa 1.710547 4 -1.094266 -0.491337 3.77 1.513 16.90 1 1 5 2 0 Mazda RX4 0.150885 6 -0.570620 -0.535093 3.90 2.620 16.46 0 1 4 4 1 Mazda RX4 Wag 0.150885 6 -0.570620 -0.535093 3.90 2.875 17.02 0 1 4 4 3 Hornet 4 Drive 0.217253 6 0.220094 -0.535093 3.08 3.215 19.44 1 0 3 1 31 Volvo 142E 0.217253 4 -0.885292 -0.549678 4.11 2.780 18.60 1 1 4 2 5 Valiant -0.330287 6 -0.046167 -0.608019 2.76 3.460 20.22 1 0 3 1 20 Toyota Corona 0.233846 4 -0.892553 -0.724700 3.70 2.465 20.01 1 0 3 1 8 Merc 230 0.449543 4 -0.725535 -0.753870 3.92 3.150 22.90 1 0 4 2 2 Datsun 710 0.449543 4 -0.990182 -0.783040 3.85 2.320 18.61 1 1 4 1 26 Porsche 914-2 0.980492 4 -0.890939 -0.812211 4.43 2.140 16.70 0 1 5 2 25 Fiat X1-9 1.196190 4 -1.224169 -1.176840 4.08 1.935 18.90 1 1 4 1 17 Fiat 128 2.042389 4 -1.226589 -1.176840 4.08 2.200 19.47 1 1 4 1 19 Toyota Corolla 2.291272 4 -1.287910 -1.191425 4.22 1.835 19.90 1 1 4 1 7 Merc 240D 0.715018 4 -0.677931 -1.235180 3.69 3.190 20.00 1 0 4 2 18 Honda Civic 1.710547 4 -1.250795 -1.381032 4.93 1.615 18.52 1 1 4 2 Standardized mpg cars_df = cars_df.sort_values(\"mpg\", ascending=False) sns.barplot( y=\"model\", x=\"mpg\", data=cars_df, color=\"b\", orient=\"h\", ) <AxesSubplot:xlabel='mpg', ylabel='model'> Standardized displacement cars_df = cars_df.sort_values(\"disp_cc\", ascending=False) sns.barplot(y=\"model\", x=\"disp_cc\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='disp_cc', ylabel='model'> Standardized horsepower cars_df = cars_df.sort_values(\"hp\", ascending=False) sns.barplot(y=\"model\", x=\"hp\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='hp', ylabel='model'>","title":"Normalize"},{"location":"normalize/#normalization-and-standardization","text":"Normalization makes data more meaningful by converting absolute values into comparisons with related values. Chris Vallier has produced this demonstration of normalization using PyJanitor. pyjanitor functions demonstrated here: min_max_scale transform_columns import janitor import pandas as pd import seaborn as sns sns.set(style=\"whitegrid\")","title":"Normalization and Standardization"},{"location":"normalize/#load-data","text":"We'll use a dataset with fuel efficiency in miles per gallon (\"mpg\"), engine displacement in cubic centimeters (\"disp\"), and horsepower (\"hp\") for a variety of car models. It's a crazy, but customary, mix of units. csv_file = \"https://gist.githubusercontent.com/seankross/a412dfbd88b3db70b74b/raw/5f23f993cd87c283ce766e7ac6b329ee7cc2e1d1/mtcars.csv\" cars_df = pd.read_csv(csv_file) Quantities without units are dangerous, so let's use pyjanitor's rename_column ... cars_df = cars_df.rename_column(\"disp\", \"disp_cc\")","title":"Load data"},{"location":"normalize/#examine-raw-data","text":"cars_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model mpg cyl disp_cc hp drat wt qsec vs am gear carb 0 Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 1 Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 2 Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 3 Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 4 Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2","title":"Examine raw data"},{"location":"normalize/#visualize","text":"Each value makes more sense viewed in comparison to the other models. We'll use simple Seaborn bar plots.","title":"Visualize"},{"location":"normalize/#mpg-by-model","text":"cars_df = cars_df.sort_values(\"mpg\", ascending=False) sns.barplot( y=\"model\", x=\"mpg\", data=cars_df, color=\"b\", orient=\"h\", ) <AxesSubplot:xlabel='mpg', ylabel='model'>","title":"mpg by model"},{"location":"normalize/#displacement-by-model","text":"cars_df = cars_df.sort_values(\"disp_cc\", ascending=False) sns.barplot(y=\"model\", x=\"disp_cc\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='disp_cc', ylabel='model'>","title":"displacement by model"},{"location":"normalize/#horsepower-by-model","text":"cars_df = cars_df.sort_values(\"hp\", ascending=False) sns.barplot(y=\"model\", x=\"hp\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='hp', ylabel='model'>","title":"horsepower by model"},{"location":"normalize/#min-max-normalization","text":"First we'll use pyjanitor's min_max_scale to rescale the mpg , disp_cc , and hp columns in-place so that each value varies from 0 to 1. ( cars_df.min_max_scale(col_name=\"mpg\", new_max=1, new_min=0) .min_max_scale(col_name=\"disp_cc\", new_max=1, new_min=0) .min_max_scale(col_name=\"hp\", new_max=1, new_min=0) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model mpg cyl disp_cc hp drat wt qsec vs am gear carb 30 Maserati Bora 0.195745 8 0.573460 1.000000 3.54 3.570 14.60 0 1 5 8 28 Ford Pantera L 0.229787 8 0.698179 0.749117 4.22 3.170 14.50 0 1 5 4 6 Duster 360 0.165957 8 0.720629 0.681979 3.21 3.570 15.84 0 0 3 4 23 Camaro Z28 0.123404 8 0.695685 0.681979 3.73 3.840 15.41 0 0 3 4 16 Chrysler Imperial 0.182979 8 0.920180 0.628975 3.23 5.345 17.42 0 0 3 4 15 Lincoln Continental 0.000000 8 0.970067 0.575972 3.00 5.424 17.82 0 0 3 4 14 Cadillac Fleetwood 0.000000 8 1.000000 0.540636 2.93 5.250 17.98 0 0 3 4 13 Merc 450SLC 0.204255 8 0.510601 0.452297 3.07 3.780 18.00 0 0 3 3 11 Merc 450SE 0.255319 8 0.510601 0.452297 3.07 4.070 17.40 0 0 3 3 12 Merc 450SL 0.293617 8 0.510601 0.452297 3.07 3.730 17.60 0 0 3 3 24 Pontiac Firebird 0.374468 8 0.820404 0.434629 3.08 3.845 17.05 0 0 3 2 4 Hornet Sportabout 0.353191 8 0.720629 0.434629 3.15 3.440 17.02 0 0 3 2 29 Ferrari Dino 0.395745 6 0.184335 0.434629 3.62 2.770 15.50 0 1 5 6 21 Dodge Challenger 0.217021 8 0.615864 0.346290 2.76 3.520 16.87 0 0 3 2 22 AMC Javelin 0.204255 8 0.580943 0.346290 3.15 3.435 17.30 0 0 3 2 10 Merc 280C 0.314894 6 0.240708 0.250883 3.92 3.440 18.90 1 0 4 4 9 Merc 280 0.374468 6 0.240708 0.250883 3.92 3.440 18.30 1 0 4 4 27 Lotus Europa 0.851064 4 0.059865 0.215548 3.77 1.513 16.90 1 1 5 2 0 Mazda RX4 0.451064 6 0.221751 0.204947 3.90 2.620 16.46 0 1 4 4 1 Mazda RX4 Wag 0.451064 6 0.221751 0.204947 3.90 2.875 17.02 0 1 4 4 3 Hornet 4 Drive 0.468085 6 0.466201 0.204947 3.08 3.215 19.44 1 0 3 1 31 Volvo 142E 0.468085 4 0.124470 0.201413 4.11 2.780 18.60 1 1 4 2 5 Valiant 0.327660 6 0.383886 0.187279 2.76 3.460 20.22 1 0 3 1 20 Toyota Corona 0.472340 4 0.122225 0.159011 3.70 2.465 20.01 1 0 3 1 8 Merc 230 0.527660 4 0.173859 0.151943 3.92 3.150 22.90 1 0 4 2 2 Datsun 710 0.527660 4 0.092043 0.144876 3.85 2.320 18.61 1 1 4 1 26 Porsche 914-2 0.663830 4 0.122724 0.137809 4.43 2.140 16.70 0 1 5 2 25 Fiat X1-9 0.719149 4 0.019706 0.049470 4.08 1.935 18.90 1 1 4 1 17 Fiat 128 0.936170 4 0.018957 0.049470 4.08 2.200 19.47 1 1 4 1 19 Toyota Corolla 1.000000 4 0.000000 0.045936 4.22 1.835 19.90 1 1 4 1 7 Merc 240D 0.595745 4 0.188576 0.035336 3.69 3.190 20.00 1 0 4 2 18 Honda Civic 0.851064 4 0.011474 0.000000 4.93 1.615 18.52 1 1 4 2 The shapes of the bar graphs remain the same, but the horizontal axes show the new scale.","title":"min-max normalization"},{"location":"normalize/#mpg-min-max-normalized","text":"cars_df = cars_df.sort_values(\"mpg\", ascending=False) sns.barplot(y=\"model\", x=\"mpg\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='mpg', ylabel='model'>","title":"mpg (min-max normalized)"},{"location":"normalize/#displacement-min-max-normalized","text":"cars_df = cars_df.sort_values(\"disp_cc\", ascending=False) sns.barplot(y=\"model\", x=\"disp_cc\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='disp_cc', ylabel='model'>","title":"displacement (min-max normalized)"},{"location":"normalize/#horsepower-min-max-normalized","text":"cars_df = cars_df.sort_values(\"hp\", ascending=False) sns.barplot(y=\"model\", x=\"hp\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='hp', ylabel='model'>","title":"horsepower (min-max normalized)"},{"location":"normalize/#standardization-z-score","text":"Next we'll convert to standard scores . This expresses each value in terms of its standard deviations from the mean, expressing where each model stands in relation to the others. We'll use pyjanitor's transform_columns to apply the standard score calculation, (x - x.mean()) / x.std() , to each value in each of the columns we're evaluating. cars_df.transform_columns( [\"mpg\", \"disp_cc\", \"hp\"], lambda x: (x - x.mean()) / x.std(), elementwise=False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } model mpg cyl disp_cc hp drat wt qsec vs am gear carb 30 Maserati Bora -0.844644 8 0.567039 2.746567 3.54 3.570 14.60 0 1 5 8 28 Ford Pantera L -0.711907 8 0.970465 1.711021 4.22 3.170 14.50 0 1 5 4 6 Duster 360 -0.960789 8 1.043081 1.433903 3.21 3.570 15.84 0 0 3 4 23 Camaro Z28 -1.126710 8 0.962396 1.433903 3.73 3.840 15.41 0 0 3 4 16 Chrysler Imperial -0.894420 8 1.688562 1.215126 3.23 5.345 17.42 0 0 3 4 15 Lincoln Continental -1.607883 8 1.849932 0.996348 3.00 5.424 17.82 0 0 3 4 14 Cadillac Fleetwood -1.607883 8 1.946754 0.850497 2.93 5.250 17.98 0 0 3 4 13 Merc 450SLC -0.811460 8 0.363713 0.485868 3.07 3.780 18.00 0 0 3 3 11 Merc 450SE -0.612354 8 0.363713 0.485868 3.07 4.070 17.40 0 0 3 3 12 Merc 450SL -0.463025 8 0.363713 0.485868 3.07 3.730 17.60 0 0 3 3 24 Pontiac Firebird -0.147774 8 1.365821 0.412942 3.08 3.845 17.05 0 0 3 2 4 Hornet Sportabout -0.230735 8 1.043081 0.412942 3.15 3.440 17.02 0 0 3 2 29 Ferrari Dino -0.064813 6 -0.691647 0.412942 3.62 2.770 15.50 0 1 5 6 21 Dodge Challenger -0.761683 8 0.704204 0.048313 2.76 3.520 16.87 0 0 3 2 22 AMC Javelin -0.811460 8 0.591245 0.048313 3.15 3.435 17.30 0 0 3 2 10 Merc 280C -0.380064 6 -0.509299 -0.345486 3.92 3.440 18.90 1 0 4 4 9 Merc 280 -0.147774 6 -0.509299 -0.345486 3.92 3.440 18.30 1 0 4 4 27 Lotus Europa 1.710547 4 -1.094266 -0.491337 3.77 1.513 16.90 1 1 5 2 0 Mazda RX4 0.150885 6 -0.570620 -0.535093 3.90 2.620 16.46 0 1 4 4 1 Mazda RX4 Wag 0.150885 6 -0.570620 -0.535093 3.90 2.875 17.02 0 1 4 4 3 Hornet 4 Drive 0.217253 6 0.220094 -0.535093 3.08 3.215 19.44 1 0 3 1 31 Volvo 142E 0.217253 4 -0.885292 -0.549678 4.11 2.780 18.60 1 1 4 2 5 Valiant -0.330287 6 -0.046167 -0.608019 2.76 3.460 20.22 1 0 3 1 20 Toyota Corona 0.233846 4 -0.892553 -0.724700 3.70 2.465 20.01 1 0 3 1 8 Merc 230 0.449543 4 -0.725535 -0.753870 3.92 3.150 22.90 1 0 4 2 2 Datsun 710 0.449543 4 -0.990182 -0.783040 3.85 2.320 18.61 1 1 4 1 26 Porsche 914-2 0.980492 4 -0.890939 -0.812211 4.43 2.140 16.70 0 1 5 2 25 Fiat X1-9 1.196190 4 -1.224169 -1.176840 4.08 1.935 18.90 1 1 4 1 17 Fiat 128 2.042389 4 -1.226589 -1.176840 4.08 2.200 19.47 1 1 4 1 19 Toyota Corolla 2.291272 4 -1.287910 -1.191425 4.22 1.835 19.90 1 1 4 1 7 Merc 240D 0.715018 4 -0.677931 -1.235180 3.69 3.190 20.00 1 0 4 2 18 Honda Civic 1.710547 4 -1.250795 -1.381032 4.93 1.615 18.52 1 1 4 2","title":"Standardization (z-score)"},{"location":"normalize/#standardized-mpg","text":"cars_df = cars_df.sort_values(\"mpg\", ascending=False) sns.barplot( y=\"model\", x=\"mpg\", data=cars_df, color=\"b\", orient=\"h\", ) <AxesSubplot:xlabel='mpg', ylabel='model'>","title":"Standardized mpg"},{"location":"normalize/#standardized-displacement","text":"cars_df = cars_df.sort_values(\"disp_cc\", ascending=False) sns.barplot(y=\"model\", x=\"disp_cc\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='disp_cc', ylabel='model'>","title":"Standardized displacement"},{"location":"normalize/#standardized-horsepower","text":"cars_df = cars_df.sort_values(\"hp\", ascending=False) sns.barplot(y=\"model\", x=\"hp\", data=cars_df, color=\"b\", orient=\"h\") <AxesSubplot:xlabel='hp', ylabel='model'>","title":"Standardized horsepower"},{"location":"pivot_long_wide/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Pivot Data from Long to Wide Form import janitor as jn import pandas as pd df = [ {\"name\": \"Alice\", \"variable\": \"wk1\", \"value\": 5}, {\"name\": \"Alice\", \"variable\": \"wk2\", \"value\": 9}, {\"name\": \"Alice\", \"variable\": \"wk3\", \"value\": 20}, {\"name\": \"Alice\", \"variable\": \"wk4\", \"value\": 22}, {\"name\": \"Bob\", \"variable\": \"wk1\", \"value\": 7}, {\"name\": \"Bob\", \"variable\": \"wk2\", \"value\": 11}, {\"name\": \"Bob\", \"variable\": \"wk3\", \"value\": 17}, {\"name\": \"Bob\", \"variable\": \"wk4\", \"value\": 33}, {\"name\": \"Carla\", \"variable\": \"wk1\", \"value\": 6}, {\"name\": \"Carla\", \"variable\": \"wk2\", \"value\": 13}, {\"name\": \"Carla\", \"variable\": \"wk3\", \"value\": 39}, {\"name\": \"Carla\", \"variable\": \"wk4\", \"value\": 40}, ] df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name variable value 0 Alice wk1 5 1 Alice wk2 9 2 Alice wk3 20 3 Alice wk4 22 4 Bob wk1 7 5 Bob wk2 11 6 Bob wk3 17 7 Bob wk4 33 8 Carla wk1 6 9 Carla wk2 13 10 Carla wk3 39 11 Carla wk4 40 Reshaping to wide form: df.pivot_wider(index=\"name\", names_from=\"variable\", values_from=\"value\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name wk1 wk2 wk3 wk4 0 Alice 5 9 20 22 1 Bob 7 11 17 33 2 Carla 6 13 39 40 Pivoting on multiple columns is possible : df = [ {\"name\": 1, \"n\": 10.0, \"pct\": 0.1}, {\"name\": 2, \"n\": 20.0, \"pct\": 0.2}, {\"name\": 3, \"n\": 30.0, \"pct\": 0.3}, ] df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name n pct 0 1 10.0 0.1 1 2 20.0 0.2 2 3 30.0 0.3 ( df.assign(num=0).pivot_wider( index=\"num\", names_from=\"name\", values_from=[\"n\", \"pct\"], names_sep=\"_\" ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num n_1 n_2 n_3 pct_1 pct_2 pct_3 0 0 10.0 20.0 30.0 0.1 0.2 0.3 You may choose not to flatten the columns, by setting flatten_levels to False : df = [ {\"dep\": 5.5, \"step\": 1, \"a\": 20, \"b\": 30}, {\"dep\": 5.5, \"step\": 2, \"a\": 25, \"b\": 37}, {\"dep\": 6.1, \"step\": 1, \"a\": 22, \"b\": 19}, {\"dep\": 6.1, \"step\": 2, \"a\": 18, \"b\": 29}, ] df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep step a b 0 5.5 1 20 30 1 5.5 2 25 37 2 6.1 1 22 19 3 6.1 2 18 29 df.pivot_wider(index=\"dep\", names_from=\"step\", flatten_levels=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } a b step 1 2 1 2 dep 5.5 20 25 30 37 6.1 22 18 19 29 The order of the levels can be changed with the levels_order parameter, which internally uses pandas' reorder_levels : df.pivot_wider( index=\"dep\", names_from=\"step\", flatten_levels=False, levels_order=[\"step\", None] ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } step 1 2 1 2 a a b b dep 5.5 20 25 30 37 6.1 22 18 19 29 df.pivot_wider( index=\"dep\", names_from=\"step\", flatten_levels=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 df.pivot_wider( index=\"dep\", names_from=\"step\", flatten_levels=True, levels_order=[\"step\", None] ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep 1_a 2_a 1_b 2_b 0 5.5 20 25 30 37 1 6.1 22 18 19 29 names_sep and names_glue come in handy in situations where names_from and/or values_from contain multiple variables; it is used primarily when the columns are flattened. The default value for names_sep is _ : # default value of names_sep is '_' df.pivot_wider(index=\"dep\", names_from=\"step\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 df.pivot_wider(index=\"dep\", names_from=\"step\", names_sep=\"\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a1 a2 b1 b2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 With names_glue you can glue the individual levels (if MultiIndex) into one (similar to names_sep ), or you can modify the final columns, as long as it can be passed to pd.Index.map : # replicate `names_sep` df.pivot_wider(index=\"dep\", names_from=\"step\", names_sep=None, names_glue=\"_\".join) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 # going beyond names_sep df.pivot_wider( index=\"dep\", names_from=\"step\", names_sep=None, names_glue=lambda col: f\"{col[0]}_step{col[1]}\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a_step1 a_step2 b_step1 b_step2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 There are scenarios where the column order of the final dataframe is important: df = [ {\"Salesman\": \"Knut\", \"Height\": 6, \"product\": \"bat\", \"price\": 5}, {\"Salesman\": \"Knut\", \"Height\": 6, \"product\": \"ball\", \"price\": 1}, {\"Salesman\": \"Knut\", \"Height\": 6, \"product\": \"wand\", \"price\": 3}, {\"Salesman\": \"Steve\", \"Height\": 5, \"product\": \"pen\", \"price\": 2}, ] df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Salesman Height product price 0 Knut 6 bat 5 1 Knut 6 ball 1 2 Knut 6 wand 3 3 Steve 5 pen 2 idx = df.groupby([\"Salesman\", \"Height\"]).cumcount().add(1) (df.assign(idx=idx).pivot_wider(index=[\"Salesman\", \"Height\"], names_from=\"idx\")) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Salesman Height product_1 product_2 product_3 price_1 price_2 price_3 0 Knut 6 bat ball wand 5.0 1.0 3.0 1 Steve 5 pen NaN NaN 2.0 NaN NaN To get the columns in a form where product alternates with price , we can combine pivot_wider (or plain pd.pivot ) with pd.sort_index and janitor.collapse_levels : ( df.assign(idx=idx) .pivot_wider(index=[\"Salesman\", \"Height\"], names_from=\"idx\", flatten_levels=False) .sort_index(level=\"idx\", axis=\"columns\", sort_remaining=False) .collapse_levels() .reset_index() ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Salesman Height product_1 price_1 product_2 price_2 product_3 price_3 0 Knut 6 bat 5.0 ball 1.0 wand 3.0 1 Steve 5 pen 2.0 NaN NaN NaN NaN df = pd.DataFrame( { \"geoid\": [1, 1, 13, 13], \"name\": [\"Alabama\", \"Alabama\", \"Georgia\", \"Georgia\"], \"variable\": [ \"pop_renter\", \"median_rent\", \"pop_renter\", \"median_rent\", ], \"estimate\": [1434765, 747, 3592422, 927], \"error\": [16736, 3, 33385, 3], } ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geoid name variable estimate error 0 1 Alabama pop_renter 1434765 16736 1 1 Alabama median_rent 747 3 2 13 Georgia pop_renter 3592422 33385 3 13 Georgia median_rent 927 3 df.pivot_wider( index=[\"geoid\", \"name\"], names_from=\"variable\", values_from=[\"estimate\", \"error\"], levels_order=[\"variable\", None], ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geoid name median_rent_estimate pop_renter_estimate median_rent_error pop_renter_error 0 1 Alabama 747 1434765 3 16736 1 13 Georgia 927 3592422 3 33385 For the reshaping above, we would like to maintain the order in variable , where pop_renter comes before median_rent ; this can be achieved by converting the variable column to a categorical, before reshaping: ( df.encode_categorical(variable=(None, \"appearance\")).pivot_wider( index=[\"geoid\", \"name\"], names_from=\"variable\", values_from=[\"estimate\", \"error\"], levels_order=[\"variable\", None], ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geoid name pop_renter_estimate median_rent_estimate pop_renter_error median_rent_error 0 1 Alabama 1434765 747 16736 3 1 13 Georgia 3592422 927 33385 3","title":"Pivot long wide"},{"location":"pivot_long_wide/#pivot-data-from-long-to-wide-form","text":"import janitor as jn import pandas as pd df = [ {\"name\": \"Alice\", \"variable\": \"wk1\", \"value\": 5}, {\"name\": \"Alice\", \"variable\": \"wk2\", \"value\": 9}, {\"name\": \"Alice\", \"variable\": \"wk3\", \"value\": 20}, {\"name\": \"Alice\", \"variable\": \"wk4\", \"value\": 22}, {\"name\": \"Bob\", \"variable\": \"wk1\", \"value\": 7}, {\"name\": \"Bob\", \"variable\": \"wk2\", \"value\": 11}, {\"name\": \"Bob\", \"variable\": \"wk3\", \"value\": 17}, {\"name\": \"Bob\", \"variable\": \"wk4\", \"value\": 33}, {\"name\": \"Carla\", \"variable\": \"wk1\", \"value\": 6}, {\"name\": \"Carla\", \"variable\": \"wk2\", \"value\": 13}, {\"name\": \"Carla\", \"variable\": \"wk3\", \"value\": 39}, {\"name\": \"Carla\", \"variable\": \"wk4\", \"value\": 40}, ] df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name variable value 0 Alice wk1 5 1 Alice wk2 9 2 Alice wk3 20 3 Alice wk4 22 4 Bob wk1 7 5 Bob wk2 11 6 Bob wk3 17 7 Bob wk4 33 8 Carla wk1 6 9 Carla wk2 13 10 Carla wk3 39 11 Carla wk4 40 Reshaping to wide form: df.pivot_wider(index=\"name\", names_from=\"variable\", values_from=\"value\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name wk1 wk2 wk3 wk4 0 Alice 5 9 20 22 1 Bob 7 11 17 33 2 Carla 6 13 39 40 Pivoting on multiple columns is possible : df = [ {\"name\": 1, \"n\": 10.0, \"pct\": 0.1}, {\"name\": 2, \"n\": 20.0, \"pct\": 0.2}, {\"name\": 3, \"n\": 30.0, \"pct\": 0.3}, ] df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name n pct 0 1 10.0 0.1 1 2 20.0 0.2 2 3 30.0 0.3 ( df.assign(num=0).pivot_wider( index=\"num\", names_from=\"name\", values_from=[\"n\", \"pct\"], names_sep=\"_\" ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } num n_1 n_2 n_3 pct_1 pct_2 pct_3 0 0 10.0 20.0 30.0 0.1 0.2 0.3 You may choose not to flatten the columns, by setting flatten_levels to False : df = [ {\"dep\": 5.5, \"step\": 1, \"a\": 20, \"b\": 30}, {\"dep\": 5.5, \"step\": 2, \"a\": 25, \"b\": 37}, {\"dep\": 6.1, \"step\": 1, \"a\": 22, \"b\": 19}, {\"dep\": 6.1, \"step\": 2, \"a\": 18, \"b\": 29}, ] df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep step a b 0 5.5 1 20 30 1 5.5 2 25 37 2 6.1 1 22 19 3 6.1 2 18 29 df.pivot_wider(index=\"dep\", names_from=\"step\", flatten_levels=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } a b step 1 2 1 2 dep 5.5 20 25 30 37 6.1 22 18 19 29 The order of the levels can be changed with the levels_order parameter, which internally uses pandas' reorder_levels : df.pivot_wider( index=\"dep\", names_from=\"step\", flatten_levels=False, levels_order=[\"step\", None] ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; } step 1 2 1 2 a a b b dep 5.5 20 25 30 37 6.1 22 18 19 29 df.pivot_wider( index=\"dep\", names_from=\"step\", flatten_levels=True, ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 df.pivot_wider( index=\"dep\", names_from=\"step\", flatten_levels=True, levels_order=[\"step\", None] ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep 1_a 2_a 1_b 2_b 0 5.5 20 25 30 37 1 6.1 22 18 19 29 names_sep and names_glue come in handy in situations where names_from and/or values_from contain multiple variables; it is used primarily when the columns are flattened. The default value for names_sep is _ : # default value of names_sep is '_' df.pivot_wider(index=\"dep\", names_from=\"step\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 df.pivot_wider(index=\"dep\", names_from=\"step\", names_sep=\"\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a1 a2 b1 b2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 With names_glue you can glue the individual levels (if MultiIndex) into one (similar to names_sep ), or you can modify the final columns, as long as it can be passed to pd.Index.map : # replicate `names_sep` df.pivot_wider(index=\"dep\", names_from=\"step\", names_sep=None, names_glue=\"_\".join) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a_1 a_2 b_1 b_2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 # going beyond names_sep df.pivot_wider( index=\"dep\", names_from=\"step\", names_sep=None, names_glue=lambda col: f\"{col[0]}_step{col[1]}\", ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dep a_step1 a_step2 b_step1 b_step2 0 5.5 20 25 30 37 1 6.1 22 18 19 29 There are scenarios where the column order of the final dataframe is important: df = [ {\"Salesman\": \"Knut\", \"Height\": 6, \"product\": \"bat\", \"price\": 5}, {\"Salesman\": \"Knut\", \"Height\": 6, \"product\": \"ball\", \"price\": 1}, {\"Salesman\": \"Knut\", \"Height\": 6, \"product\": \"wand\", \"price\": 3}, {\"Salesman\": \"Steve\", \"Height\": 5, \"product\": \"pen\", \"price\": 2}, ] df = pd.DataFrame(df) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Salesman Height product price 0 Knut 6 bat 5 1 Knut 6 ball 1 2 Knut 6 wand 3 3 Steve 5 pen 2 idx = df.groupby([\"Salesman\", \"Height\"]).cumcount().add(1) (df.assign(idx=idx).pivot_wider(index=[\"Salesman\", \"Height\"], names_from=\"idx\")) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Salesman Height product_1 product_2 product_3 price_1 price_2 price_3 0 Knut 6 bat ball wand 5.0 1.0 3.0 1 Steve 5 pen NaN NaN 2.0 NaN NaN To get the columns in a form where product alternates with price , we can combine pivot_wider (or plain pd.pivot ) with pd.sort_index and janitor.collapse_levels : ( df.assign(idx=idx) .pivot_wider(index=[\"Salesman\", \"Height\"], names_from=\"idx\", flatten_levels=False) .sort_index(level=\"idx\", axis=\"columns\", sort_remaining=False) .collapse_levels() .reset_index() ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Salesman Height product_1 price_1 product_2 price_2 product_3 price_3 0 Knut 6 bat 5.0 ball 1.0 wand 3.0 1 Steve 5 pen 2.0 NaN NaN NaN NaN df = pd.DataFrame( { \"geoid\": [1, 1, 13, 13], \"name\": [\"Alabama\", \"Alabama\", \"Georgia\", \"Georgia\"], \"variable\": [ \"pop_renter\", \"median_rent\", \"pop_renter\", \"median_rent\", ], \"estimate\": [1434765, 747, 3592422, 927], \"error\": [16736, 3, 33385, 3], } ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geoid name variable estimate error 0 1 Alabama pop_renter 1434765 16736 1 1 Alabama median_rent 747 3 2 13 Georgia pop_renter 3592422 33385 3 13 Georgia median_rent 927 3 df.pivot_wider( index=[\"geoid\", \"name\"], names_from=\"variable\", values_from=[\"estimate\", \"error\"], levels_order=[\"variable\", None], ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geoid name median_rent_estimate pop_renter_estimate median_rent_error pop_renter_error 0 1 Alabama 747 1434765 3 16736 1 13 Georgia 927 3592422 3 33385 For the reshaping above, we would like to maintain the order in variable , where pop_renter comes before median_rent ; this can be achieved by converting the variable column to a categorical, before reshaping: ( df.encode_categorical(variable=(None, \"appearance\")).pivot_wider( index=[\"geoid\", \"name\"], names_from=\"variable\", values_from=[\"estimate\", \"error\"], levels_order=[\"variable\", None], ) ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } geoid name pop_renter_estimate median_rent_estimate pop_renter_error median_rent_error 0 1 Alabama 1434765 747 16736 3 1 13 Georgia 3592422 927 33385 3","title":"Pivot Data from Long to Wide Form"},{"location":"pyjanitor_intro/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); pyjanitor Usage Walkthrough pyjanitor is a Python-based API on top of pandas inspired by the janitor R package. It aims to provide a clean, understandable interface based on method chaining for common and less-common tasks involving data cleaning and DataFrame manipulation. The core philosophy and augmentations on pandas ' approach to data cleaning and DataFrame manipulation include: A method-chaining paradigm for coding efficiency & clarity of code through greatly improved readability. Implementation of common, useful DataFrame manipulation tasks that saves on repetitive code. Focus on active tense / verb approaches to function naming to provide at-a-glance understanding of a data manipulation pipeline. Why pyjanitor ? Originally a simple port of the R package, pyjanitor has evolved from a set of convenient data cleaning routines into an experiment with the method-chaining paradigm. Data preprocessing is best expressed as a directed acyclic graph (DAG) of actions taken on data. We take a base data file as the starting point and perform actions on it such as removing null/empty rows, replacing them with other values, adding/renaming/removing columns of data, filtering rows, and more. The pandas API has been invaluable for the Python data science ecosystem and implements method chaining for a subset of methods as part of the API. For example, resetting indexes ( .reset_index() ), dropping null values ( .dropna() ), and more are accomplished via the appropriate pd.DataFrame method calls. Inspired by the R statistical language ecosystem, where consistent and good API design in the dplyr package enables end-users, who are not necessarily developers, to concisely express data processing code, pyjanitor has evolved into a language for expressing the data processing DAG for pandas users. What is method chaining? To accomplish these goals, actions for which we would need to invoke imperative-style statements can be replaced with method chains that allow the user to read off the logical order of actions taken. Note the annotated example below. First, we introduce the textual description of a sample data cleaning pipeline: Create DataFrame . Delete one column. Drop rows with empty values in two particular columns. Rename another two columns. Add a new column. Reset index to account for the missing row we removed above In pandas code, this would look as such: df = pd.DataFrame(...) # create a pandas DataFrame somehow. del df['column1'] # delete a column from the dataframe. df = df.dropna(subset=['column2', 'column3']) # drop rows that have empty values in column 2 and 3. df = df.rename({'column2': 'unicorns', 'column3': 'dragons'}) # rename column2 and column3 df['new_column'] = ['iterable', 'of', 'items'] # add a new column. df.reset_index(inplace=True, drop=True) # reset index to account for the missing row we removed above The pyjanitor approach With pyjanitor , we enable method chaining with method names that are verbs which describe the action taken: df = ( pd.DataFrame(...) .remove_columns(['column1']) .dropna(subset=['column2', 'column3']) .rename_column('column2', 'unicorns') .rename_column('column3', 'dragons') .add_column('new_column', ['iterable', 'of', 'items']) .reset_index(drop=True) ) We believe the pyjanitor chaining-based approach leads to much cleaner code where the intent of a series of DataFrame manipulations is much more immediately clear. pyjanitor \u2019s etymology has a two-fold relationship to \u201ccleanliness\u201d. Firstly, it\u2019s about extending pandas with convenient data cleaning routines. Secondly, it\u2019s about providing a cleaner, method-chaining, verb-based API for common pandas routines. A survey of pyjanitor functions Cleaning column names (multi-indexes are possible!) Removing empty rows and columns Identifying duplicate entries Encoding columns as categorical Splitting your data into features and targets (for machine learning) Adding, removing, and renaming columns Coalesce multiple columns into a single column Convert excel date (serial format) into a Python datetime format Expand a single column that has delimited, categorical values into dummy-encoded variables A full list of functionality that pyjanitor implements can be found in the API docs . Some things that are different Some pyjanitor methods are DataFrame -mutating operations, i.e., in place. Given that in a method-chaining paradigm, DataFrame s that would be created at each step of the chain cannot be accessed anyway, duplication of data at each step would lead to unnecessary, potential considerable slowdowns and increased memory usage due to data-copying operations. The severity of such copying scales with DataFrame size. Take care to understand which functions change the original DataFrame you are chaining on if it is necessary to preserve that data. If it is, you can simply .copy() it as the first step in a df.copy().operation1().operation2()... chain. How it works pyjanitor relies on the Pandas Flavor package to register new functions as object methods that can be called directly on DataFrame s. For example: import pandas as pd import pandas_flavor as pf @pf.register_dataframe_method def remove_column(df, column_name: str): del df[column_name] return df df = ( pd.read_csv('my_data.csv') .remove_column('my_column_name') .operation2(...) ) Importing the janitor package immediately registers these functions. The fact that each DataFrame method pyjanitor registers returns the DataFrame is what gives it the capability to method chain. Note that pyjanitor explicitly does not modify any existing pandas methods / functionality. Demo of various DataFrame manipulation tasks using pyjanitor Here, we'll walk through some useful pyjanitor -based approaches to cleaning and manipulating DataFrame s. Code preamble: import janitor import numpy as np import pandas as pd from pyprojroot import here Let's take a look at our dataset: df = pd.read_excel(here() / \"data/dirty_data.xlsx\", engine=\"openpyxl\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Last Name Employee Status Subject Hire Date % Allocated Full time? do not edit! ---> Certification Certification.1 Certification.2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN 2 Alicia Keys Teacher Music 37118.0 1.00 Yes NaN Instr. music Vocal music NaN 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes NaN PENDING Computers NaN 4 Desus Nice Administration Dean 41431.0 1.00 Yes NaN PENDING NaN NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 7 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 James Joyce Teacher English 32994.0 0.50 No NaN NaN English 6-12 NaN 9 Hedy Lamarr Teacher Science 27919.0 0.50 No NaN PENDING NaN NaN 10 Carlos Boozer Coach Basketball 42221.0 NaN No NaN Physical ed NaN NaN 11 Young Boozer Coach NaN 34700.0 NaN No NaN NaN Political sci. NaN 12 Micheal Larsen Teacher English 40071.0 0.80 No NaN Vocal music English NaN We can see that this dataset is dirty in a number of ways, including the following: Column names contain spaces, punctuation marks, and inconsistent casing One row ( 7 ) with completely missing data One column ( do not edit! ---> ) with completely missing data Clean up our data using a pyjanitor method-chaining pipeline Let's run through a demo DataFrame cleaning procedure: cleaned_df = ( pd.read_excel(here() / \"data/dirty_data.xlsx\", engine=\"openpyxl\") .clean_names() .remove_empty() .rename_column(\"%_allocated\", \"percent_allocated\") .rename_column(\"full_time_\", \"full_time\") .coalesce([\"certification\", \"certification_1\"], \"certification\") .encode_categorical([\"subject\", \"employee_status\", \"full_time\"]) .convert_excel_date(\"hire_date\") .reset_index(drop=True) ) cleaned_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 2008-08-30 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 2008-08-30 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 2001-08-15 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 1975-05-01 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 2013-06-06 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 1930-03-20 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 1990-05-01 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 1976-06-08 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 2015-08-05 NaN No Physical ed NaN 10 Young Boozer Coach NaN 1995-01-01 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 2009-09-15 0.80 No Vocal music English The cleaned DataFrame looks much better and quite a bit more usable for our downstream tasks. Step-by-step walkthrough of pyjanitor DataFrame manipulations Just for clearer understanding of the above, let's see how pyjanitor progressively modified the data. Loading data in: df = pd.read_excel(here() / \"data/dirty_data.xlsx\", engine=\"openpyxl\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Last Name Employee Status Subject Hire Date % Allocated Full time? do not edit! ---> Certification Certification.1 Certification.2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN 2 Alicia Keys Teacher Music 37118.0 1.00 Yes NaN Instr. music Vocal music NaN 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes NaN PENDING Computers NaN 4 Desus Nice Administration Dean 41431.0 1.00 Yes NaN PENDING NaN NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 7 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 James Joyce Teacher English 32994.0 0.50 No NaN NaN English 6-12 NaN 9 Hedy Lamarr Teacher Science 27919.0 0.50 No NaN PENDING NaN NaN 10 Carlos Boozer Coach Basketball 42221.0 NaN No NaN Physical ed NaN NaN 11 Young Boozer Coach NaN 34700.0 NaN No NaN NaN Political sci. NaN 12 Micheal Larsen Teacher English 40071.0 0.80 No NaN Vocal music English NaN Clean up names by removing whitespace, punctuation / symbols, capitalization: df = df.clean_names() df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date %_allocated full_time_ do_not_edit!_> certification certification_1 certification_2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN 2 Alicia Keys Teacher Music 37118.0 1.00 Yes NaN Instr. music Vocal music NaN 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes NaN PENDING Computers NaN 4 Desus Nice Administration Dean 41431.0 1.00 Yes NaN PENDING NaN NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 7 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 James Joyce Teacher English 32994.0 0.50 No NaN NaN English 6-12 NaN 9 Hedy Lamarr Teacher Science 27919.0 0.50 No NaN PENDING NaN NaN 10 Carlos Boozer Coach Basketball 42221.0 NaN No NaN Physical ed NaN NaN 11 Young Boozer Coach NaN 34700.0 NaN No NaN NaN Political sci. NaN 12 Micheal Larsen Teacher English 40071.0 0.80 No NaN Vocal music English NaN Remove entirely empty rows / columns: df = df.remove_empty() df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date %_allocated full_time_ certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.50 No NaN English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 42221.0 NaN No Physical ed NaN 10 Young Boozer Coach NaN 34700.0 NaN No NaN Political sci. 11 Micheal Larsen Teacher English 40071.0 0.80 No Vocal music English Rename particular columns: df = df.rename_column(\"%_allocated\", \"percent_allocated\").rename_column( \"full_time_\", \"full_time\" ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.50 No NaN English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 42221.0 NaN No Physical ed NaN 10 Young Boozer Coach NaN 34700.0 NaN No NaN Political sci. 11 Micheal Larsen Teacher English 40071.0 0.80 No Vocal music English Take first non- NaN row value in two columns: df = df.coalesce([\"certification\", \"certification_1\"], \"certification\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 42221.0 NaN No Physical ed NaN 10 Young Boozer Coach NaN 34700.0 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 40071.0 0.80 No Vocal music English Convert string object rows to categorical to save on memory consumption and speed up access: df.dtypes first_name object last_name object employee_status object subject object hire_date float64 percent_allocated float64 full_time object certification object certification_1 object dtype: object df.encode_categorical([\"subject\", \"employee_status\", \"full_time\"]) df.dtypes first_name object last_name object employee_status object subject object hire_date float64 percent_allocated float64 full_time object certification object certification_1 object dtype: object Convert Excel date-formatted column to a more interpretable format: df.convert_excel_date(\"hire_date\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 2008-08-30 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 2008-08-30 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 2001-08-15 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 1975-05-01 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 2013-06-06 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 1930-03-20 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 1990-05-01 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 1976-06-08 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 2015-08-05 NaN No Physical ed NaN 10 Young Boozer Coach NaN 1995-01-01 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 2009-09-15 0.80 No Vocal music English Example analysis of the data Let's perform analysis on the above, cleaned DataFrame . First we add some additional, randomly-generated data. Note that we .copy() the original to preserve it, given that the following would otherwise modify it: data_df = cleaned_df.copy().add_columns( lucky_number=np.random.randint(0, 10, len(cleaned_df)), age=np.random.randint(10, 100, len(cleaned_df)), employee_of_month_count=np.random.randint(0, 5, len(cleaned_df)), ) data_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 lucky_number age employee_of_month_count 0 Jason Bourne Teacher PE 2008-08-30 0.75 Yes Physical ed Theater 4 38 4 1 Jason Bourne Teacher Drafting 2008-08-30 0.25 Yes Physical ed Theater 2 95 3 2 Alicia Keys Teacher Music 2001-08-15 1.00 Yes Instr. music Vocal music 4 59 4 3 Ada Lovelace Teacher NaN 1975-05-01 1.00 Yes PENDING Computers 6 43 2 4 Desus Nice Administration Dean 2013-06-06 1.00 Yes PENDING NaN 2 95 2 5 Chien-Shiung Wu Teacher Physics 1930-03-20 0.50 Yes Science 6-12 Physics 7 68 1 6 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.50 Yes Science 6-12 Physics 9 58 1 7 James Joyce Teacher English 1990-05-01 0.50 No English 6-12 English 6-12 9 25 0 8 Hedy Lamarr Teacher Science 1976-06-08 0.50 No PENDING NaN 4 16 0 9 Carlos Boozer Coach Basketball 2015-08-05 NaN No Physical ed NaN 6 70 1 10 Young Boozer Coach NaN 1995-01-01 NaN No Political sci. Political sci. 3 92 4 11 Micheal Larsen Teacher English 2009-09-15 0.80 No Vocal music English 5 39 4 Calculate mean, median of all numerical columns after grouping by employee status. Use .collapse_levels() , a pyjanitor convenience function, to convert the DataFrame returned by .agg() from having multi-level columns (because we supplied a list of aggregation operations) to single-level by concatenating the level names with an underscore: stats_df = ( data_df.groupby(\"employee_status\") .agg([\"mean\", \"median\"]) .collapse_levels() .reset_index() ) stats_df /tmp/ipykernel_1975/210043318.py:2: FutureWarning: ['first_name', 'last_name', 'subject', 'full_time', 'certification', 'certification_1'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning. data_df.groupby(\"employee_status\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee_status hire_date_mean hire_date_median percent_allocated_mean percent_allocated_median lucky_number_mean lucky_number_median age_mean age_median employee_of_month_count_mean employee_of_month_count_median 0 Administration 2013-06-06 00:00:00 2013-06-06 00:00:00 1.000000 1.0 2.000000 2.0 95.0 95.0 2.000000 2.0 1 Coach 2005-04-18 12:00:00 2005-04-18 12:00:00 NaN NaN 4.500000 4.5 81.0 81.0 2.500000 2.5 2 Teacher 1981-03-29 13:20:00 1990-05-01 00:00:00 0.644444 0.5 5.555556 5.0 49.0 43.0 2.111111 2.0","title":"Pyjanitor intro"},{"location":"pyjanitor_intro/#pyjanitor-usage-walkthrough","text":"pyjanitor is a Python-based API on top of pandas inspired by the janitor R package. It aims to provide a clean, understandable interface based on method chaining for common and less-common tasks involving data cleaning and DataFrame manipulation. The core philosophy and augmentations on pandas ' approach to data cleaning and DataFrame manipulation include: A method-chaining paradigm for coding efficiency & clarity of code through greatly improved readability. Implementation of common, useful DataFrame manipulation tasks that saves on repetitive code. Focus on active tense / verb approaches to function naming to provide at-a-glance understanding of a data manipulation pipeline.","title":"pyjanitor Usage Walkthrough"},{"location":"pyjanitor_intro/#why-pyjanitor","text":"Originally a simple port of the R package, pyjanitor has evolved from a set of convenient data cleaning routines into an experiment with the method-chaining paradigm. Data preprocessing is best expressed as a directed acyclic graph (DAG) of actions taken on data. We take a base data file as the starting point and perform actions on it such as removing null/empty rows, replacing them with other values, adding/renaming/removing columns of data, filtering rows, and more. The pandas API has been invaluable for the Python data science ecosystem and implements method chaining for a subset of methods as part of the API. For example, resetting indexes ( .reset_index() ), dropping null values ( .dropna() ), and more are accomplished via the appropriate pd.DataFrame method calls. Inspired by the R statistical language ecosystem, where consistent and good API design in the dplyr package enables end-users, who are not necessarily developers, to concisely express data processing code, pyjanitor has evolved into a language for expressing the data processing DAG for pandas users.","title":"Why pyjanitor?"},{"location":"pyjanitor_intro/#what-is-method-chaining","text":"To accomplish these goals, actions for which we would need to invoke imperative-style statements can be replaced with method chains that allow the user to read off the logical order of actions taken. Note the annotated example below. First, we introduce the textual description of a sample data cleaning pipeline: Create DataFrame . Delete one column. Drop rows with empty values in two particular columns. Rename another two columns. Add a new column. Reset index to account for the missing row we removed above In pandas code, this would look as such: df = pd.DataFrame(...) # create a pandas DataFrame somehow. del df['column1'] # delete a column from the dataframe. df = df.dropna(subset=['column2', 'column3']) # drop rows that have empty values in column 2 and 3. df = df.rename({'column2': 'unicorns', 'column3': 'dragons'}) # rename column2 and column3 df['new_column'] = ['iterable', 'of', 'items'] # add a new column. df.reset_index(inplace=True, drop=True) # reset index to account for the missing row we removed above","title":"What is method chaining?"},{"location":"pyjanitor_intro/#the-pyjanitor-approach","text":"With pyjanitor , we enable method chaining with method names that are verbs which describe the action taken: df = ( pd.DataFrame(...) .remove_columns(['column1']) .dropna(subset=['column2', 'column3']) .rename_column('column2', 'unicorns') .rename_column('column3', 'dragons') .add_column('new_column', ['iterable', 'of', 'items']) .reset_index(drop=True) ) We believe the pyjanitor chaining-based approach leads to much cleaner code where the intent of a series of DataFrame manipulations is much more immediately clear. pyjanitor \u2019s etymology has a two-fold relationship to \u201ccleanliness\u201d. Firstly, it\u2019s about extending pandas with convenient data cleaning routines. Secondly, it\u2019s about providing a cleaner, method-chaining, verb-based API for common pandas routines.","title":"The pyjanitor approach"},{"location":"pyjanitor_intro/#a-survey-of-pyjanitor-functions","text":"Cleaning column names (multi-indexes are possible!) Removing empty rows and columns Identifying duplicate entries Encoding columns as categorical Splitting your data into features and targets (for machine learning) Adding, removing, and renaming columns Coalesce multiple columns into a single column Convert excel date (serial format) into a Python datetime format Expand a single column that has delimited, categorical values into dummy-encoded variables A full list of functionality that pyjanitor implements can be found in the API docs .","title":"A survey of pyjanitor functions"},{"location":"pyjanitor_intro/#some-things-that-are-different","text":"Some pyjanitor methods are DataFrame -mutating operations, i.e., in place. Given that in a method-chaining paradigm, DataFrame s that would be created at each step of the chain cannot be accessed anyway, duplication of data at each step would lead to unnecessary, potential considerable slowdowns and increased memory usage due to data-copying operations. The severity of such copying scales with DataFrame size. Take care to understand which functions change the original DataFrame you are chaining on if it is necessary to preserve that data. If it is, you can simply .copy() it as the first step in a df.copy().operation1().operation2()... chain.","title":"Some things that are different"},{"location":"pyjanitor_intro/#how-it-works","text":"pyjanitor relies on the Pandas Flavor package to register new functions as object methods that can be called directly on DataFrame s. For example: import pandas as pd import pandas_flavor as pf @pf.register_dataframe_method def remove_column(df, column_name: str): del df[column_name] return df df = ( pd.read_csv('my_data.csv') .remove_column('my_column_name') .operation2(...) ) Importing the janitor package immediately registers these functions. The fact that each DataFrame method pyjanitor registers returns the DataFrame is what gives it the capability to method chain. Note that pyjanitor explicitly does not modify any existing pandas methods / functionality.","title":"How it works"},{"location":"pyjanitor_intro/#demo-of-various-dataframe-manipulation-tasks-using-pyjanitor","text":"Here, we'll walk through some useful pyjanitor -based approaches to cleaning and manipulating DataFrame s.","title":"Demo of various DataFrame manipulation tasks using pyjanitor"},{"location":"pyjanitor_intro/#code-preamble","text":"import janitor import numpy as np import pandas as pd from pyprojroot import here","title":"Code preamble:"},{"location":"pyjanitor_intro/#lets-take-a-look-at-our-dataset","text":"df = pd.read_excel(here() / \"data/dirty_data.xlsx\", engine=\"openpyxl\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Last Name Employee Status Subject Hire Date % Allocated Full time? do not edit! ---> Certification Certification.1 Certification.2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN 2 Alicia Keys Teacher Music 37118.0 1.00 Yes NaN Instr. music Vocal music NaN 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes NaN PENDING Computers NaN 4 Desus Nice Administration Dean 41431.0 1.00 Yes NaN PENDING NaN NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 7 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 James Joyce Teacher English 32994.0 0.50 No NaN NaN English 6-12 NaN 9 Hedy Lamarr Teacher Science 27919.0 0.50 No NaN PENDING NaN NaN 10 Carlos Boozer Coach Basketball 42221.0 NaN No NaN Physical ed NaN NaN 11 Young Boozer Coach NaN 34700.0 NaN No NaN NaN Political sci. NaN 12 Micheal Larsen Teacher English 40071.0 0.80 No NaN Vocal music English NaN We can see that this dataset is dirty in a number of ways, including the following: Column names contain spaces, punctuation marks, and inconsistent casing One row ( 7 ) with completely missing data One column ( do not edit! ---> ) with completely missing data","title":"Let's take a look at our dataset:"},{"location":"pyjanitor_intro/#clean-up-our-data-using-a-pyjanitor-method-chaining-pipeline","text":"Let's run through a demo DataFrame cleaning procedure: cleaned_df = ( pd.read_excel(here() / \"data/dirty_data.xlsx\", engine=\"openpyxl\") .clean_names() .remove_empty() .rename_column(\"%_allocated\", \"percent_allocated\") .rename_column(\"full_time_\", \"full_time\") .coalesce([\"certification\", \"certification_1\"], \"certification\") .encode_categorical([\"subject\", \"employee_status\", \"full_time\"]) .convert_excel_date(\"hire_date\") .reset_index(drop=True) ) cleaned_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 2008-08-30 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 2008-08-30 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 2001-08-15 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 1975-05-01 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 2013-06-06 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 1930-03-20 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 1990-05-01 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 1976-06-08 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 2015-08-05 NaN No Physical ed NaN 10 Young Boozer Coach NaN 1995-01-01 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 2009-09-15 0.80 No Vocal music English The cleaned DataFrame looks much better and quite a bit more usable for our downstream tasks.","title":"Clean up our data using a pyjanitor method-chaining pipeline"},{"location":"pyjanitor_intro/#step-by-step-walkthrough-of-pyjanitor-dataframe-manipulations","text":"Just for clearer understanding of the above, let's see how pyjanitor progressively modified the data. Loading data in: df = pd.read_excel(here() / \"data/dirty_data.xlsx\", engine=\"openpyxl\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } First Name Last Name Employee Status Subject Hire Date % Allocated Full time? do not edit! ---> Certification Certification.1 Certification.2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN 2 Alicia Keys Teacher Music 37118.0 1.00 Yes NaN Instr. music Vocal music NaN 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes NaN PENDING Computers NaN 4 Desus Nice Administration Dean 41431.0 1.00 Yes NaN PENDING NaN NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 7 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 James Joyce Teacher English 32994.0 0.50 No NaN NaN English 6-12 NaN 9 Hedy Lamarr Teacher Science 27919.0 0.50 No NaN PENDING NaN NaN 10 Carlos Boozer Coach Basketball 42221.0 NaN No NaN Physical ed NaN NaN 11 Young Boozer Coach NaN 34700.0 NaN No NaN NaN Political sci. NaN 12 Micheal Larsen Teacher English 40071.0 0.80 No NaN Vocal music English NaN Clean up names by removing whitespace, punctuation / symbols, capitalization: df = df.clean_names() df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date %_allocated full_time_ do_not_edit!_> certification certification_1 certification_2 0 Jason Bourne Teacher PE 39690.0 0.75 Yes NaN Physical ed Theater NaN 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes NaN Physical ed Theater NaN 2 Alicia Keys Teacher Music 37118.0 1.00 Yes NaN Instr. music Vocal music NaN 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes NaN PENDING Computers NaN 4 Desus Nice Administration Dean 41431.0 1.00 Yes NaN PENDING NaN NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes NaN Science 6-12 Physics NaN 7 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 8 James Joyce Teacher English 32994.0 0.50 No NaN NaN English 6-12 NaN 9 Hedy Lamarr Teacher Science 27919.0 0.50 No NaN PENDING NaN NaN 10 Carlos Boozer Coach Basketball 42221.0 NaN No NaN Physical ed NaN NaN 11 Young Boozer Coach NaN 34700.0 NaN No NaN NaN Political sci. NaN 12 Micheal Larsen Teacher English 40071.0 0.80 No NaN Vocal music English NaN Remove entirely empty rows / columns: df = df.remove_empty() df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date %_allocated full_time_ certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.50 No NaN English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 42221.0 NaN No Physical ed NaN 10 Young Boozer Coach NaN 34700.0 NaN No NaN Political sci. 11 Micheal Larsen Teacher English 40071.0 0.80 No Vocal music English Rename particular columns: df = df.rename_column(\"%_allocated\", \"percent_allocated\").rename_column( \"full_time_\", \"full_time\" ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.50 No NaN English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 42221.0 NaN No Physical ed NaN 10 Young Boozer Coach NaN 34700.0 NaN No NaN Political sci. 11 Micheal Larsen Teacher English 40071.0 0.80 No Vocal music English Take first non- NaN row value in two columns: df = df.coalesce([\"certification\", \"certification_1\"], \"certification\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 39690.0 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 39690.0 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 37118.0 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 27515.0 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 41431.0 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 11037.0 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 11037.0 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 32994.0 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 27919.0 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 42221.0 NaN No Physical ed NaN 10 Young Boozer Coach NaN 34700.0 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 40071.0 0.80 No Vocal music English Convert string object rows to categorical to save on memory consumption and speed up access: df.dtypes first_name object last_name object employee_status object subject object hire_date float64 percent_allocated float64 full_time object certification object certification_1 object dtype: object df.encode_categorical([\"subject\", \"employee_status\", \"full_time\"]) df.dtypes first_name object last_name object employee_status object subject object hire_date float64 percent_allocated float64 full_time object certification object certification_1 object dtype: object Convert Excel date-formatted column to a more interpretable format: df.convert_excel_date(\"hire_date\") df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 0 Jason Bourne Teacher PE 2008-08-30 0.75 Yes Physical ed Theater 1 Jason Bourne Teacher Drafting 2008-08-30 0.25 Yes Physical ed Theater 2 Alicia Keys Teacher Music 2001-08-15 1.00 Yes Instr. music Vocal music 3 Ada Lovelace Teacher NaN 1975-05-01 1.00 Yes PENDING Computers 4 Desus Nice Administration Dean 2013-06-06 1.00 Yes PENDING NaN 5 Chien-Shiung Wu Teacher Physics 1930-03-20 0.50 Yes Science 6-12 Physics 6 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.50 Yes Science 6-12 Physics 7 James Joyce Teacher English 1990-05-01 0.50 No English 6-12 English 6-12 8 Hedy Lamarr Teacher Science 1976-06-08 0.50 No PENDING NaN 9 Carlos Boozer Coach Basketball 2015-08-05 NaN No Physical ed NaN 10 Young Boozer Coach NaN 1995-01-01 NaN No Political sci. Political sci. 11 Micheal Larsen Teacher English 2009-09-15 0.80 No Vocal music English","title":"Step-by-step walkthrough of pyjanitor DataFrame manipulations"},{"location":"pyjanitor_intro/#example-analysis-of-the-data","text":"Let's perform analysis on the above, cleaned DataFrame . First we add some additional, randomly-generated data. Note that we .copy() the original to preserve it, given that the following would otherwise modify it: data_df = cleaned_df.copy().add_columns( lucky_number=np.random.randint(0, 10, len(cleaned_df)), age=np.random.randint(10, 100, len(cleaned_df)), employee_of_month_count=np.random.randint(0, 5, len(cleaned_df)), ) data_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first_name last_name employee_status subject hire_date percent_allocated full_time certification certification_1 lucky_number age employee_of_month_count 0 Jason Bourne Teacher PE 2008-08-30 0.75 Yes Physical ed Theater 4 38 4 1 Jason Bourne Teacher Drafting 2008-08-30 0.25 Yes Physical ed Theater 2 95 3 2 Alicia Keys Teacher Music 2001-08-15 1.00 Yes Instr. music Vocal music 4 59 4 3 Ada Lovelace Teacher NaN 1975-05-01 1.00 Yes PENDING Computers 6 43 2 4 Desus Nice Administration Dean 2013-06-06 1.00 Yes PENDING NaN 2 95 2 5 Chien-Shiung Wu Teacher Physics 1930-03-20 0.50 Yes Science 6-12 Physics 7 68 1 6 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.50 Yes Science 6-12 Physics 9 58 1 7 James Joyce Teacher English 1990-05-01 0.50 No English 6-12 English 6-12 9 25 0 8 Hedy Lamarr Teacher Science 1976-06-08 0.50 No PENDING NaN 4 16 0 9 Carlos Boozer Coach Basketball 2015-08-05 NaN No Physical ed NaN 6 70 1 10 Young Boozer Coach NaN 1995-01-01 NaN No Political sci. Political sci. 3 92 4 11 Micheal Larsen Teacher English 2009-09-15 0.80 No Vocal music English 5 39 4 Calculate mean, median of all numerical columns after grouping by employee status. Use .collapse_levels() , a pyjanitor convenience function, to convert the DataFrame returned by .agg() from having multi-level columns (because we supplied a list of aggregation operations) to single-level by concatenating the level names with an underscore: stats_df = ( data_df.groupby(\"employee_status\") .agg([\"mean\", \"median\"]) .collapse_levels() .reset_index() ) stats_df /tmp/ipykernel_1975/210043318.py:2: FutureWarning: ['first_name', 'last_name', 'subject', 'full_time', 'certification', 'certification_1'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning. data_df.groupby(\"employee_status\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } employee_status hire_date_mean hire_date_median percent_allocated_mean percent_allocated_median lucky_number_mean lucky_number_median age_mean age_median employee_of_month_count_mean employee_of_month_count_median 0 Administration 2013-06-06 00:00:00 2013-06-06 00:00:00 1.000000 1.0 2.000000 2.0 95.0 95.0 2.000000 2.0 1 Coach 2005-04-18 12:00:00 2005-04-18 12:00:00 NaN NaN 4.500000 4.5 81.0 81.0 2.500000 2.5 2 Teacher 1981-03-29 13:20:00 1990-05-01 00:00:00 0.644444 0.5 5.555556 5.0 49.0 43.0 2.111111 2.0","title":"Example analysis of the data"},{"location":"sort_naturally/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Using sort_naturally import janitor import pandas as pd import pandas_flavor as pf Let's say we have a pandas DataFrame that contains wells that we need to sort alphanumerically. data = { \"Well\": [\"A21\", \"A3\", \"A21\", \"B2\", \"B51\", \"B12\"], \"Value\": [1, 2, 13, 3, 4, 7], } df = pd.DataFrame(data) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Well Value 0 A21 1 1 A3 2 2 A21 13 3 B2 3 4 B51 4 5 B12 7 A human would sort it in the order: A3, A21, A21, B2, B12, B51 However, default sorting in pandas doesn't allow that: df.sort_values(\"Well\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Well Value 0 A21 1 2 A21 13 1 A3 2 5 B12 7 3 B2 3 4 B51 4 Lexiographic sorting doesn't get us to where we want. A12 shouldn't come before A3, and B11 shouldn't come before B2. How might we fix this? df.sort_naturally(\"Well\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Well Value 1 A3 2 0 A21 1 2 A21 13 3 B2 3 5 B12 7 4 B51 4 Now we're in sorting bliss! :)","title":"Sort naturally"},{"location":"sort_naturally/#using-sort_naturally","text":"import janitor import pandas as pd import pandas_flavor as pf Let's say we have a pandas DataFrame that contains wells that we need to sort alphanumerically. data = { \"Well\": [\"A21\", \"A3\", \"A21\", \"B2\", \"B51\", \"B12\"], \"Value\": [1, 2, 13, 3, 4, 7], } df = pd.DataFrame(data) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Well Value 0 A21 1 1 A3 2 2 A21 13 3 B2 3 4 B51 4 5 B12 7 A human would sort it in the order: A3, A21, A21, B2, B12, B51 However, default sorting in pandas doesn't allow that: df.sort_values(\"Well\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Well Value 0 A21 1 2 A21 13 1 A3 2 5 B12 7 3 B2 3 4 B51 4 Lexiographic sorting doesn't get us to where we want. A12 shouldn't come before A3, and B11 shouldn't come before B2. How might we fix this? df.sort_naturally(\"Well\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Well Value 1 A3 2 0 A21 1 2 A21 13 3 B2 3 5 B12 7 4 B51 4 Now we're in sorting bliss! :)","title":"Using sort_naturally"},{"location":"teacher_pupil/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Processing International Teacher Data Background This sample data comes from the UNESCO Institute of Statistics and can be found at tidytuesdays' github repo . This subset of the the data collected by the UNESCO Institute of Statistics contains country-level data on the number of teachers, teacher-to-student ratios, and related figures. import janitor import pandas as pd import pandas_flavor as pf dirty_csv = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-07/EDULIT_DS_06052019101747206.csv\" dirty_df = pd.read_csv(dirty_csv) dirty_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDULIT_IND Indicator LOCATION Country TIME Time Value Flag Codes Flags 0 PTRHC_2 Pupil-teacher ratio in lower secondary educati... MRT Mauritania 2013 2013 56.59395 NaN NaN 1 PTRHC_2 Pupil-teacher ratio in lower secondary educati... MRT Mauritania 2014 2014 51.94690 NaN NaN 2 PTRHC_2 Pupil-teacher ratio in lower secondary educati... MRT Mauritania 2015 2015 53.22717 NaN NaN 3 PTRHC_2 Pupil-teacher ratio in lower secondary educati... MRT Mauritania 2016 2016 38.18923 NaN NaN 4 PTRHC_1 Pupil-teacher ratio in primary education (head... COD Democratic Republic of the Congo 2012 2012 34.74758 NaN NaN Data Dictionary Below is the data dictionary from the tidytuesday github repo. variable class description edulit_ind character Unique ID indicator character Education level group (\"Lower Secondary Education\", \"Primary Education\", \"Upper Secondary Education\", \"Pre-Primary Education\", \"Secondary Education\", \"Tertiary Education\", \"Post-Secondary Non-Tertiary Education\") country_code character Country code country character Country Full name year integer (date) Year student_ratio double Student to teacher ratio (lower = fewer students/teacher) flag_codes character Code to indicate some metadata about exceptions flags character Metadata about exceptions The indicator variable describles the education level for each observation. Let's evaluate the actual values of the Indicator column in the data. # The set of unique values in the indicator column set(dirty_df.Indicator) {'Pupil-teacher ratio in lower secondary education (headcount basis)', 'Pupil-teacher ratio in post-secondary non-tertiary education (headcount basis)', 'Pupil-teacher ratio in pre-primary education (headcount basis)', 'Pupil-teacher ratio in primary education (headcount basis)', 'Pupil-teacher ratio in secondary education (headcount basis)', 'Pupil-teacher ratio in tertiary education (headcount basis)', 'Pupil-teacher ratio in upper secondary education (headcount basis)'} Notice that strings in the dataframe each contain \"Pupil-teach ratio in\" & \"(headcount basis)\". We don't need all of this text to analyze the data. need some custom functions to clean up the strings. We'll need a function that removes a substring, given a pattern, from values in columns. Another function that removes trailing and leading characters from a value in a column. And finally, a function to make the first letter in each string upper case. Data Cleaning @pf.register_dataframe_method def str_remove(df, column_name: str, pat: str, *args, **kwargs): \"\"\"Remove a substring, given its pattern from a string value, in a given column\"\"\" df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) return df @pf.register_dataframe_method def str_trim(df, column_name: str, *args, **kwargs): \"\"\"Remove trailing and leading characters, in a given column\"\"\" df[column_name] = df[column_name].str.strip(*args, **kwargs) return df @pf.register_dataframe_method def str_title(df, column_name: str, *args, **kwargs): \"\"\"Make the first letter in each word upper case\"\"\" df[column_name] = df[column_name].str.title(*args, **kwargs) return df @pf.register_dataframe_method def drop_duplicated_column(df, column_name: str, column_order: int = 0): \"\"\"Remove duplicated columns and retain only a column given its order. Order 0 is to remove the first column, Order 1 is to remove the second column, and etc \"\"\" cols = list(df.columns) col_indexes = [ col_idx for col_idx, col_name in enumerate(cols) if col_name == column_name ] # given that a column could be duplicated, user could opt based on its order removed_col_idx = col_indexes[column_order] # get the column indexes without column that is being removed filtered_cols = [c_i for c_i, c_v in enumerate(cols) if c_i != removed_col_idx] return df.iloc[:, filtered_cols] Note in the next example how we are able to chain our manipulations of the data into one process without losing our ability to explain what we are doing. The is the preferred framework for using pyjanitor py_clean_df = ( dirty_df.clean_names() # modify string values .str_remove(\"indicator\", \"Pupil-teacher ratio in\") .str_remove(\"indicator\", \"(headcount basis)\") .str_remove(\"indicator\", \"\\\\(\\\\)\") .str_trim(\"indicator\") .str_trim(\"country\") .str_title(\"indicator\") # remove `time` column (which is duplicated). The second `time` is being removed .drop_duplicated_column(\"time\", 1) # renaming columns .rename_column(\"location\", \"country_code\") .rename_column(\"value\", \"student_ratio\") .rename_column(\"time\", \"year\") ) py_clean_df.head() /tmp/ipykernel_2019/4179283386.py:4: FutureWarning: The default value of regex will change from True to False in a future version. df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) /tmp/ipykernel_2019/4179283386.py:4: FutureWarning: The default value of regex will change from True to False in a future version. df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } edulit_ind indicator country_code country year student_ratio flag_codes flags 0 PTRHC_2 Lower Secondary Education MRT Mauritania 2013 56.59395 NaN NaN 1 PTRHC_2 Lower Secondary Education MRT Mauritania 2014 51.94690 NaN NaN 2 PTRHC_2 Lower Secondary Education MRT Mauritania 2015 53.22717 NaN NaN 3 PTRHC_2 Lower Secondary Education MRT Mauritania 2016 38.18923 NaN NaN 4 PTRHC_1 Primary Education COD Democratic Republic of the Congo 2012 34.74758 NaN NaN # ensure that the output from janitor is similar with the clean r's janitor r_clean_csv = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-07/student_teacher_ratio.csv\" r_clean_df = pd.read_csv(r_clean_csv) pd.testing.assert_frame_equal(r_clean_df, py_clean_df)","title":"Teacher pupil"},{"location":"teacher_pupil/#processing-international-teacher-data","text":"","title":"Processing International Teacher Data"},{"location":"teacher_pupil/#background","text":"This sample data comes from the UNESCO Institute of Statistics and can be found at tidytuesdays' github repo . This subset of the the data collected by the UNESCO Institute of Statistics contains country-level data on the number of teachers, teacher-to-student ratios, and related figures. import janitor import pandas as pd import pandas_flavor as pf dirty_csv = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-07/EDULIT_DS_06052019101747206.csv\" dirty_df = pd.read_csv(dirty_csv) dirty_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDULIT_IND Indicator LOCATION Country TIME Time Value Flag Codes Flags 0 PTRHC_2 Pupil-teacher ratio in lower secondary educati... MRT Mauritania 2013 2013 56.59395 NaN NaN 1 PTRHC_2 Pupil-teacher ratio in lower secondary educati... MRT Mauritania 2014 2014 51.94690 NaN NaN 2 PTRHC_2 Pupil-teacher ratio in lower secondary educati... MRT Mauritania 2015 2015 53.22717 NaN NaN 3 PTRHC_2 Pupil-teacher ratio in lower secondary educati... MRT Mauritania 2016 2016 38.18923 NaN NaN 4 PTRHC_1 Pupil-teacher ratio in primary education (head... COD Democratic Republic of the Congo 2012 2012 34.74758 NaN NaN","title":"Background"},{"location":"teacher_pupil/#data-dictionary","text":"Below is the data dictionary from the tidytuesday github repo. variable class description edulit_ind character Unique ID indicator character Education level group (\"Lower Secondary Education\", \"Primary Education\", \"Upper Secondary Education\", \"Pre-Primary Education\", \"Secondary Education\", \"Tertiary Education\", \"Post-Secondary Non-Tertiary Education\") country_code character Country code country character Country Full name year integer (date) Year student_ratio double Student to teacher ratio (lower = fewer students/teacher) flag_codes character Code to indicate some metadata about exceptions flags character Metadata about exceptions The indicator variable describles the education level for each observation. Let's evaluate the actual values of the Indicator column in the data. # The set of unique values in the indicator column set(dirty_df.Indicator) {'Pupil-teacher ratio in lower secondary education (headcount basis)', 'Pupil-teacher ratio in post-secondary non-tertiary education (headcount basis)', 'Pupil-teacher ratio in pre-primary education (headcount basis)', 'Pupil-teacher ratio in primary education (headcount basis)', 'Pupil-teacher ratio in secondary education (headcount basis)', 'Pupil-teacher ratio in tertiary education (headcount basis)', 'Pupil-teacher ratio in upper secondary education (headcount basis)'} Notice that strings in the dataframe each contain \"Pupil-teach ratio in\" & \"(headcount basis)\". We don't need all of this text to analyze the data. need some custom functions to clean up the strings. We'll need a function that removes a substring, given a pattern, from values in columns. Another function that removes trailing and leading characters from a value in a column. And finally, a function to make the first letter in each string upper case.","title":"Data Dictionary"},{"location":"teacher_pupil/#data-cleaning","text":"@pf.register_dataframe_method def str_remove(df, column_name: str, pat: str, *args, **kwargs): \"\"\"Remove a substring, given its pattern from a string value, in a given column\"\"\" df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) return df @pf.register_dataframe_method def str_trim(df, column_name: str, *args, **kwargs): \"\"\"Remove trailing and leading characters, in a given column\"\"\" df[column_name] = df[column_name].str.strip(*args, **kwargs) return df @pf.register_dataframe_method def str_title(df, column_name: str, *args, **kwargs): \"\"\"Make the first letter in each word upper case\"\"\" df[column_name] = df[column_name].str.title(*args, **kwargs) return df @pf.register_dataframe_method def drop_duplicated_column(df, column_name: str, column_order: int = 0): \"\"\"Remove duplicated columns and retain only a column given its order. Order 0 is to remove the first column, Order 1 is to remove the second column, and etc \"\"\" cols = list(df.columns) col_indexes = [ col_idx for col_idx, col_name in enumerate(cols) if col_name == column_name ] # given that a column could be duplicated, user could opt based on its order removed_col_idx = col_indexes[column_order] # get the column indexes without column that is being removed filtered_cols = [c_i for c_i, c_v in enumerate(cols) if c_i != removed_col_idx] return df.iloc[:, filtered_cols] Note in the next example how we are able to chain our manipulations of the data into one process without losing our ability to explain what we are doing. The is the preferred framework for using pyjanitor py_clean_df = ( dirty_df.clean_names() # modify string values .str_remove(\"indicator\", \"Pupil-teacher ratio in\") .str_remove(\"indicator\", \"(headcount basis)\") .str_remove(\"indicator\", \"\\\\(\\\\)\") .str_trim(\"indicator\") .str_trim(\"country\") .str_title(\"indicator\") # remove `time` column (which is duplicated). The second `time` is being removed .drop_duplicated_column(\"time\", 1) # renaming columns .rename_column(\"location\", \"country_code\") .rename_column(\"value\", \"student_ratio\") .rename_column(\"time\", \"year\") ) py_clean_df.head() /tmp/ipykernel_2019/4179283386.py:4: FutureWarning: The default value of regex will change from True to False in a future version. df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) /tmp/ipykernel_2019/4179283386.py:4: FutureWarning: The default value of regex will change from True to False in a future version. df[column_name] = df[column_name].str.replace(pat, \"\", *args, **kwargs) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } edulit_ind indicator country_code country year student_ratio flag_codes flags 0 PTRHC_2 Lower Secondary Education MRT Mauritania 2013 56.59395 NaN NaN 1 PTRHC_2 Lower Secondary Education MRT Mauritania 2014 51.94690 NaN NaN 2 PTRHC_2 Lower Secondary Education MRT Mauritania 2015 53.22717 NaN NaN 3 PTRHC_2 Lower Secondary Education MRT Mauritania 2016 38.18923 NaN NaN 4 PTRHC_1 Primary Education COD Democratic Republic of the Congo 2012 34.74758 NaN NaN # ensure that the output from janitor is similar with the clean r's janitor r_clean_csv = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-07/student_teacher_ratio.csv\" r_clean_df = pd.read_csv(r_clean_csv) pd.testing.assert_frame_equal(r_clean_df, py_clean_df)","title":"Data Cleaning"},{"location":"transform_column/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); from random import choice import janitor import numpy as np import pandas as pd Transforming columns Introduction There are two ways to use the transform_column function: by passing in a function that operates elementwise, or by passing in a function that operates columnwise. We will show you both in this notebook. Numeric Data data = np.random.normal(size=(1_000_000, 4)) df = pd.DataFrame(data).clean_names() Using the elementwise application: %%timeit # We are using a lambda function that operates on each element, # to highlight the point about elementwise operations. df.transform_column(\"0\", lambda x: np.abs(x), \"abs_0\") 1.08 s \u00b1 1.65 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) And now using columnwise application: %%timeit df.transform_column(\"0\", lambda s: np.abs(s), elementwise=False) 8.5 ms \u00b1 70.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Because np.abs is vectorizable over the entire series, it runs about 50X faster. If you know your function is vectorizable, then take advantage of the fact, and use it inside transform_column . After all, all that transform_column has done is provide a method-chainable way of applying the function. String Data Let's see it in action with string-type data. def make_strings(length: int): return \"\".join(choice(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\") for _ in range(length)) strings = (make_strings(30) for _ in range(1_000_000)) stringdf = pd.DataFrame({\"data\": list(strings)}) Firstly, by raw function application: def first_five(s): return s.str[0:5] %%timeit stringdf.assign(data=first_five(stringdf[\"data\"])) 279 ms \u00b1 616 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %%timeit first_five(stringdf[\"data\"]) 194 ms \u00b1 742 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %%timeit stringdf[\"data\"].str[0:5] 194 ms \u00b1 166 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) %%timeit stringdf[\"data\"].apply(lambda x: x[0:5]) 183 ms \u00b1 168 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) It appears assigning the result to a column comes with a bit of overhead. Now, by using transform_column with default settings: %%timeit stringdf.transform_column(\"data\", lambda x: x[0:5]) 211 ms \u00b1 740 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Now by using transform_column while also leveraging string methods: %%timeit stringdf.transform_column(\"data\", first_five, elementwise=False) 279 ms \u00b1 1.49 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)","title":"Transform column"},{"location":"transform_column/#transforming-columns","text":"","title":"Transforming columns"},{"location":"transform_column/#introduction","text":"There are two ways to use the transform_column function: by passing in a function that operates elementwise, or by passing in a function that operates columnwise. We will show you both in this notebook.","title":"Introduction"},{"location":"transform_column/#numeric-data","text":"data = np.random.normal(size=(1_000_000, 4)) df = pd.DataFrame(data).clean_names() Using the elementwise application: %%timeit # We are using a lambda function that operates on each element, # to highlight the point about elementwise operations. df.transform_column(\"0\", lambda x: np.abs(x), \"abs_0\") 1.08 s \u00b1 1.65 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) And now using columnwise application: %%timeit df.transform_column(\"0\", lambda s: np.abs(s), elementwise=False) 8.5 ms \u00b1 70.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Because np.abs is vectorizable over the entire series, it runs about 50X faster. If you know your function is vectorizable, then take advantage of the fact, and use it inside transform_column . After all, all that transform_column has done is provide a method-chainable way of applying the function.","title":"Numeric Data"},{"location":"transform_column/#string-data","text":"Let's see it in action with string-type data. def make_strings(length: int): return \"\".join(choice(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\") for _ in range(length)) strings = (make_strings(30) for _ in range(1_000_000)) stringdf = pd.DataFrame({\"data\": list(strings)}) Firstly, by raw function application: def first_five(s): return s.str[0:5] %%timeit stringdf.assign(data=first_five(stringdf[\"data\"])) 279 ms \u00b1 616 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %%timeit first_five(stringdf[\"data\"]) 194 ms \u00b1 742 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %%timeit stringdf[\"data\"].str[0:5] 194 ms \u00b1 166 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) %%timeit stringdf[\"data\"].apply(lambda x: x[0:5]) 183 ms \u00b1 168 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) It appears assigning the result to a column comes with a bit of overhead. Now, by using transform_column with default settings: %%timeit stringdf.transform_column(\"data\", lambda x: x[0:5]) 211 ms \u00b1 740 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) Now by using transform_column while also leveraging string methods: %%timeit stringdf.transform_column(\"data\", first_five, elementwise=False) 279 ms \u00b1 1.49 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)","title":"String Data"}]}